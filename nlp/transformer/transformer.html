
<!DOCTYPE html>


<html lang="en" data-content_root="../../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Transformer &#8212; LazyNotes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../../_static/doctools.js?v=9a2dae69"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../../_static/copybutton.js?v=f281be69"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'nlp/transformer/transformer';</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Tokenization" href="tokenizer.html" />
    <link rel="prev" title="Precision &amp; Quantization" href="../../dl/quant.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="LazyNotes - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="LazyNotes - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../dl/module.html">Module</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dl/optim.html">Optimization (Parameter)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dl/loss.html">Loss</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dl/quant.html">Precision &amp; Quantization</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">NLP</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="current reference internal" href="#">Transformer</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="tokenizer.html">Tokenization</a></li>
<li class="toctree-l2"><a class="reference internal" href="pe.html">Positional Encoding</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../models.html">Models</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../training/training.html">Training</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../training/peft.html">PEFT</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/rlhf.html">RL for LLMs</a></li>

<li class="toctree-l2"><a class="reference internal" href="../training/misc.html">Misc</a></li>
<li class="toctree-l2"><a class="reference internal" href="../training/parallel.html">Distributed Training</a></li>
</ul>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../inference/inference.html">Inference</a><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul>
<li class="toctree-l2"><a class="reference internal" href="../inference/decoding.html">Decoding</a></li>
<li class="toctree-l2"><a class="reference internal" href="../inference/misc.html">Misc</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../eval.html">Evaluation</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Math</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../math/info.html">Information Theory</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../about.html">About</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../dsa.html">Coding</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fnlp/transformer/transformer.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/nlp/transformer/transformer.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Transformer</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding">Token Embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-causal-attention">Masked/Causal Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#output">Output</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="transformer">
<h1>Transformer<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>What</strong>: <strong>Self-attention</strong> for modeling relationships between all tokens in input sequence.</p></li>
<li><p><strong>Why</strong>: <strong>Long-range dependencies</strong> + <strong>Parallel processing</strong>.</p></li>
<li><p><strong>How</strong>: <span id="id1">[<a class="reference internal" href="../../references.html#id12" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.">6</a>]</span></p></li>
</ul>
<a class="reference internal image-reference" href="../../_images/transformer.png"><img alt="../../_images/transformer.png" class="align-center" src="../../_images/transformer.png" style="width: 500px;" />
</a>
<!-- ````{dropdown} Input
```{image} ../images/transformer/input.png
:align: center
:width: 400px
```
````

````{dropdown} MHA
```{image} ../images/transformer/attention.png
:align: center
:width: 400px
```
```` -->
<section id="tokenization">
<h2>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Sequence <span class="math notranslate nohighlight">\(\rightarrow\)</span> Tokens</p></li>
<li><p><strong>Why</strong>: Machines can only read numbers.</p></li>
</ul>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Why subword-level vocab? Why not whole words? Why not characters?</em></p>
<ul class="simple">
<li><p>Word-level vocab explode with out-of-vocab words.</p></li>
<li><p>Char-level vocab misses morphology.</p></li>
<li><p>Subword offers a fixed-size, open-vocab symbol set which can handle rare words while maintaining morphology.</p></li>
</ul>
</div>
<br/>
</section>
<section id="token-embedding">
<h2>Token Embedding<a class="headerlink" href="#token-embedding" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Tokens <span class="math notranslate nohighlight">\(\rightarrow\)</span> Semantic vectors.</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p>Discrete tokens <span class="math notranslate nohighlight">\(\xrightarrow{\text{map to}}\)</span> Continuous vectors</p></li>
<li><p>Vocab index <span class="math notranslate nohighlight">\(\xrightarrow{\text{map to}}\)</span> Semantic meaning</p></li>
<li><p>Vocab size <span class="math notranslate nohighlight">\(\xrightarrow{\text{reduced to}}\)</span> hidden size</p></li>
</ul>
</li>
<li><p><strong>How</strong>: <a class="reference internal" href="#../dl/module.md#linear"><span class="xref myst">Linear</span></a>.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(T=(t_1,\cdots,t_m)\)</span>: Input token sequence (after tokenization).</p></li>
<li><p><span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times d_{\text{model}}}\)</span>: Output semantic vectors.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(E\in\mathbb{R}^{V\times d_{\text{model}}}\)</span>: Embedding matrix/look-up table.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(m\)</span>: #Tokens.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_{\text{model}}\)</span>: Embedding dim for the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: Vocab size.</p></li>
</ul>
</li>
</ul>
<p>Token Embedding:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
X=\begin{bmatrix}
E_{t_1} \\
\vdots \\
E_{t_m}
\end{bmatrix}
\end{split}\]</div>
</div>
<br/>
</section>
<section id="positional-encoding">
<h2>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Semantic vectors + Positional vectors <span class="math notranslate nohighlight">\(\rightarrow\)</span> Position-aware vectors</p></li>
<li><p><strong>Why</strong>: Transformers don’t know positions due to parallelism, BUT positions matter.</p>
<ul>
<li><p>No PE <span class="math notranslate nohighlight">\(\rightarrow\)</span> Self-attention scores remain unchanged regardless of token orders</p></li>
</ul>
</li>
<li><p><strong>How</strong>: Add positional vectors onto semantic vectors.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times d_{\text{model}}}\)</span>: Input/Output semantic vectors.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(P\in\mathbb{R}^{m\times d_{\text{model}}}\)</span>: Positional embedding vectors.</p></li>
</ul>
</li>
</ul>
<p>Positional Encoding:</p>
<div class="math notranslate nohighlight">
\[
X\leftarrow X+P
\]</div>
</div>
<br/>
</section>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Different weights for different parts.</p>
<ul>
<li><p>Focus on important parts &amp; Diffuse on trivial parts.</p></li>
</ul>
</li>
<li><p><strong>Why</strong>: Enable models to dynamically align &amp; retrieve most relevant parts of input sequence when generating each output token.</p></li>
</ul>
<section id="self-attention">
<h3>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Each token in the sequence pays attention to all other tokens in the same sequence, to produce <strong>context-aware representations</strong>.</p></li>
<li><p><strong>Why</strong>: <strong>Long-range dependencies</strong> + <strong>Parallel processing</strong></p></li>
<li><p><strong>How</strong>: Information Retrieval.</p>
<ol class="arabic simple">
<li><p>All tokens <span class="math notranslate nohighlight">\(\xrightarrow{\text{convert to}}\)</span> <span class="math notranslate nohighlight">\(Q\)</span>,<span class="math notranslate nohighlight">\(K\)</span>,<span class="math notranslate nohighlight">\(V\)</span></p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Q\)</span>: What are you looking for?</p></li>
<li><p><span class="math notranslate nohighlight">\(K\)</span>: What are the keywords for identification?</p></li>
<li><p><span class="math notranslate nohighlight">\(V\)</span>: What is the content?</p></li>
</ul>
</li>
<li><p>For each token <span class="math notranslate nohighlight">\(t\)</span>:</p>
<ol class="arabic simple">
<li><p>Multiply its query &amp; all tokens’ keys <span class="math notranslate nohighlight">\(\rightarrow\)</span> Relevance scores</p></li>
<li><p>Scale &amp; Softmax the scores <span class="math notranslate nohighlight">\(\rightarrow\)</span> Attention weights</p></li>
<li><p>Weighted sum of all tokens’ values <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(t\)</span>’s contextual representation</p></li>
</ol>
</li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>Input:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times d_{\text{model}}}\)</span>: Input sequence</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_Q\in\mathbb{R}^{d_{\text{model}}\times d_K}\)</span>: Weight matrix for Q.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_K\in\mathbb{R}^{d_{\text{model}}\times d_K}\)</span>: Weight matrix for K.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_V\in\mathbb{R}^{d_{\text{model}}\times d_V}\)</span>: Weight matrix for V.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(m\)</span>: #Tokens.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_{\text{model}}\)</span>: Embedding dim of input sequence.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_K\)</span>: Embedding dim of Q &amp; K.</p>
<ul>
<li><p>Q &amp; K share the same embedding dim for matrix multiplication.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(d_V\)</span>: Embedding dim of V (practically the same as <span class="math notranslate nohighlight">\(d_K\)</span>).</p></li>
</ul>
</li>
<li><p>Misc:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Q=XW_Q\in\mathbb{R}^{m\times d_K}\)</span>: Q vectors for all tokens.</p></li>
<li><p><span class="math notranslate nohighlight">\(K=XW_K\in\mathbb{R}^{m\times d_K}\)</span>: K vectors for all tokens.</p></li>
<li><p><span class="math notranslate nohighlight">\(V=XW_V\in\mathbb{R}^{m\times d_V}\)</span>: V vectors for all tokens.</p></li>
</ul>
</li>
</ul>
<p>Scaled Dot-Product Attention:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_K}}\right)V
\]</div>
</div>
<div class="important dropdown admonition">
<p class="admonition-title">Derivation (Backprop)</p>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S=\frac{QK^T}{\sqrt{d_K}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A=\text{softmax}(S)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Y=AV\)</span></p></li>
</ul>
<p>Process:</p>
<ol class="arabic simple">
<li><p>V:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial V}=A^T\frac{\partial\mathcal{L}}{\partial Y}
\]</div>
<ol class="arabic simple" start="2">
<li><p>A:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial A}=\frac{\partial\mathcal{L}}{\partial Y}V^T
\]</div>
<ol class="arabic simple" start="3">
<li><p>S:</p></li>
</ol>
<ul>
<li><p>Recall that for <span class="math notranslate nohighlight">\(\mathbf{a}=\text{softmax}(\mathbf{s})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
	\frac{\partial a_i}{\partial s_j}=a_i(\delta_{ij}-a_j)
	\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_{ij}=1\text{ if }i=j\text{ else }0\)</span>.</p>
</li>
<li><p>For each row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(S\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
	\frac{\partial\mathcal{L}}{\partial S_{ij}}&amp;=\sum_{k=1}^{m}\frac{\partial\mathcal{L}}{\partial A_{ik}}\frac{\partial A_{ik}}{\partial S_{ij}} \\
	&amp;=\frac{\partial\mathcal{L}}{\partial A_{ij}}A_{ij}-A_{ij}\frac{\partial\mathcal{L}}{\partial A_{ij}}A_{ij}-A_{ij}\sum_{k\neq j}\frac{\partial\mathcal{L}}{\partial A_{ik}}A_{ik} \\
	&amp;=\frac{\partial\mathcal{L}}{\partial A_{ij}}A_{ij}-A_{ij}\sum_{k=1}^{m}\frac{\partial\mathcal{L}}{\partial A_{ik}}A_{ik}
	\end{align*}\end{split}\]</div>
</li>
</ul>
<ol class="arabic simple" start="4">
<li><p>Q&amp;K:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial Q}=\frac{\partial\mathcal{L}}{\partial S}\frac{K}{\sqrt{d_K}} \\
&amp;\frac{\partial\mathcal{L}}{\partial K}=\frac{\partial\mathcal{L}}{\partial S}^T\frac{Q}{\sqrt{d_K}}
\end{align*}\end{split}\]</div>
<ol class="arabic simple" start="5">
<li><p>Ws:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W_Q}=X^T\frac{\partial\mathcal{L}}{\partial Q}\\
&amp;\frac{\partial\mathcal{L}}{\partial W_K}=X^T\frac{\partial\mathcal{L}}{\partial K}\\
&amp;\frac{\partial\mathcal{L}}{\partial W_V}=X^T\frac{\partial\mathcal{L}}{\partial V}
\end{align*}\end{split}\]</div>
<ol class="arabic simple" start="6">
<li><p>X:</p></li>
</ol>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial X}=\frac{\partial\mathcal{L}}{\partial Q}W_Q^T+\frac{\partial\mathcal{L}}{\partial K}W_K^T+\frac{\partial\mathcal{L}}{\partial V}W_V^T
\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>⬆️ Computational cost <span class="math notranslate nohighlight">\(\leftarrow\)</span> <span class="math notranslate nohighlight">\(O(n^2)\)</span> (?)</p></li>
<li><p>Fixed sequence length.</p></li>
</ul>
<p><em>Why scale?</em></p>
<ol class="arabic simple">
<li><p>Dot product scales with dimension size.</p></li>
<li><p>Assume elements follow <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>, then dot product follows <span class="math notranslate nohighlight">\(\mathcal{N}(0,d_K)\)</span>.</p></li>
<li><p>Scaling normalizes this variance.</p></li>
</ol>
<p><em>Why softmax?</em></p>
<ul class="simple">
<li><p>Scores <span class="math notranslate nohighlight">\(\rightarrow\)</span> Probability distribution</p>
<ul>
<li><p>All weights &gt; 0.</p></li>
<li><p>All weights sum to 1.</p></li>
</ul>
</li>
<li><p>Score margins are amplified <span class="math notranslate nohighlight">\(\rightarrow\)</span> More attention to relevant elements</p></li>
</ul>
</div>
</section>
<section id="masked-causal-attention">
<h3>Masked/Causal Attention<a class="headerlink" href="#masked-causal-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Self-attention BUT each token can only see its previous tokens (and itself).</p></li>
<li><p><strong>Why</strong>: Autoregressive generation.</p></li>
<li><p><strong>How</strong>: For each token, mask attention scores of all future tokens to <span class="math notranslate nohighlight">\(-\infty\)</span> before softmax.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{softmax}(-\infty)\)</span>=0</p></li>
</ul>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Causal Attention:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(M\in\mathbb{R}^{m\times m}\)</span>: Mask matrix</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(M_{ij}=0\text{ if }i\geq j\)</span> else <span class="math notranslate nohighlight">\(-\infty\)</span></p></li>
</ul>
</li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Conditions?</em></p>
<ul class="simple">
<li><p>Only applicable in decoder.</p>
<ul>
<li><p>Encoder’s goal: Convert sequence into a meaningful representation.</p></li>
<li><p>Decoder’s goal: predict next token.</p></li>
</ul>
</li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Unidirectional context.</p></li>
<li><p>Limited context for early tokens.</p>
<ul>
<li><p>Token 1 only sees 1 token.</p></li>
<li><p>Token 2 only sees 2 tokens.</p></li>
<li><p>…</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="cross-attention">
<h3>Cross Attention<a class="headerlink" href="#cross-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Scaled Dot-Product Attention BUT</p>
<ul>
<li><p>K&amp;V <span class="math notranslate nohighlight">\(\leftarrow\)</span> Source (e.g., Encoder)</p></li>
<li><p>Q <span class="math notranslate nohighlight">\(\leftarrow\)</span> Curr sequence (i.e., Decoder)</p></li>
</ul>
</li>
<li><p><strong>Why</strong>: Additional source info may be helpful for predicting next token for curr sequence.</p></li>
<li><p><strong>How</strong>: See <a class="reference internal" href="#self-attention"><span class="xref myst">Self-Attention</span></a> but <span class="math notranslate nohighlight">\(K \&amp; V\)</span> are from Encoder.</p></li>
</ul>
</section>
<section id="multi-head-attention">
<h3>Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Multiple self-attention modules running in parallel.</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p>1 attention module <span class="math notranslate nohighlight">\(\xrightarrow{\text{monitor}}\)</span> 1 representation subspace</p></li>
<li><p>Language is complex: morphology, syntax, semantics, context, …</p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> attention modules <span class="math notranslate nohighlight">\(\xrightarrow{\text{monitor}}\)</span> <span class="math notranslate nohighlight">\(h\)</span> representation subspaces</p></li>
</ul>
</li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Each head takes the input token sequence and generates an output via self-attention.</p></li>
<li><p>Concatenate all outputs.</p></li>
<li><p>Linear transform to match embedding dim.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_O\in\mathbb{R}^{(h\cdot d_v)\times d_{\text{model}}}\)</span>: Weight matrix to transform concatenated head outputs.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(h\)</span>: #Heads.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{head}_i\in\mathbb{R}^{m\times d_V}\)</span>: Weighted contextual representation of input sequence from head <span class="math notranslate nohighlight">\(i\)</span>.</p></li>
</ul>
</li>
</ul>
<p>MHA:</p>
<div class="math notranslate nohighlight">
\[
\text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,\cdots,\text{head}_h)W_O
\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>⬆️ Computational cost</p></li>
<li><p>⬇️ Interpretability</p></li>
<li><p>Redundancy <span class="math notranslate nohighlight">\(\leftarrow\)</span> Some heads may learn similar patterns</p></li>
</ul>
</div>
<br>
</section>
</section>
<section id="encoder">
<h2>Encoder<a class="headerlink" href="#encoder" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>:</p>
<ol class="arabic simple">
<li><p>MHA + Residual Connection + LayerNorm</p></li>
<li><p>Position-wise FFN + Residual Connection + LayerNorm</p></li>
</ol>
</li>
<li><p><strong>Why</strong>: MHA merely forms a softmax-weighted linear blend of other tokens’ value vectors <span class="math notranslate nohighlight">\(\rightarrow\)</span> Where is non-liearity &amp; info complexity?</p>
<ul>
<li><p><strong>Non-linearity</strong>: Two-layer FFN with ReLU in between.</p></li>
<li><p><strong>Info complexity</strong>: Each token gets its own info processing stage: “Diffuse <span class="math notranslate nohighlight">\(\rightarrow\)</span> Activate <span class="math notranslate nohighlight">\(\rightarrow\)</span> Compress”.</p>
<ul>
<li><p>A mere weighted sum of value vectors <span class="math notranslate nohighlight">\(\rightarrow\)</span> Higher-order feature mixes.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p><strong>MHA</strong>: Immediately establish global context from input sequence BEFORE per-token processing.</p>
<ol class="arabic simple">
<li><p><strong>Residual Connection</strong>: Preserve the original signal &amp; Ensure training stability for deep stacks.</p></li>
<li><p><strong>LayerNorm</strong>: Re-center &amp; Rescale outputs to curb <a class="reference internal" href="#../dl/issues.md#internal-covariate-shift"><span class="xref myst">covariate shift</span></a>.</p></li>
</ol>
</li>
<li><p><strong>Position-wise FFN</strong>: Apply FFN independently <strong>to each token vector</strong> <span class="math notranslate nohighlight">\(\rightarrow\)</span> Transform features within each position’s channel dimension, w/o exchanging info across positions.</p>
<ol class="arabic simple">
<li><p><strong>Residual Connection</strong>: Preserve the original signal &amp; Ensure training stability for deep stacks.</p></li>
<li><p><strong>LayerNorm</strong>: Re-center &amp; Rescale outputs to curb <a class="reference internal" href="#../dl/issues.md#internal-covariate-shift"><span class="xref myst">covariate shift</span></a>.</p></li>
</ol>
</li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R^{d_{\text{model}}}}\)</span>: Input token vector.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_1\in\mathbb{R}^{d_{\text{model}}\times d_{\text{FFN}}}, \mathbf{b}_1\in\mathbb{R}^{d_{\text{FFN}}}\)</span>: Diffuse weights &amp; biases.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_2\in\mathbb{R}^{d_{\text{FFN}}\times d_{\text{model}}}, \mathbf{b}_2\in\mathbb{R}^{d_{\text{model}}}\)</span>: Compress weights &amp; biases.</p></li>
</ul>
</li>
</ul>
<p>Position-wise FFN:</p>
<div class="math notranslate nohighlight">
\[
FFN(\mathbf{x})=\max(0,\mathbf{x}W_1+\mathbf{b}_1)W_2+\mathbf{b}_2
\]</div>
</div>
</section>
<section id="decoder">
<h2>Decoder<a class="headerlink" href="#decoder" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>:</p>
<ol class="arabic simple">
<li><p>Masked MHA + Residual Connection + LayerNorm</p></li>
<li><p>(Optional) Cross MHA + Residual Connection + LayerNorm</p></li>
<li><p>Position-wise FFN + Residual Connection + LayerNorm</p></li>
</ol>
</li>
<li><p><strong>Why</strong>: See <a class="reference internal" href="#maskedcausal-attention"><span class="xref myst">Masked MHA</span></a>, <a class="reference internal" href="#cross-attention"><span class="xref myst">Cross MHA</span></a> and <a class="reference internal" href="#encoder"><span class="xref myst">Position-wise FFN</span></a>.</p>
<ul>
<li><p>BUT why cross after mask?</p>
<ul>
<li><p>The goal of decoder is Next Token Prediction.</p></li>
<li><p>In order to predict anything, we need the context first.</p></li>
<li><p>Masked MHA provides AR context for the target token.</p></li>
<li><p>THEN, we can add supplementary info from Cross MHA.</p></li>
<li><p>THEN, we do per-token processing.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
</section>
<section id="output">
<h2>Output<a class="headerlink" href="#output" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Embeddings <span class="math notranslate nohighlight">\(\rightarrow\)</span> Token probability distribution.</p></li>
<li><p><strong>Why</strong>: Next Token Prediction.</p></li>
<li><p><strong>How</strong>: Linear + Softmax.</p>
<ul>
<li><p><strong>Linear</strong>: Shape conversion: Embedding dim <span class="math notranslate nohighlight">\(\rightarrow\)</span> Vocab size.</p></li>
<li><p><strong>Softmax</strong>: Logits <span class="math notranslate nohighlight">\(\rightarrow\)</span> Probs</p></li>
</ul>
</li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><a class="reference external" href="https://jalammar.github.io/illustrated-transformer/">Jay Alammar’s Illustrated Transformer</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/pdf/1706.03762">The OG transformer paper</a></p></li>
<li><p><a class="reference external" href="https://huggingface.co/learn/llm-course/chapter1/1?fw=pt">HF’s LLM course</a></p></li>
<li><p><a class="reference external" href="https://wangkuiyi.github.io/positional_encoding.html">Yi Wang’s Positional Encoding</a></p></li>
</ul>
</section>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./nlp/transformer"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../../dl/quant.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Precision &amp; Quantization</p>
      </div>
    </a>
    <a class="right-next"
       href="tokenizer.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Tokenization</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding">Token Embedding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-causal-attention">Masked/Causal Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#encoder">Encoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#decoder">Decoder</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#output">Output</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Renyi Qu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>