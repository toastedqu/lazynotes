
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Layer &#8212; LazyNotes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dl/layer';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="RL for LLMs" href="../llm/rl.html" />
    <link rel="prev" title="LazyNotes" href="../intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="LazyNotes - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="LazyNotes - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Layer</a></li>





</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../llm/rl.html">RL for LLMs</a></li>

<li class="toctree-l1"><a class="reference internal" href="../llm/peft.html">PEFT</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fdl/layer.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/dl/layer.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Layer</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Layer</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#basic">Basic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input">Input</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding">Token Embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sinusoidal-pe">Sinusoidal PE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-causal-attention">Masked/Causal Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional">Convolutional</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#depthwise-separable-convolution">Depthwise Separable Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#atrous-dilated-convolution">Atrous/Dilated Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">Pooling</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent">Recurrent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gru">GRU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm">LSTM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bidirectional">Bidirectional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked">Stacked</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#activation">Activation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-like">Binary-like</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-units-rectified">Linear Units (Rectified)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lrelu">LReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prelu">PReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rrelu">RReLU</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-units-exponential">Linear Units (Exponential)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elu">ELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selu">SELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#celu">CELU</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-units-others">Linear Units (Others)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gelu">GELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#silu">SiLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softplus">Softplus</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass">Multiclass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmin">Softmin</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="layer">
<h1>Layer<a class="headerlink" href="#layer" title="Link to this heading">#</a></h1>
<p>A layer is a function that maps input tensor <span class="math notranslate nohighlight">\(X\)</span> to output tensor <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<p>Let <span class="math notranslate nohighlight">\(g\)</span> denote the gradient <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial y}\)</span> for readability.</p>
<br/>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="basic">
<h1>Basic<a class="headerlink" href="#basic" title="Link to this heading">#</a></h1>
<section id="linear">
<h2>Linear<a class="headerlink" href="#linear" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Linear transformation.</p></li>
<li><p><strong>Why</strong>: The simplest way to transform data &amp; learn patterns.</p></li>
<li><p><strong>How</strong>: input features * weights (+ bias).</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">Vector</label><div class="tab-content docutils">
<p><strong>Notations</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{H_{in}}\)</span>: Input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{H_{out}}\)</span>: Output vector.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W\in\mathbb{R}^{H_{out}\times H_{in}}\)</span>: Weight matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{b}\in\mathbb{R}^{H_{out}}\)</span>: Bias vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_{in}\)</span>: Input feature dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_{out}\)</span>: Output feature dimension.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\textbf{y}=W\textbf{x}+\textbf{b}
\]</div>
<p><strong>Backward</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W}=\textbf{g}\textbf{x}^T \\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{b}}=\textbf{g}\\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{x}}=W^T\textbf{g}
\end{align*}\end{split}\]</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Tensor</label><div class="tab-content docutils">
<p><strong>Notations</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{*\times H_{in}}\)</span>: Input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{*\times H_{out}}\)</span>: Output tensor.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W\in\mathbb{R}^{H_{out}\times H_{in}}\)</span>: Weight matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{b}\in\mathbb{R}^{H_{out}}\)</span>: Bias vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_{in}\)</span>: Input feature dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_{out}\)</span>: Output feature dimension.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\textbf{Y}=\textbf{X}W^T+\textbf{b}
\]</div>
<p><strong>Backward</strong>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W}=\textbf{g}^T\textbf{X} \\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{b}}=\sum_*\textbf{g}_*\\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{x}}=\textbf{g}W
\end{align*}\end{split}\]</div>
</div>
</div>
</div>
</section>
<section id="dropout">
<h2>Dropout<a class="headerlink" href="#dropout" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Randomly ignore some neurons during training.</p></li>
<li><p><strong>Why</strong>: To reduce overfitting.</p></li>
<li><p><strong>How</strong>: During training:</p>
<ol class="arabic simple">
<li><p>Randomly set a fraction of neurons to 0.</p></li>
<li><p>Scale the outputs/gradients on active neurons by the keep probability.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">Vector</label><div class="tab-content docutils">
<p><strong>Notations</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{H_{in}}\)</span>: Input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{H_{in}}\)</span>: Output vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: Keep probability.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textbf{m}\in\mathbb{R}^{H_{in}}\)</span>: binary mask, where each element <span class="math notranslate nohighlight">\(m\sim\text{Bernoulli}(p)\)</span>.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\textbf{y}=\frac{\textbf{m}\odot\textbf{x}}{p}
\]</div>
<p><strong>Backward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{x}} = \frac{\textbf{m}\odot\textbf{g}}{p}
\]</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">Tensor</label><div class="tab-content docutils">
<p><strong>Notations</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{*\times H_{in}}\)</span>: Input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{*\times H_{in}}\)</span>: Output tensor.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: Keep probability.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textbf{M}\in\mathbb{R}^{*\times H_{in}}\)</span>: binary mask, where each element <span class="math notranslate nohighlight">\(m\sim\text{Bernoulli}(p)\)</span>.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\textbf{Y}=\frac{\textbf{M}\odot\textbf{X}}{p}
\]</div>
<p><strong>Backward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{X}} = \frac{\textbf{M}\odot\textbf{g}}{p}
\]</div>
</div>
</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>⬆️Training time <span class="math notranslate nohighlight">\(\leftarrow\)</span> Longer convergence</p></li>
<li><p>Needs Hyperparameter Tuning</p></li>
</ul>
</div>
</section>
<section id="residual-connection">
<h2>Residual Connection<a class="headerlink" href="#residual-connection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Model the residual (<span class="math notranslate nohighlight">\(Y-X\)</span>) instead of the output (<span class="math notranslate nohighlight">\(Y\)</span>).</p></li>
<li><p><strong>Why</strong>: To mitigate <a class="reference internal" href="issues.html#vanishing/exploding-gradient"><span class="std std-ref">vanishing/exploding gradients</span></a>.</p></li>
<li><p><strong>How</strong>: Add input <span class="math notranslate nohighlight">\(X\)</span> to block output <span class="math notranslate nohighlight">\(F(X)\)</span>.</p>
<ul>
<li><p>If the feature dimension of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(F(X)\)</span> doesn’t match, use a shortcut linear layer on <span class="math notranslate nohighlight">\(X\)</span> to change its feature dimension.</p></li>
</ul>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--2-input--1" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--1">Vector</label><div class="tab-content docutils">
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{H_{in}}\)</span>: Input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{H_{out}}\)</span>: Output vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(\cdot)\in\mathbb{R}^{H_{out}}\)</span>: The aggregate function of all layers within the residual block.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\textbf{y}=F(\textbf{x})+\textbf{x}
\]</div>
<p><strong>Backward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{x}}=\mathbf{g}(1+\frac{\partial F(\textbf{x})}{\partial\textbf{x}})
\]</div>
</div>
<input class="tab-input" id="tab-set--2-input--2" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--2">Tensor</label><div class="tab-content docutils">
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{*\times H_{in}}\)</span>: Input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{*\times H_{out}}\)</span>: Output tensor.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(\cdot)\in\mathbb{R}^{H_{out}}\)</span>: The aggregate function of all layers within the residual block.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\textbf{Y}=F(\textbf{X})+\textbf{X}
\]</div>
<p><strong>Backward</strong>:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{X}}=\mathbf{g}(1+\frac{\partial F(\textbf{X})}{\partial\textbf{X}})
\]</div>
</div>
</div>
</div>
</section>
<section id="normalization">
<h2>Normalization<a class="headerlink" href="#normalization" title="Link to this heading">#</a></h2>
<section id="batch-normalization">
<h3>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Normalize each feature across input samples to zero mean &amp; unit variance.</p></li>
<li><p><strong>Why</strong>: To mitigate <a class="reference internal" href="issues.html#vanishing/internal-covariate-shift"><span class="std std-ref">internal covariate shift</span></a>.</p></li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Calculate the mean and variance for each batch.</p></li>
<li><p>Normalize the batch.</p></li>
<li><p>Scale and shift the normalized output using learnable params.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{m\times n}\)</span>: Input matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{m\times n}\)</span>: Output matrix.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\gamma\in\mathbb{R}\)</span>: Scale param.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\in\mathbb{R}\)</span>: Shift param.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<ol class="arabic">
<li><p>Calculate the mean and variance for each batch.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \boldsymbol{\mu}_B&amp;=\frac{1}{m}\sum_{i=1}^{m}\textbf{x}_i\\
    \boldsymbol{\sigma}_B^2&amp;=\frac{1}{m}\sum_{i=1}^{m}(\textbf{x}_i-\boldsymbol{\mu}_B)^2
    \end{align*}\end{split}\]</div>
</li>
<li><p>Normalize each batch.</p>
<div class="math notranslate nohighlight">
\[
    \textbf{z}_i=\frac{\textbf{x}_i-\boldsymbol{\mu}_B}{\sqrt{\boldsymbol{\sigma}_B^2+\epsilon}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant to avoid dividing by 0.</p>
</li>
<li><p>Scale and shift the normalized output.</p>
<div class="math notranslate nohighlight">
\[
    \textbf{y}_i=\gamma\textbf{z}_i+\beta
    \]</div>
</li>
</ol>
<p><strong>Backward</strong>:</p>
<ol class="arabic">
<li><p>Gradient w.r.t. params:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial\gamma}=\sum_{i=1}^{m}\textbf{g}_i\textbf{z}_i\\
    &amp;\frac{\partial\mathcal{L}}{\partial\beta}=\sum_{i=1}^{m}\textbf{g}_i
    \end{align*}\end{split}\]</div>
</li>
<li><p>Gradient w.r.t. input:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}=\gamma\textbf{g}_i\\
    &amp;\frac{\partial\mathcal{L}}{\partial\boldsymbol{\sigma}_B^2}=\sum_{i=1}^{m}\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}(\textbf{x}_i-\boldsymbol{\mu}_B)\left(-\frac{1}{2}(\boldsymbol{\sigma}_B^2+\epsilon)^{-\frac{3}{2}}\right)\\
    &amp;\frac{\partial\mathcal{L}}{\partial\boldsymbol{\mu}_B}=\sum_{i=1}^{m}\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}\cdot\left(-\frac{1}{\sqrt{\boldsymbol{\sigma}_B^2+\epsilon}}\right)+\frac{\partial\mathcal{L}}{\partial\boldsymbol{\sigma}_B^2}\cdot\left(-\frac{2}{m}\sum_{i=1}^{m}(\textbf{x}_i-\boldsymbol{\mu}_B)\right)\\
    &amp;\frac{\partial\mathcal{L}}{\partial\textbf{x}_i}=\frac{1}{\sqrt{\boldsymbol{\sigma}_B^2+\epsilon}}\left(\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}+\frac{2}{m}\frac{\partial\ma                                                                         thcal{L}}{\partial\boldsymbol{\sigma}_B^2}(\textbf{x}_i-\boldsymbol{\mu}_B)+\frac{1}{m}\frac{\partial\mathcal{L}}{\partial\boldsymbol{\mu}_B}\right)
    \end{align*}\end{split}\]</div>
</li>
</ol>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>Accelerates training with higher learning rates.</p></li>
<li><p>Reduces sensitivity to weight initialization.</p></li>
<li><p>Mitigates <a class="reference internal" href="issues.html#vanishing/exploding-gradient"><span class="std std-ref">vanishing/exploding gradients</span></a>.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Adds computation overhead and complexity.</p></li>
<li><p>Works best when each mini-batch is representative of the overall input distribution to accurately estimate the mean and variance.</p></li>
<li><p>Causes potential issues in certain cases like small mini-batches or when batch statistics differ from overall dataset statistics.</p></li>
</ul>
</div>
</section>
<section id="layer-normalization">
<h3>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Normalize each sample across the input features to zero mean and unit variance.</p></li>
<li><p><strong>Why</strong>: Batch normalization depends on the batch size.</p>
<ul>
<li><p>When it’s too big, high computational cost.</p></li>
<li><p>When it’s too small, the batch may not be representative of the underlying data distribution.</p></li>
<li><p>Hyperparam tuning is required to find the optimal batch size, leading to high computational cost.</p></li>
</ul>
</li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Calculate the mean and variance for each feature.</p></li>
<li><p>Normalize the feature.</p></li>
<li><p>Scale and shift the normalized output using learnable params.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>It’s easy to explain with the vector form for batch normalization, but it’s more intuitive to explain with the scalar form for layer normalization.</p>
<p><strong>Notations</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x_{ij}\in\mathbb{R}\)</span>: <span class="math notranslate nohighlight">\(j\)</span>th feature value for <span class="math notranslate nohighlight">\(i\)</span>th input sample.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{ij}\in\mathbb{R}\)</span>: <span class="math notranslate nohighlight">\(j\)</span>th feature value for <span class="math notranslate nohighlight">\(i\)</span>th output sample.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\in\mathbb{R}^n\)</span>: Scale param.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}\in\mathbb{R}^n\)</span>: Shift param.</p></li>
</ul>
</li>
</ul>
<p><strong>Forward</strong>:</p>
<ol class="arabic">
<li><p>Calculate the mean and variance for each feature.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \mu_i&amp;=\frac{1}{n}\sum_{j=1}^{n}x_{ij}\\
    \sigma_i^2&amp;=\frac{1}{n}\sum_{j=1}^{n}(x_{ij}-\mu_i)^2
    \end{align*}\end{split}\]</div>
</li>
<li><p>Normalize each feature.</p>
<div class="math notranslate nohighlight">
\[
    z_{ij}=\frac{x_{ij}-\mu_i}{\sqrt{\sigma_i^2+\epsilon}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant to avoid dividing by 0.</p>
</li>
<li><p>Scale and shift the normalized output.</p>
<div class="math notranslate nohighlight">
\[
    y_{ij}=\gamma_jz_{ij}+\beta_j
    \]</div>
</li>
</ol>
<p><strong>Backward</strong>:</p>
<ol class="arabic">
<li><p>Gradient w.r.t. params:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial\gamma_j}=\sum_{i=1}^{m}g_{ij}z_{ij}\\
    &amp;\frac{\partial\mathcal{L}}{\partial\beta_j}=\sum_{i=1}^{m}g_{ij}
    \end{align*}\end{split}\]</div>
</li>
<li><p>Gradient w.r.t. input:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial z_{ij}}=\gamma_jg_{ij}\\
    &amp;\frac{\partial\mathcal{L}}{\partial\sigma_i^2}=\sum_{j=1}^{n}\frac{\partial\mathcal{L}}{\partial z_{ij}}(x_{ij}-\mu_i)\left(-\frac{1}{2}(\sigma_i^2+\epsilon)^{-\frac{3}{2}}\right)\\
    &amp;\frac{\partial\mathcal{L}}{\partial\mu_i}=\sum_{j=1}^{n}\frac{\partial\mathcal{L}}{\partial z_{ij}}\cdot\left(-\frac{1}{\sqrt{\sigma_i^2+\epsilon}}\right)+\frac{\partial\mathcal{L}}{\partial\sigma_i^2}\cdot\left(-\frac{2}{n}\sum_{j=1}^{n}(x_{ij}-\mu_i)\right)\\
    &amp;\frac{\partial\mathcal{L}}{x_{ij}}=\frac{1}{\sqrt{\sigma_i^2+\epsilon}}\left(\frac{\partial\mathcal{L}}{\partial z_{ij}}+\frac{2}{n}\frac{\partial\mathcal{L}}{\partial\sigma_i^2}(x_{ij}-\mu_i)+\frac{1}{n}\frac{\partial\mathcal{L}}{\partial\mu_i}\right)
    \end{align*}\end{split}\]</div>
</li>
</ol>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros</em>:</p>
<ul class="simple">
<li><p>Reduces hyperparam tuning effort.</p></li>
<li><p>High consistency during training and inference.</p></li>
<li><p>Mitigates <a class="reference internal" href="issues.html#vanishing/exploding-gradient"><span class="std std-ref">vanishing/exploding gradients</span></a>..</p></li>
</ul>
<p><em>Cons</em>:</p>
<ul class="simple">
<li><p>Adds computation overhead and complexity.</p></li>
<li><p>Inapplicable in CNNs due to varied statistics of spatial features.</p></li>
</ul>
</div>
<br/>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="transformer">
<h1>Transformer<a class="headerlink" href="#transformer" title="Link to this heading">#</a></h1>
<!-- ```{image} ../../images/transformer.png
:align: center
:width: 500px
``` -->
<ul class="simple">
<li><p><strong>What</strong>: <strong>Self-attention</strong> for sequential data.</p></li>
<li><p><strong>Why</strong>: <strong>Long-range dependencies</strong> + <strong>Parallel processing</strong></p></li>
<li><p><strong>How</strong>:</p>
<ul>
<li><p>Input:</p>
<ol class="arabic simple">
<li><p><strong>Tokenization</strong>: Sequence <span class="math notranslate nohighlight">\(\xrightarrow{\text{split}}\)</span> Tokens</p></li>
<li><p><strong>Token Embedding</strong>: Tokens <span class="math notranslate nohighlight">\(\rightarrow\)</span> Semantic vectors</p></li>
<li><p><strong>Positional Encoding</strong>: Semantic vectors <span class="math notranslate nohighlight">\(\xrightarrow{+\text{positional info}}\)</span> Position-aware vectors</p></li>
</ol>
</li>
<li><p>Attention:</p>
<ol class="arabic simple">
<li><p><strong>Encoder</strong>: (Input) Position-aware vectors <span class="math notranslate nohighlight">\(\rightarrow\)</span> (Input) Context-aware vectors</p></li>
<li><p><strong>Decoder</strong>:</p>
<ul>
<li><p><strong>Encoder-Decoder Decoder</strong>: (Input) Context-aware vectors + (Output) Position-aware vectors <span class="math notranslate nohighlight">\(\rightarrow\)</span> (Output) Context-aware vectors</p></li>
<li><p><strong>Decoder-Only Decoder</strong>: (Input) Position-aware vectors <span class="math notranslate nohighlight">\(\rightarrow\)</span> (Input) Masked context-aware vectors</p></li>
</ul>
</li>
</ol>
</li>
<li><p>Output:</p>
<ol class="arabic simple">
<li><p><strong>Output Layer</strong>: (Output) Context-aware vectors <span class="math notranslate nohighlight">\(\xrightarrow{\text{predict}}\)</span> Next token</p></li>
</ol>
</li>
</ul>
</li>
</ul>
<p><br><br></p>
<section id="input">
<h2>Input<a class="headerlink" href="#input" title="Link to this heading">#</a></h2>
<section id="tokenization">
<h3>Tokenization<a class="headerlink" href="#tokenization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Sequence <span class="math notranslate nohighlight">\(\xrightarrow{\text{split}}\)</span> Tokens</p></li>
<li><p><strong>Why</strong>: Machines can only read numbers.</p></li>
<li><p><strong>How</strong>: (tbd)</p></li>
</ul>
</section>
<section id="token-embedding">
<h3>Token Embedding<a class="headerlink" href="#token-embedding" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Tokens <span class="math notranslate nohighlight">\(\rightarrow\)</span> Semantic vectors.</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p>Discrete <span class="math notranslate nohighlight">\(\rightarrow\)</span> Continuous</p></li>
<li><p>Vocab index <span class="math notranslate nohighlight">\(\rightarrow\)</span> Semantic meaning</p></li>
<li><p>Vocab size <span class="math notranslate nohighlight">\(\xrightarrow{\text{reduced to}}\)</span> hidden size</p></li>
</ul>
</li>
<li><p><strong>How</strong>: Look-up table / <a class="reference internal" href="#../basics.md#linear"><span class="xref myst">Linear</span></a>.</p></li>
</ul>
</section>
<section id="positional-encoding">
<h3>Positional Encoding<a class="headerlink" href="#positional-encoding" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Semantic vectors <span class="math notranslate nohighlight">\(\xrightarrow{+\text{positional info}}\)</span> Position-aware vectors</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p>Transformers don’t know positions.</p></li>
<li><p>BUT positions matter!</p>
<ul>
<li><p>No PE <span class="math notranslate nohighlight">\(\rightarrow\)</span> self-attention scores remain unchanged regardless of token orders <span id="id1">[<a class="reference internal" href="../references.html#id11" title="Kuiyi Wang. Why transformer models need positional encoding. https://wangkuiyi.github.io/positional_encoding.html. [Online; accessed &lt;today&gt;].">7</a>]</span>.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<section id="sinusoidal-pe">
<h4>Sinusoidal PE<a class="headerlink" href="#sinusoidal-pe" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>What</strong>: Positional info <span class="math notranslate nohighlight">\(\rightarrow\)</span> Sine waves</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p>Continuous &amp; multi-scale <span class="math notranslate nohighlight">\(\rightarrow\)</span> Generalize to sequences of arbitrary lengths</p></li>
<li><p>No params <span class="math notranslate nohighlight">\(\rightarrow\)</span> Low computational cost</p></li>
<li><p>Empirically performed as well as learned PE</p></li>
</ul>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Sinusoidal PE:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;PE_{(pos, 2i)}=\sin\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right) \\
&amp;PE_{(pos, 2i+1)}=\cos\left(\frac{pos}{10000^{\frac{2i}{d_{\text{model}}}}}\right)
\end{align*}\end{split}\]</div>
<ul class="simple">
<li><p>Input:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(pos\in\mathbb{R}\)</span>: Token position.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(i\)</span>: Embedding dimension index.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_{\text{model}}\)</span>: Embedding dimension.</p></li>
</ul>
</li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>No params <span class="math notranslate nohighlight">\(\rightarrow\)</span> No learning of task-specific position patterns.</p></li>
<li><p>Requires uniform token importance across the sequence. <span id="id2">[<a class="reference internal" href="../references.html#id12" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 2017.">6</a>]</span></p></li>
<li><p>Cannot capture complex, relative, or local positional relationships.</p></li>
</ul>
</div>
<p><br><br></p>
</section>
</section>
</section>
<section id="attention">
<h2>Attention<a class="headerlink" href="#attention" title="Link to this heading">#</a></h2>
<section id="self-attention">
<h3>Self-Attention<a class="headerlink" href="#self-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Each element in the sequence pays attention to each other.</p></li>
<li><p><strong>Why</strong>: <strong>Long-range dependencies</strong> + <strong>Parallel processing</strong></p></li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>All elements <span class="math notranslate nohighlight">\(\rightarrow\)</span> QKV</p>
<ul>
<li><p>Q: What are you looking for?</p></li>
<li><p>K: What are your keywords for search?</p></li>
<li><p>V: What info do you have?</p></li>
</ul>
</li>
<li><p>For each token T:</p>
<ol class="arabic simple">
<li><p>T’s Query &amp; All Keys <span class="math notranslate nohighlight">\(\rightarrow\)</span> Relevance scores</p></li>
<li><p><span class="math notranslate nohighlight">\(\rightarrow\)</span> Attention weights</p></li>
<li><p><span class="math notranslate nohighlight">\(\rightarrow\)</span> Weighted sum of T’s Value (i.e., T’s contextual representation)</p></li>
</ol>
</li>
</ol>
</li>
</ul>
<details class="sd-sphinx-override sd-dropdown sd-card sd-mb-3">
<summary class="sd-summary-title sd-card-header">
<span class="sd-summary-text">ELI5</span><span class="sd-summary-state-marker sd-summary-chevron-right"><svg version="1.1" width="1.5em" height="1.5em" class="sd-octicon sd-octicon-chevron-right" viewBox="0 0 24 24" aria-hidden="true"><path d="M8.72 18.78a.75.75 0 0 1 0-1.06L14.44 12 8.72 6.28a.751.751 0 0 1 .018-1.042.751.751 0 0 1 1.042-.018l6.25 6.25a.75.75 0 0 1 0 1.06l-6.25 6.25a.75.75 0 0 1-1.06 0Z"></path></svg></span></summary><div class="sd-summary-content sd-card-body docutils">
<p class="sd-card-text">You are in a top AI conference.</p>
<p class="sd-card-text">Each guy is an element.</p>
<p class="sd-card-text">You have some dumb question in mind. (Q)</p>
<p class="sd-card-text">Each guy has their badges and posters with titles and metadata. (K)</p>
<p class="sd-card-text">Each guy knows the details of their projects. (V)</p>
<p class="sd-card-text">You walk around &amp; check out the whole venue.</p>
<p class="sd-card-text">You see topics that you don’t really care. You skim &amp; skip.</p>
<p class="sd-card-text">You see topics that are related to your question. You talk to the guys to learn more.</p>
<p class="sd-card-text">You see topics that you are obsessed with. You ask the guys a billion follow-up questions and memorize every single technical detail of their Github implementation.</p>
<p class="sd-card-text">The conference ends.</p>
<p class="sd-card-text">You have learnt something about everything, but not everything weighs the same in your heart.</p>
</div>
</details><div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Scaled Dot-Product Attention:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q,K,V)=\text{softmax}\left(\frac{QK^T}{\sqrt{d_K}}\right)V
\]</div>
<ul class="simple">
<li><p>Input:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(X\in\mathbb{R}^{m\times n}\)</span>: Input sequence</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_Q\in\mathbb{R}^{n\times d_K}\)</span>: Weight matrix for Q.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_K\in\mathbb{R}^{n\times d_K}\)</span>: Weight matrix for K.</p></li>
<li><p><span class="math notranslate nohighlight">\(W_V\in\mathbb{R}^{n\times d_V}\)</span>: Weight matrix for V.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(m\)</span>: #Tokens.</p></li>
<li><p><span class="math notranslate nohighlight">\(n\)</span>: #Features/Hidden size.</p></li>
<li><p><span class="math notranslate nohighlight">\(d_K\)</span>: Hidden size of Q &amp; K.</p>
<ul>
<li><p>Q &amp; K share the same hidden size for matrix multiplication.</p></li>
</ul>
</li>
<li><p><span class="math notranslate nohighlight">\(d_V\)</span>: Hidden size of V.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(Q=XW_Q\in\mathbb{R}^{m\times d_K}\)</span>: Q vectors for all elems.</p></li>
<li><p><span class="math notranslate nohighlight">\(K=XW_K\in\mathbb{R}^{m\times d_K}\)</span>: K vectors for all elems.</p></li>
<li><p><span class="math notranslate nohighlight">\(V=XW_V\in\mathbb{R}^{m\times d_V}\)</span>: V vectors for all elems.</p></li>
</ul>
</li>
</ul>
</div>
<div class="important dropdown admonition">
<p class="admonition-title">Derivation (Backprop)</p>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(S=\frac{QK^T}{\sqrt{d_K}}\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(A=\text{softmax}(S)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(Y=AV\)</span></p></li>
</ul>
<p>STEP 1 - V:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial V}=A^T\frac{\partial L}{\partial Y}
\]</div>
<p>STEP 2 - A:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial A}=\frac{\partial L}{\partial Y}V^T
\]</div>
<p>STEP 3 - S:</p>
<ul>
<li><p>Recall that for <span class="math notranslate nohighlight">\(\mathbf{a}=\text{softmax}(\mathbf{s})\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    \frac{\partial a_i}{\partial s_j}=a_i(\delta_{ij}-a_j)
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\delta_{ij}=1\text{ if }i=j\text{ else }0\)</span>.</p>
</li>
<li><p>For each row <span class="math notranslate nohighlight">\(i\)</span> of <span class="math notranslate nohighlight">\(S\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \frac{\partial L}{\partial S_{ij}}&amp;=\sum_{k=1}^{m}\frac{\partial L}{\partial A_{ik}}\frac{\partial A_{ik}}{\partial S_{ij}} \\
    &amp;=\frac{\partial L}{\partial A_{ij}}A_{ij}-A_{ij}\frac{\partial L}{\partial A_{ij}}A_{ij}-A_{ij}\sum_{k\neq j}\frac{\partial L}{\partial A_{ik}}A_{ik} \\
    &amp;=\frac{\partial L}{\partial A_{ij}}A_{ij}-A_{ij}\sum_{k=1}^{m}\frac{\partial L}{\partial A_{ik}}A_{ik}
    \end{align*}\end{split}\]</div>
</li>
</ul>
<p>STEP 4 - Q&amp;K:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial L}{\partial Q}=\frac{\partial L}{\partial S}\frac{K}{\sqrt{d_K}} \\
&amp;\frac{\partial L}{\partial K}=\frac{\partial L}{\partial S}^T\frac{Q}{\sqrt{d_K}}
\end{align*}\end{split}\]</div>
<p>STEP 5 - Ws:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial L}{\partial W_Q}=X^T\frac{\partial L}{\partial Q}\\
&amp;\frac{\partial L}{\partial W_K}=X^T\frac{\partial L}{\partial K}\\
&amp;\frac{\partial L}{\partial W_V}=X^T\frac{\partial L}{\partial V}
\end{align*}\end{split}\]</div>
<p>STEP 6 - X:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial L}{\partial X}=\frac{\partial L}{\partial Q}W_Q^T+\frac{\partial L}{\partial K}W_K^T+\frac{\partial L}{\partial V}W_V^T
\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>⬆️ Computational cost <span class="math notranslate nohighlight">\(\leftarrow\)</span> <span class="math notranslate nohighlight">\(O(n^2)\)</span> (?)</p></li>
<li><p>Fixed sequence length.</p></li>
</ul>
<p><em>Why scale?</em></p>
<ol class="arabic simple">
<li><p>Dot product scales with dimension size.</p></li>
<li><p>Assume elements follow <span class="math notranslate nohighlight">\(\mathcal{N}(0,1)\)</span>, then dot product follows <span class="math notranslate nohighlight">\(\mathcal{N}(0,d_K)\)</span>.</p></li>
<li><p>Scaling normalizes this variance.</p></li>
</ol>
<p><em>Why softmax?</em></p>
<ul class="simple">
<li><p>Scores <span class="math notranslate nohighlight">\(\rightarrow\)</span> Probability distribution</p>
<ul>
<li><p>All weights &gt; 0.</p></li>
<li><p>All weights sum to 1.</p></li>
</ul>
</li>
<li><p>Score margins are amplified <span class="math notranslate nohighlight">\(\rightarrow\)</span> More attention to relevant elements</p></li>
</ul>
</div>
</section>
<section id="masked-causal-attention">
<h3>Masked/Causal Attention<a class="headerlink" href="#masked-causal-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Self-attention BUT each token can only see its previous tokens (and itself).</p></li>
<li><p><strong>Why</strong>: Autoregressive generation.</p></li>
<li><p><strong>How</strong>: For each token, mask attention scores of all future tokens to <span class="math notranslate nohighlight">\(-\infty\)</span> before softmax.</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{softmax}(-\infty)\)</span>=0</p></li>
</ul>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Causal Attention:</p>
<div class="math notranslate nohighlight">
\[
\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}} + M\right)V
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(M\in\mathbb{R}^{m\times m}\)</span>: Mask matrix</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(M_{ij}=0\text{ if }i\geq j\)</span> else <span class="math notranslate nohighlight">\(-\infty\)</span></p></li>
</ul>
</li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Conditions?</em></p>
<ul class="simple">
<li><p>Only applicable in decoder.</p>
<ul>
<li><p>Main goal of encoder: convert sequence into a meaningful representation.</p></li>
<li><p>Main goal of decoder: predict next token.</p></li>
</ul>
</li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Unidirectional context.</p></li>
<li><p>Limited context for early tokens.</p>
<ul>
<li><p>Token 1 only sees 1 token.</p></li>
<li><p>Token 2 only sees 2 tokens.</p></li>
<li><p>…</p></li>
</ul>
</li>
</ul>
</div>
</section>
<section id="cross-attention">
<h3>Cross Attention<a class="headerlink" href="#cross-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Scaled Dot-Product Attention BUT</p>
<ul>
<li><p>K&amp;V <span class="math notranslate nohighlight">\(\leftarrow\)</span> Source (e.g., Encoder)</p></li>
<li><p>Q <span class="math notranslate nohighlight">\(\leftarrow\)</span> Current sequence (i.e., Decoder)</p></li>
</ul>
</li>
<li><p><strong>Why</strong>: Additional source info may be helpful for predicting next token for current sequence.</p></li>
<li><p><strong>How</strong>: See <a class="reference internal" href="#self-attention"><span class="xref myst">Self-Attention</span></a>.</p></li>
</ul>
</section>
<section id="multi-head-attention">
<h3>Multi-Head Attention<a class="headerlink" href="#multi-head-attention" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Multiple self-attention modules running in parallel.</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(1\)</span> attention module <span class="math notranslate nohighlight">\(\xrightarrow{\text{monitor}}\)</span> <span class="math notranslate nohighlight">\(1\)</span> representation subspace</p></li>
<li><p>Language is complex: morphology, syntax, semantics, context, …</p></li>
<li><p><span class="math notranslate nohighlight">\(h\)</span> attention modules <span class="math notranslate nohighlight">\(\xrightarrow{\text{monitor}}\)</span> <span class="math notranslate nohighlight">\(h\)</span> representation subspaces</p></li>
</ul>
</li>
<li><p><strong>How</strong>: Each head <span class="math notranslate nohighlight">\(\xrightarrow{\text{self-attention}}\)</span> Each output <span class="math notranslate nohighlight">\(\xrightarrow{\text{concatenate}}\)</span> All outputs <span class="math notranslate nohighlight">\(\xrightarrow{\text{linear transform}}\)</span> Final output</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>MHA:</p>
<div class="math notranslate nohighlight">
\[
\text{MultiHead}(Q,K,V)=\text{Concat}(\text{head}_1,\cdots,\text{head}_h)W_O
\]</div>
<ul class="simple">
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W_O\)</span>: Weight matrix to transform concatenated head outputs.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(h\)</span>: #Heads.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\text{head}_i\in\mathbb{R}^{m\times d_V}\)</span>: Weighted representation of input sequence from the <span class="math notranslate nohighlight">\(i\)</span>th head.</p></li>
</ul>
</li>
</ul>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>⬆️ Computational cost</p></li>
<li><p>⬇️ Interpretability</p></li>
<li><p>Redundancy <span class="math notranslate nohighlight">\(\leftarrow\)</span> some heads may learn similar patterns</p></li>
</ul>
</div>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convolutional">
<h1>Convolutional<a class="headerlink" href="#convolutional" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>What</strong>: Apply a set of filters to input data to extract local features. (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf">paper</a>)</p></li>
<li><p><strong>Why</strong>: To learn spatial hierarchies of features.</p></li>
<li><p><strong>How</strong>: Slide multiple filters/kernel (i.e., small matrices) over the input data.</p>
<ul>
<li><p>At each step, perform element-wise multiplication and summation between each filter and the scanned area, producing a feature map.</p></li>
</ul>
</li>
<li><p><strong>When</strong>: Used with grid-like data such as images and video frames.</p></li>
<li><p><strong>Where</strong>: Computer Vision.</p></li>
<li><p><strong>Pros</strong>:</p>
<ul>
<li><p>Translation invariance.</p></li>
<li><p>Efficiently captures spatial hierarchies.</p></li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>
<ul>
<li><p>High computational cost for big data</p></li>
<li><p>Requires big data to be performant.</p></li>
<li><p>Requires extensive hyperparam tuning.</p></li>
</ul>
</li>
</ul>
<!-- ```{admonition} Math
:class: note, dropdown
**Notations**:
    - IO:
        - $\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}$: Input volume.
        - $\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{out}}$: Output volume.
    - Params:
        - $\mathbf{W}\in\mathbb{R}^{F_{H}\times F_{W}\times C_{out}\times C_{in}}$: Filters.
        - $\mathbf{b}\in\mathbb{R}^{C_{out}}$: Biases.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $C_{out}$: #Filters (i.e., #Output channels).
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
        - $p$: Padding size.
**Forward**:

    $$
    Y_{h,w,c_{out}}=\sum_{c_{in}=1}^{C_{in}}\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\cdot X_{sh+i-p,sw+j-p,c_{in}}+b_{c_{out}}
    $$

    where

    $$\begin{align*}
    H_{out}&=\left\lfloor\frac{H_{in}+2p-f_h}{s}\right\rfloor+1\\
    W_{out}&=\left\lfloor\frac{W_{in}+2p-f_w}{s}\right\rfloor+1
    \end{align*}$$

**Backward**:

    $$\begin{align*}
    &\frac{\partial\mathcal{L}}{\partial W_{i,j,c_{out},c_{in}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\cdot X_{sh+i-p, sw+j-p, c_{in}}\\
    &\frac{\partial\mathcal{L}}{\partial b_{c_{out}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\
    &\frac{\partial\mathcal{L}}{\partial X_{i,j,c_{in}}}=\sum_{c_{out}=1}^{C_{out}}\sum_{h=1}^{f_h}\sum_{w=1}^{f_w}g_{h,w,c_{out}}\cdot W_{i-sh+p,j-sw+p,c_{out},c_{in}}
    \end{align*}$$

    Notice it is similar to backprop of linear layer except it sums over the scanned area and removes padding.
``` -->
<section id="depthwise-separable-convolution">
<h2>Depthwise Separable Convolution<a class="headerlink" href="#depthwise-separable-convolution" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Depthwise convolution + Pointwise convolution. (<a class="reference external" href="https://arxiv.org/pdf/1610.02357">paper</a>)</p></li>
<li><p><strong>Why</strong>: To significantly reduce computational cost and #params.</p></li>
<li><p><strong>How</strong>:</p>
<ul>
<li><p><strong>Depthwise</strong>: Use a single filter independently per channel.</p></li>
<li><p><strong>Pointwise</strong>: Use Conv1d to combine the outputs of depthwise convolution.</p></li>
</ul>
</li>
<li><p><strong>When</strong>: When computational efficiency and model size are crucial.</p></li>
<li><p><strong>Where</strong>: <a class="reference external" href="https://arxiv.org/pdf/1704.04861">MobileNets</a>, <a class="reference external" href="https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf">Xception</a>, etc.</p></li>
<li><p><strong>Pros</strong>: Significantly higher computational efficiency (time &amp; space).</p></li>
<li><p><strong>Cons</strong>: Lower accuracy.</p></li>
</ul>
<!-- ```{admonition} Math
:class: note, dropdown
**Notations**:
    - IO:
        - $\mathbf{X} \in \mathbb{R}^{H_{in} \times W_{in} \times C_{in}}$: Input volume.
        - $\mathbf{Y} \in \mathbb{R}^{H_{out} \times W_{out} \times C_{out}}$: Output volume.
    - Params:
        - $\mathbf{W^d} \in \mathbb{R}^{f_h \times f_w \times C_{in}}$: Depthwise filters.
        - $\mathbf{b^d} \in \mathbb{R}^{C_{in}}$: Depthwise biases.
        - $\mathbf{W^p} \in \mathbb{R}^{1 \times 1 \times C_{in} \times C_{out}}$: Pointwise filters.
        - $\mathbf{b^p} \in \mathbb{R}^{C_{out}}$: Pointwise biases.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $C_{out}$: #Output channels.
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
        - $p$: Padding size.
**Forward**:
    1. Depthwise convolution: Calculate $\mathbf{Z} \in \mathbb{R}^{H_{out} \times W_{out} \times C_{in}}$:

        $$
        Z_{h,w,c_{in}} = \sum_{i=1}^{f_h} \sum_{j=1}^{f_w} W^d_{i,j,c_{in}} \cdot X_{sh+i-p, sw+j-p, c_{in}} + b^d_{c_{in}}
        $$

    2. Pointwise convolution:

        $$
        Y_{h,w,c_{out}} = \sum_{c_{in}=1}^{C_{in}} W^p_{1,1,c_{in},c_{out}} \cdot Z_{h,w,c_{in}} + b^p_{c_{out}}
        $$
        where
        $$\begin{align*}
        H_{out} &= \left\lfloor \frac{H_{in} + 2p - f_h}{s} \right\rfloor + 1 \\
        W_{out} &= \left\lfloor \frac{W_{in} + 2p - f_w}{s} \right\rfloor + 1
        \end{align*}$$

**Backward**:
    1. Pointwise convolution: Let $g^{p}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{out}}$ be $\frac{\partial\mathcal{L}}{\partial\mathbf{Y}}$.

        $$\begin{align*}
        &\frac{\partial \mathcal{L}}{\partial W^p_{1,1,c_{in},c_{out}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}} \cdot Z_{h,w,c_{in}}\\
        &\frac{\partial \mathcal{L}}{\partial b^p_{c_{out}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}}\\
        &\frac{\partial \mathcal{L}}{\partial Z_{h,w,c_{in}}} = \sum_{c_{out}=1}^{C_{out}} g^{p}_{h,w,c_{out}} \cdot W^p_{1,1,c_{in},c_{out}}
        \end{align*}$$
    2. Depthwise convolution: Let $g^{d}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{in}}$ be $\frac{\partial\mathcal{L}}{\partial\mathbf{Z}}$.

        $$\begin{align*}
        &\frac{\partial \mathcal{L}}{\partial W^d_{i,j,c_{in}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}} \cdot X_{sh+i-p, sw+j-p, c_{in}}\\
        &\frac{\partial \mathcal{L}}{\partial b_{d,c_{in}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}}\\
        &\frac{\partial \mathcal{L}}{\partial X_{i,j,c_{in}}} = \sum_{h=1}^{f_h} \sum_{w=1}^{f_w} g^d_{h,w,c_{in}} \cdot W^d_{i-sh+p,j-sw+p,c_{in}}
        \end{align*}$$
``` -->
</section>
<section id="atrous-dilated-convolution">
<h2>Atrous/Dilated Convolution<a class="headerlink" href="#atrous-dilated-convolution" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Add holes between filter elements (i.e., dilation). (<a class="reference external" href="https://arxiv.org/pdf/1511.07122">paper</a>)</p></li>
<li><p><strong>Why</strong>: The filters can capture larger contextual info without increasing #params.</p></li>
<li><p><strong>How</strong>: Introduce a dilation rate <span class="math notranslate nohighlight">\(r\)</span> to determine the space between the filter elements. Then compute convolution accordingly.</p></li>
<li><p><strong>When</strong>: When understanding the broader context is important.</p></li>
<li><p><strong>Where</strong>: Semantic image segmentation, object detection, depth estimation, optical flow estimation, etc.</p></li>
<li><p><strong>Pros</strong>:</p>
<ul>
<li><p>Larger receptive fields without increasing #params.</p></li>
<li><p>Captures multi-scale info without upsampling layers.</p></li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>
<ul>
<li><p>Requires very careful hyperparam tuning, or info loss.</p></li>
</ul>
</li>
</ul>
<!-- ```{admonition} Math
:class: note, dropdown
**Notations**:
    - IO:
        - $\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}$: Input volume.
        - $\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{out}}$: Output volume.
    - Params:
        - $\mathbf{W}\in\mathbb{R}^{F_{H}\times F_{W}\times C_{out}\times C_{in}}$: Filters.
        - $\mathbf{b}\in\mathbb{R}^{C_{out}}$: Biases.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $C_{out}$: #Filters (i.e., #Output channels).
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
        - $p$: Padding size.
        - $r$: Dilation rate.
**Forward**:
    1. (optional) Pad input tensor: $\mathbf{X}^\text{pad}\in\mathbb{R}^{(H_{in}+2p)\times (W_{in}+2p)\times C_{in}}$
    2. Perform element-wise multiplication (i.e., convolution):

        $$
        Y_{h,w,c_{out}}=\sum_{c_{in}=1}^{C_{in}}\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\cdot X_{sh+r(i-1)-p,sw+r(j-1)-p,c_{in}}+b_{c_{out}}
        $$

        where

        $$\begin{align*}
        H_{out}&=\left\lfloor\frac{H_{in}+2p-r(f_h-1)-1}{s}\right\rfloor+1\\
        W_{out}&=\left\lfloor\frac{W_{in}+2p-r(f_w-1)-1}{s}\right\rfloor+1
        \end{align*}$$

**Backward**:

    $$\begin{align*}
    &\frac{\partial\mathcal{L}}{\partial W_{i,j,c_{out},c_{in}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\cdot X_{sh+r(i-1)-p, sw+r(j-1)-p, c_{in}}\\
    &\frac{\partial\mathcal{L}}{\partial b_{c_{out}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\
    &\frac{\partial\mathcal{L}}{\partial X_{i,j,c_{in}}}=\sum_{c_{out}=1}^{C_{out}}\sum_{h=1}^{f_h}\sum_{w=1}^{f_w}g_{h,w,c_{out}}\cdot W_{r(i-1)-sh+p,r(j-1)-sw+p,c_{out},c_{in}}
    \end{align*}$$
``` -->
</section>
<section id="pooling">
<h2>Pooling<a class="headerlink" href="#pooling" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Convolution but (<a class="reference external" href="https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf">paper</a>)</p>
<ul>
<li><p>computes a heuristic per scanned patch.</p></li>
<li><p>uses the same #channels.</p></li>
</ul>
</li>
<li><p><strong>Why</strong>: Dimensionality reduction while preserving dominant features.</p></li>
<li><p><strong>How</strong>: Slide the pooling window over the input &amp; apply the heuristic within the scanned patch.</p>
<ul>
<li><p><strong>Max</strong>: Output the maximum value from each patch.</p></li>
<li><p><strong>Average</strong>: Output the average value of each patch.</p></li>
</ul>
</li>
<li><p><strong>When</strong>: When downsampling is necessary.</p></li>
<li><p><strong>Where</strong>: After convolutional layer.</p></li>
<li><p><strong>Pros</strong>:</p>
<ul>
<li><p>Significantly higher computational efficiency (time &amp; space).</p></li>
<li><p>No params to train.</p></li>
<li><p>Reduces overfitting.</p></li>
<li><p>Preserves translation invariance without losing too much info.</p></li>
<li><p>High robustness.</p></li>
</ul>
</li>
<li><p><strong>Cons</strong>:</p>
<ul>
<li><p>Slight spatial info loss.</p></li>
<li><p>Requires hyperparam tuning.</p>
<ul>
<li><p>Large filter or stride results in coarse features.</p></li>
</ul>
</li>
</ul>
</li>
<li><p><strong>Max vs Average</strong>:</p>
<ul>
<li><p><strong>Max</strong>: Captures most dominant features; higher robustness.</p></li>
<li><p><strong>Avg</strong>: Preserves more info; provides smoother features; dilutes the importance of dominant features.</p></li>
</ul>
</li>
</ul>
<!-- ```{admonition} Math
:class: note, dropdown
**Notations**:
    - IO:
        - $\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}$: Input volume.
        - $\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{in}}$: Output volume.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
**Forward**:

    $$\begin{array}{ll}
    \text{Max:} & Y_{h,w,c}=\max_{i=1,\cdots,f_h\ |\ j=1,\cdots,f_w}X_{sh+i,sw+j,c}\\
    \text{Avg:} & Y_{h,w,c}=\frac{1}{f_hf_w}\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}X_{sh+i,sw+j,c}
    \end{array}$$

    where

    $$\begin{align*}
    H_{out}&=\left\lfloor\frac{H_{in}-f_h}{s}\right\rfloor+1\\
    W_{out}&=\left\lfloor\frac{W_{in}-f_h}{s}\right\rfloor+1
    \end{align*}$$

**Backward**:

    $$\begin{array}{ll}
    \text{Max:} & \frac{\partial\mathcal{L}}{\partial X_{sh+i,sw+j,c}}=g_{h,w,c}\text{ if }X_{sh+i,sw+j,c}=Y_{h,w,c}\\
    \text{Avg:} & \frac{\partial\mathcal{L}}{\partial X_{sh+i,sw+j,c}}=\frac{g_{h,w,c}}{f_hf_w}
    \end{array}$$

    - Max: Gradients only propagate to the max element of each window.
    - Avg: Gradients are equally distributed among all elements in each window.
``` -->
<br/>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="recurrent">
<h1>Recurrent<a class="headerlink" href="#recurrent" title="Link to this heading">#</a></h1>
<a class="reference internal image-reference" href="images/RNN.png"><img alt="images/RNN.png" class="align-center" src="images/RNN.png" style="width: 400px;" />
</a>
<div class="math notranslate nohighlight">
\[
h_t=\tanh(x_tW_{xh}^T+h_{t-1}W_{hh}^T)
\]</div>
<p>Idea: <strong>recurrence</strong> - maintain a hidden state that captures information about previous inputs in the sequence</p>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( x_t\)</span>: input at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{in}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( h_t\)</span>: hidden state at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((D,m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( W_{xh}\)</span>: weight matrix of shape <span class="math notranslate nohighlight">\((H_{out},H_{in})\)</span> if initial layer, else <span class="math notranslate nohighlight">\((H_{out},DH_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( W_{hh}\)</span>: weight matrix of shape <span class="math notranslate nohighlight">\((H_{out},H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( H_{in}\)</span>: input size, #features in <span class="math notranslate nohighlight">\(x_t \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( H_{out}\)</span>: hidden size, #features in <span class="math notranslate nohighlight">\(h_t \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( m \)</span>: batch size</p></li>
<li><p><span class="math notranslate nohighlight">\( D\)</span>: <span class="math notranslate nohighlight">\(=2\)</span> if bi-directional else <span class="math notranslate nohighlight">\(1 \)</span></p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>Short-term memory: hard to carry info from earlier steps to later ones if long seq</p></li>
<li><p>Vanishing gradient: gradients in earlier parts become extremely small if long seq</p></li>
</ul>
<section id="gru">
<h2>GRU<a class="headerlink" href="#gru" title="Link to this heading">#</a></h2>
<a class="reference internal image-reference" href="images/GRU.png"><img alt="images/GRU.png" class="align-center" src="images/GRU.png" style="width: 400px;" />
</a>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;r_t=\sigma(x_tW_{xr}^T+h_{t-1}W_{hr}^T) \\\\
&amp;z_t=\sigma(x_tW_{xz}^T+h_{t-1}W_{hz}^T) \\\\
&amp;\tilde{h}\_t=\tanh(x_tW_{xn}^T+r_t\odot(h_{t-1}W_{hn}^T)) \\\\
&amp;h_t=(1-z_t)\odot\tilde{h}\_t+z_t\odot h_{t-1}
\end{align*}\end{split}\]</div>
<p>Idea: Gated Recurrent Unit - use 2 gates to address long-term info propagation issue in RNN:</p>
<ol class="arabic simple">
<li><p><strong>Reset gate</strong>: determine how much of <span class="math notranslate nohighlight">\( h_{t-1}\)</span> should be ignored when computing <span class="math notranslate nohighlight">\(\tilde{h}\_t \)</span>.</p></li>
<li><p><strong>Update gate</strong>: determine how much of <span class="math notranslate nohighlight">\( h_{t-1}\)</span> should be retained for <span class="math notranslate nohighlight">\(h_t \)</span>.</p></li>
<li><p><strong>Candidate</strong>: calculate candidate <span class="math notranslate nohighlight">\( \tilde{h}\_t\)</span> with reset <span class="math notranslate nohighlight">\(h_{t-1} \)</span>.</p></li>
<li><p><strong>Final</strong>: calculate weighted average between candidate <span class="math notranslate nohighlight">\( \tilde{h}\_t\)</span> and prev state <span class="math notranslate nohighlight">\(h_{t-1} \)</span> with the retain ratio.</p></li>
</ol>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( r_t\)</span>: reset gate at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( z_t\)</span>: update gate at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \tilde{h}\_t\)</span>: candidate hidden state at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( \odot \)</span>: element-wise product</p></li>
</ul>
</section>
<section id="lstm">
<h2>LSTM<a class="headerlink" href="#lstm" title="Link to this heading">#</a></h2>
<a class="reference internal image-reference" href="images/LSTM.png"><img alt="images/LSTM.png" class="align-center" src="images/LSTM.png" style="width: 400px;" />
</a>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;i_t=\sigma(x_tW_{xi}^T+h_{t-1}W_{hi}^T) \\\\
&amp;f_t=\sigma(x_tW_{xf}^T+h_{t-1}W_{hf}^T) \\\\
&amp;\tilde{c}\_t=\tanh(x_tW_{xc}^T+h_{t-1}W_{hc}^T) \\\\
&amp;c_t=f_t\odot c_{t-1}+i_t\odot \tilde{c}\_t \\\\
&amp;o_t=\sigma(x_tW_{xo}^T+h_{t-1}W_{ho}^T) \\\\
&amp;h_t=o_t\odot\tanh(c_t)
\end{align*}\end{split}\]</div>
<p>Idea: Long Short-Term Memory - use 3 gates:</p>
<ol class="arabic simple">
<li><p><strong>Input gate</strong>: determine what new info from <span class="math notranslate nohighlight">\( x_t\)</span> should be added to cell state <span class="math notranslate nohighlight">\(c_t \)</span>.</p></li>
<li><p><strong>Forget gate</strong>: determine what info from prev cell <span class="math notranslate nohighlight">\( c_{t-1} \)</span> should be forgotten.</p></li>
<li><p><strong>Candidate cell</strong>: create a new candidate cell from <span class="math notranslate nohighlight">\( x_t\)</span> and <span class="math notranslate nohighlight">\(h_{t-1} \)</span>.</p></li>
<li><p><strong>Update cell</strong>: use <span class="math notranslate nohighlight">\( i_t\)</span> and <span class="math notranslate nohighlight">\(f_t \)</span> to combine prev and new candidate cells.</p></li>
<li><p><strong>Output gate</strong>: determine what info from curr cell <span class="math notranslate nohighlight">\( c_t\)</span> should be added to output <span class="math notranslate nohighlight">\(h_t \)</span>.</p></li>
<li><p><strong>Final</strong>: simply apply <span class="math notranslate nohighlight">\( o_t\)</span> to activated cell <span class="math notranslate nohighlight">\(c_t \)</span>.</p></li>
</ol>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( i_t\)</span>: input gate at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( f_t\)</span>: forget gate at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( c_t\)</span>: cell state at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{cell}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( o_t\)</span>: output gate at time <span class="math notranslate nohighlight">\(t\)</span> of shape <span class="math notranslate nohighlight">\((m,H_{out}) \)</span></p></li>
<li><p><span class="math notranslate nohighlight">\( H_{cell}\)</span>: cell hidden size (in most cases same as <span class="math notranslate nohighlight">\(H_{out} \)</span>)</p></li>
</ul>
</section>
<section id="bidirectional">
<h2>Bidirectional<a class="headerlink" href="#bidirectional" title="Link to this heading">#</a></h2>
</section>
<section id="stacked">
<h2>Stacked<a class="headerlink" href="#stacked" title="Link to this heading">#</a></h2>
<br/>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="activation">
<h1>Activation<a class="headerlink" href="#activation" title="Link to this heading">#</a></h1>
<p>An activation function adds nonlinearity to the output of a layer to enhance complexity. <a class="reference internal" href="#relu"><span class="xref myst">ReLU</span></a> and <a class="reference internal" href="#softmax"><span class="xref myst">Softmax</span></a> are SOTA.</p>
<p>Notations:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( z \)</span>: input (element-wise)</p></li>
</ul>
<section id="binary-like">
<h2>Binary-like<a class="headerlink" href="#binary-like" title="Link to this heading">#</a></h2>
<section id="sigmoid">
<h3>Sigmoid<a class="headerlink" href="#sigmoid" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\sigma(z)=\frac{1}{1+e^{-z}}
\]</div>
<p>Idea:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \sigma(z)\in(0,1)\)</span> and <span class="math notranslate nohighlight">\(\sigma(0)=0.5 \)</span>.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>imitation of the firing rate of a neuron, 0 if too negative and 1 if too positive.</p></li>
<li><p>smooth gradient.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>vanishing gradient: gradients rapidly shrink to 0 along backprop as long as any input is too positive or too negative.</p></li>
<li><p>non-zero centric bias <span class="math notranslate nohighlight">\( \rightarrow \)</span> non-zero mean activations.</p></li>
<li><p>computationally expensive.</p></li>
</ul>
</section>
<section id="tanh">
<h3>Tanh<a class="headerlink" href="#tanh" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
\]</div>
<p>Idea:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \tanh(z)\in(-1,1)\)</span> and <span class="math notranslate nohighlight">\(\tanh(0)=0 \)</span>.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>zero-centered</p></li>
<li><p>imitation of the firing rate of a neuron, -1 if too negative and 1 if too positive.</p></li>
<li><p>smooth gradient.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>vanishing gradient.</p></li>
<li><p>computationally expensive.</p></li>
</ul>
</section>
</section>
<section id="linear-units-rectified">
<h2>Linear Units (Rectified)<a class="headerlink" href="#linear-units-rectified" title="Link to this heading">#</a></h2>
<section id="relu">
<h3>ReLU<a class="headerlink" href="#relu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{ReLU}(z)=\max{(0,z)}
\]</div>
<p>Name: Rectified Linear Unit</p>
<p>Idea:</p>
<ul class="simple">
<li><p>convert negative linear outputs to 0.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>no vanishing gradient</p></li>
<li><p>activate fewer neurons</p></li>
<li><p>much less computationally expensive compared to sigmoid and tanh.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>dying ReLU: if most inputs are negative, then most neurons output 0 <span class="math notranslate nohighlight">\( \rightarrow\)</span> no gradient for such neurons <span class="math notranslate nohighlight">\(\rightarrow\)</span> no param update <span class="math notranslate nohighlight">\(\rightarrow \)</span> they die. (NOTE: A SOLVABLE DISADVANTAGE)</p>
<ul>
<li><p>Cause 1: high learning rate <span class="math notranslate nohighlight">\( \rightarrow\)</span> too much subtraction in param update <span class="math notranslate nohighlight">\(\rightarrow\)</span> weight too negative <span class="math notranslate nohighlight">\(\rightarrow \)</span> input for neuron too negative.</p></li>
<li><p>Cause 2: bias too negative <span class="math notranslate nohighlight">\( \rightarrow \)</span> input for neuron too negative.</p></li>
</ul>
</li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>. (NOTE: NOT A SEVERE DISADVANTAGE SO FAR)</p></li>
</ul>
</section>
<section id="lrelu">
<h3>LReLU<a class="headerlink" href="#lrelu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{LReLU}(z)=\max{(\alpha z,z)}
\]</div>
<p>Name: Leaky Rectified Linear Unit</p>
<p>Params:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \alpha\in(0,1) \)</span>: hyperparam (negative slope), default 0.01.</p></li>
</ul>
<p>Idea:</p>
<ul class="simple">
<li><p>scale negative linear outputs by <span class="math notranslate nohighlight">\( \alpha \)</span>.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>no dying ReLU.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>slightly more computationally expensive than ReLU.</p></li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>.</p></li>
</ul>
</section>
<section id="prelu">
<h3>PReLU<a class="headerlink" href="#prelu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{PReLU}(z)=\max{(\alpha z,z)}
\]</div>
<p>Name: Parametric Rectified Linear Unit</p>
<p>Params:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \alpha\in(0,1) \)</span>: learnable param (negative slope), default 0.25.</p></li>
</ul>
<p>Idea:</p>
<ul class="simple">
<li><p>scale negative linear outputs by a learnable <span class="math notranslate nohighlight">\( \alpha \)</span>.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>a variable, adaptive param learned from data.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>slightly more computationally expensive than LReLU.</p></li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>.</p></li>
</ul>
</section>
<section id="rrelu">
<h3>RReLU<a class="headerlink" href="#rrelu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{RReLU}(z)=\max{(\alpha z,z)}
\]</div>
<p>Name: Randomized Rectified Linear Unit</p>
<p>Params:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \alpha\sim\mathrm{Uniform}(l,u) \)</span>: a random number sampled from a uniform distribution.</p></li>
<li><p><span class="math notranslate nohighlight">\( l,u \)</span>: hyperparams (lower bound, upper bound)</p></li>
</ul>
<p>Idea:</p>
<ul class="simple">
<li><p>scale negative linear outputs by a random <span class="math notranslate nohighlight">\( \alpha \)</span>.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>reduce overfitting by randomization.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>slightly more computationally expensive than LReLU.</p></li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>.</p></li>
</ul>
</section>
</section>
<section id="linear-units-exponential">
<h2>Linear Units (Exponential)<a class="headerlink" href="#linear-units-exponential" title="Link to this heading">#</a></h2>
<section id="elu">
<h3>ELU<a class="headerlink" href="#elu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{ELU}(z)=\begin{cases}
z &amp; \mathrm{if}\ z\geq0 \\\\
\alpha(e^z-1) &amp; \mathrm{if}\ z&lt;0
\end{cases}
\end{split}\]</div>
<p>Name: Exponential Linear Unit</p>
<p>Params:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \alpha \)</span>: hyperparam, default 1.</p></li>
</ul>
<p>Idea:</p>
<ul class="simple">
<li><p>convert negative linear outputs to the non-linear exponential function above.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>mean unit activation is closer to 0 <span class="math notranslate nohighlight">\( \rightarrow \)</span> reduce bias shift (i.e., non-zero mean activation is intrinsically a bias for the next layer.)</p></li>
<li><p>lower computational complexity compared to batch normalization.</p></li>
<li><p>smooth to <span class="math notranslate nohighlight">\( -\alpha \)</span> slowly with smaller derivatives that decrease forwardprop variation.</p></li>
<li><p>faster learning and higher accuracy for image classification in practice.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>slightly more computationally expensive than ReLU.</p></li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>.</p></li>
</ul>
</section>
<section id="selu">
<h3>SELU<a class="headerlink" href="#selu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{SELU}(z)=\lambda\begin{cases}
z &amp; \mathrm{if}\ z\geq0 \\
\alpha(e^z-1) &amp; \mathrm{if}\ z&lt;0
\end{cases}
\end{split}\]</div>
<p>Name: Scaled Exponential Linear Unit</p>
<p>Params:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \alpha \)</span>: hyperparam, default 1.67326.</p></li>
<li><p><span class="math notranslate nohighlight">\( \lambda \)</span>: hyperparam (scale), default 1.05070.</p></li>
</ul>
<p>Idea:</p>
<ul class="simple">
<li><p>scale ELU.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>self-normalization <span class="math notranslate nohighlight">\( \rightarrow \)</span> activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>more computationally expensive than ReLU.</p></li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>.</p></li>
</ul>
</section>
<section id="celu">
<h3>CELU<a class="headerlink" href="#celu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[\begin{split}
\mathrm{CELU}(z)=\begin{cases}
z &amp; \mathrm{if}\ z\geq0\\
\alpha(e^{\frac{z}{\alpha}}-1) &amp; \mathrm{if}\ z&lt;0
\end{cases}
\end{split}\]</div>
<p>Name: Continuously Differentiable Exponential Linear Unit</p>
<p>Params:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\( \alpha \)</span>: hyperparam, default 1.</p></li>
</ul>
<p>Idea:</p>
<ul class="simple">
<li><p>scale the exponential part of ELU with <span class="math notranslate nohighlight">\( \frac{1}{\alpha} \)</span> to make it continuously differentiable.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>smooth gradient due to continuous differentiability (i.e., <span class="math notranslate nohighlight">\( \mathrm{CELU}'(0)=1 \)</span>).</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>slightly more computationally expensive than ELU.</p></li>
<li><p>activation explosion as <span class="math notranslate nohighlight">\( z\rightarrow\infty \)</span>.</p></li>
</ul>
</section>
</section>
<section id="linear-units-others">
<h2>Linear Units (Others)<a class="headerlink" href="#linear-units-others" title="Link to this heading">#</a></h2>
<section id="gelu">
<h3>GELU<a class="headerlink" href="#gelu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{GELU}(z)=z*\Phi(z)=0.5z(1+\tanh{[\sqrt{\frac{2}{\pi}}(z+0.044715z^3)]})
\]</div>
<p>Name: Gaussian Error Linear Unit</p>
<p>Idea:</p>
<ul class="simple">
<li><p>weigh each output value by its Gaussian cdf.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>throw away gate structure and add probabilistic-ish feature to neuron outputs.</p></li>
<li><p>seemingly better performance than the ReLU and ELU families, SOTA in transformers.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>slightly more computationally expensive than ReLU.</p></li>
<li><p>lack of practical testing at the moment.</p></li>
</ul>
</section>
<section id="silu">
<h3>SiLU<a class="headerlink" href="#silu" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{SiLU}(z)=z*\sigma(z)
\]</div>
<p>Name: Sigmoid Linear Unit</p>
<p>Idea:</p>
<ul class="simple">
<li><p>weigh each output value by its sigmoid value.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>throw away gate structure.</p></li>
<li><p>seemingly better performance than the ReLU and ELU families.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>worse than GELU.</p></li>
</ul>
</section>
<section id="softplus">
<h3>Softplus<a class="headerlink" href="#softplus" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{softplus}(z)=\frac{1}{\beta}\log{(1+e^{\beta z})}
\]</div>
<p>Idea:</p>
<ul class="simple">
<li><p>smooth approximation of ReLU.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>differentiable and thus theoretically better than ReLU.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>empirically far worse than ReLU in terms of computation and performance.</p></li>
</ul>
</section>
</section>
<section id="multiclass">
<h2>Multiclass<a class="headerlink" href="#multiclass" title="Link to this heading">#</a></h2>
<section id="softmax">
<h3>Softmax<a class="headerlink" href="#softmax" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{softmax}(z_i)=\frac{\exp{(z_i)}}{\sum_j{\exp{(z_j)}}}
\]</div>
<p>Idea:</p>
<ul class="simple">
<li><p>convert each value <span class="math notranslate nohighlight">\( z_i\)</span> in the output tensor <span class="math notranslate nohighlight">\(\mathbf{z}\)</span> into its corresponding exponential probability s.t. <span class="math notranslate nohighlight">\(\sum_i{\mathrm{softmax}(z_i)}=1 \)</span>.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>your single best choice for multiclass classification.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>mutually exclusive classes (i.e., one input can only be classified into one class.)</p></li>
</ul>
</section>
<section id="softmin">
<h3>Softmin<a class="headerlink" href="#softmin" title="Link to this heading">#</a></h3>
<div class="math notranslate nohighlight">
\[
\mathrm{softmin}(z_i)=\mathrm{softmax}(-z_i)=\frac{\exp{(-z_i)}}{\sum_j{\exp{(-z_j)}}}
\]</div>
<p>Idea:</p>
<ul class="simple">
<li><p>reverse softmax.</p></li>
</ul>
<p>Pros:</p>
<ul class="simple">
<li><p>suitable for multiclass classification.</p></li>
</ul>
<p>Cons:</p>
<ul class="simple">
<li><p>why not softmax.</p></li>
</ul>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LazyNotes</p>
      </div>
    </a>
    <a class="right-next"
       href="../llm/rl.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">RL for LLMs</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Layer</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#basic">Basic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#transformer">Transformer</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#input">Input</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tokenization">Tokenization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#token-embedding">Token Embedding</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#positional-encoding">Positional Encoding</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#sinusoidal-pe">Sinusoidal PE</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#attention">Attention</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#self-attention">Self-Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#masked-causal-attention">Masked/Causal Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#cross-attention">Cross Attention</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#multi-head-attention">Multi-Head Attention</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolutional">Convolutional</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#depthwise-separable-convolution">Depthwise Separable Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#atrous-dilated-convolution">Atrous/Dilated Convolution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#pooling">Pooling</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#recurrent">Recurrent</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#gru">GRU</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#lstm">LSTM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#bidirectional">Bidirectional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#stacked">Stacked</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#activation">Activation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-like">Binary-like</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-units-rectified">Linear Units (Rectified)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lrelu">LReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#prelu">PReLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#rrelu">RReLU</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-units-exponential">Linear Units (Exponential)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elu">ELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#selu">SELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#celu">CELU</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-units-others">Linear Units (Others)</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gelu">GELU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#silu">SiLU</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softplus">Softplus</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#multiclass">Multiclass</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#softmin">Softmin</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Renyi Qu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>