{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8561aff",
   "metadata": {},
   "source": [
    "# Layer\n",
    "A layer is a function that maps input tensor $X$ to output tensor $Y$.\n",
    "\n",
    "Let $g$ denote the gradient $\\frac{\\partial\\mathcal{L}}{\\partial y}$ for readability.\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Basic\n",
    "## Linear\n",
    "- **What**: Linear transformation.\n",
    "- **Why**: The simplest way to transform data & learn patterns.\n",
    "- **How**: input features * weights (+ bias).\n",
    "\n",
    "````{admonition} Math\n",
    ":class: note, dropdown\n",
    "```{tab} Vector\n",
    "Notations:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{H_{out}}$: Output vector.\n",
    "- Params:\n",
    "    - $W\\in\\mathbb{R}^{H_{out}\\times H_{in}}$: Weight matrix.\n",
    "    - $\\textbf{b}\\in\\mathbb{R}^{H_{out}}$: Bias vector.\n",
    "- Hyperparams:\n",
    "    - $H_{in}$: Input feature dimension.\n",
    "    - $H_{out}$: Output feature dimension.\n",
    "\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "\\textbf{y}=W\\textbf{x}+\\textbf{b}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial W}=\\textbf{g}\\textbf{x}^T \\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{b}}=\\textbf{g}\\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}}=W^T\\textbf{g}\n",
    "\\end{align*}$$\n",
    "```\n",
    "```{tab} Tensor\n",
    "Notations:\n",
    "- IO:\n",
    "    - $\\mathbf{X}\\in\\mathbb{R}^{*\\times H_{in}}$: Input tensor.\n",
    "    - $\\mathbf{Y}\\in\\mathbb{R}^{*\\times H_{out}}$: Output tensor.\n",
    "- Params:\n",
    "    - $W\\in\\mathbb{R}^{H_{out}\\times H_{in}}$: Weight matrix.\n",
    "    - $\\textbf{b}\\in\\mathbb{R}^{H_{out}}$: Bias vector.\n",
    "- Hyperparams:\n",
    "    - $H_{in}$: Input feature dimension.\n",
    "    - $H_{out}$: Output feature dimension.\n",
    "\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "\\textbf{Y}=\\textbf{X}W^T+\\textbf{b}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial W}=\\textbf{g}^T\\textbf{X} \\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{b}}=\\sum_*\\textbf{g}_*\\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}}=\\textbf{g}W\n",
    "\\end{align*}$$\n",
    "````\n",
    "\n",
    "\n",
    "## Dropout\n",
    "- **What**: Randomly ignore some neurons during training.\n",
    "- **Why**: To reduce overfitting.\n",
    "- **How**: During training:\n",
    "    1. Randomly set a fraction of neurons to 0.\n",
    "    2. Scale the outputs/gradients on active neurons by the keep probability.\n",
    "\n",
    "````{admonition} Math\n",
    ":class: note, dropdown\n",
    "```{tab} Vector\n",
    "Notations:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{H_{in}}$: Output vector.\n",
    "- Hyperparams:\n",
    "    - $p$: Keep probability.\n",
    "- Intermediate values:\n",
    "    - $\\textbf{m}\\in\\mathbb{R}^{H_{in}}$: binary mask, where each element $m\\sim\\text{Bernoulli}(p)$.\n",
    "\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "\\textbf{y}=\\frac{\\textbf{m}\\odot\\textbf{x}}{p}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}} = \\frac{\\textbf{m}\\odot\\textbf{g}}{p}\n",
    "$$\n",
    "```\n",
    "```{tab} Tensor\n",
    "Notations:\n",
    "- IO:\n",
    "    - $\\mathbf{X}\\in\\mathbb{R}^{*\\times H_{in}}$: Input tensor.\n",
    "    - $\\mathbf{Y}\\in\\mathbb{R}^{*\\times H_{in}}$: Output tensor.\n",
    "- Hyperparams:\n",
    "    - $p$: Keep probability.\n",
    "- Intermediate values:\n",
    "    - $\\textbf{M}\\in\\mathbb{R}^{*\\times H_{in}}$: binary mask, where each element $m\\sim\\text{Bernoulli}(p)$.\n",
    "\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "\\textbf{Y}=\\frac{\\textbf{M}\\odot\\textbf{X}}{p}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{X}} = \\frac{\\textbf{M}\\odot\\textbf{g}}{p}\n",
    "$$\n",
    "```\n",
    "````\n",
    "\n",
    "```{admonition} Q&A\n",
    ":class: tip, dropdown\n",
    "*Cons?*\n",
    "- ⬆️Training time $\\leftarrow$ Longer convergence\n",
    "- Needs Hyperparameter Tuning\n",
    "```\n",
    "\n",
    "## Residual Connection\n",
    "- **What**: Model the residual ($Y-X$) instead of the output ($Y$).\n",
    "- **Why**: To mitigate [vanishing/exploding gradients](../dl/issues.md/#vanishing/exploding-gradient).\n",
    "- **How**: Add input $X$ to block output $F(X)$.\n",
    "    - If the feature dimension of $X$ and $F(X)$ doesn't match, use a shortcut linear layer on $X$ to change its feature dimension.\n",
    "\n",
    "````{admonition} Math\n",
    ":class: note, dropdown\n",
    "```{tab} Vector\n",
    "**Notation**:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{H_{out}}$: Output vector.\n",
    "- Hyperparams:\n",
    "    - $F(\\cdot)\\in\\mathbb{R}^{H_{out}}$: The aggregate function of all layers within the residual block.\n",
    "\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "\\textbf{y}=F(\\textbf{x})+\\textbf{x}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}}=\\mathbf{g}(1+\\frac{\\partial F(\\textbf{x})}{\\partial\\textbf{x}})\n",
    "$$\n",
    "```\n",
    "```{tab} Tensor\n",
    "**Notation**:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{*\\times H_{in}}$: Input tensor.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{*\\times H_{out}}$: Output tensor.\n",
    "- Hyperparams:\n",
    "    - $F(\\cdot)\\in\\mathbb{R}^{H_{out}}$: The aggregate function of all layers within the residual block.\n",
    "\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "\\textbf{Y}=F(\\textbf{X})+\\textbf{X}\n",
    "$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{X}}=\\mathbf{g}(1+\\frac{\\partial F(\\textbf{X})}{\\partial\\textbf{X}})\n",
    "$$\n",
    "```\n",
    "````\n",
    "\n",
    "\n",
    "## Normalization\n",
    "### Batch Normalization\n",
    "- **What**: Normalize each feature across input samples to zero mean & unit variance.\n",
    "- **Why**: To mitigate [internal covariate shift](../dl/issues.md/#vanishing/internal-covariate-shift).\n",
    "- **How**:\n",
    "    1. Calculate the mean and variance for each batch.\n",
    "    2. Normalize the batch.\n",
    "    3. Scale and shift the normalized output using learnable params.\n",
    "\n",
    "\n",
    "```{admonition} Math\n",
    ":class: note, dropdown\n",
    "**Notation**:\n",
    "- IO:\n",
    "    - $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$: Input matrix.\n",
    "    - $\\mathbf{Y}\\in\\mathbb{R}^{m\\times n}$: Output matrix.\n",
    "- Params:\n",
    "    - $\\gamma\\in\\mathbb{R}$: Scale param.\n",
    "    - $\\beta\\in\\mathbb{R}$: Shift param.\n",
    "\n",
    "Forward:\n",
    "1. Calculate the mean and variance for each batch.\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    \\boldsymbol{\\mu}_B&=\\frac{1}{m}\\sum_{i=1}^{m}\\textbf{x}_i\\\\\n",
    "    \\boldsymbol{\\sigma}_B^2&=\\frac{1}{m}\\sum_{i=1}^{m}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)^2\n",
    "    \\end{align*}$$\n",
    "\n",
    "2. Normalize each batch.\n",
    "\n",
    "    $$\n",
    "    \\textbf{z}_i=\\frac{\\textbf{x}_i-\\boldsymbol{\\mu}_B}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}}\n",
    "    $$\n",
    "\n",
    "    where $\\epsilon$ is a small constant to avoid dividing by 0.\n",
    "\n",
    "3. Scale and shift the normalized output.\n",
    "\n",
    "    $$\n",
    "    \\textbf{y}_i=\\gamma\\textbf{z}_i+\\beta\n",
    "    $$\n",
    "\n",
    "Backward:\n",
    "1. Gradient w.r.t. params:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\gamma}=\\sum_{i=1}^{m}\\textbf{g}_i\\textbf{z}_i\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\beta}=\\sum_{i=1}^{m}\\textbf{g}_i\n",
    "    \\end{align*}$$\n",
    "2. Gradient w.r.t. input:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}=\\gamma\\textbf{g}_i\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\sigma}_B^2}=\\sum_{i=1}^{m}\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)\\left(-\\frac{1}{2}(\\boldsymbol{\\sigma}_B^2+\\epsilon)^{-\\frac{3}{2}}\\right)\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mu}_B}=\\sum_{i=1}^{m}\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}\\cdot\\left(-\\frac{1}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}}\\right)+\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\sigma}_B^2}\\cdot\\left(-\\frac{2}{m}\\sum_{i=1}^{m}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)\\right)\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}_i}=\\frac{1}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}}\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}+\\frac{2}{m}\\frac{\\partial\\ma                                                                         thcal{L}}{\\partial\\boldsymbol{\\sigma}_B^2}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)+\\frac{1}{m}\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mu}_B}\\right)\n",
    "    \\end{align*}$$\n",
    "```\n",
    "\n",
    "```{admonition} Q&A\n",
    ":class: tip, dropdown\n",
    "*Pros?*\n",
    "- Accelerates training with higher learning rates.\n",
    "- Reduces sensitivity to weight initialization.\n",
    "- Mitigates [vanishing/exploding gradients](../dl/issues.md/#vanishing/exploding-gradient).\n",
    "\n",
    "*Cons?*\n",
    "- Adds computation overhead and complexity.\n",
    "- Works best when each mini-batch is representative of the overall input distribution to accurately estimate the mean and variance.\n",
    "- Causes potential issues in certain cases like small mini-batches or when batch statistics differ from overall dataset statistics.\n",
    "```\n",
    "\n",
    "### Layer Normalization\n",
    "- **What**: Normalize each sample across the input features to zero mean and unit variance.\n",
    "- **Why**: Batch normalization depends on the batch size.\n",
    "    - When it's too big, high computational cost.\n",
    "    - When it's too small, the batch may not be representative of the underlying data distribution.\n",
    "    - Hyperparam tuning is required to find the optimal batch size, leading to high computational cost.\n",
    "- **How**:\n",
    "    1. Calculate the mean and variance for each feature.\n",
    "    2. Normalize the feature.\n",
    "    3. Scale and shift the normalized output using learnable params.\n",
    "\n",
    "```{admonition} Math\n",
    ":class: note, dropdown\n",
    "It's easy to explain with the vector form for batch normalization, but it's more intuitive to explain with the scalar form for layer normalization.\n",
    "\n",
    "Notations:\n",
    "- IO:\n",
    "    - $x_{ij}\\in\\mathbb{R}$: $j$th feature value for $i$th input sample.\n",
    "    - $y_{ij}\\in\\mathbb{R}$: $j$th feature value for $i$th output sample.\n",
    "- Params:\n",
    "    - $\\boldsymbol{\\gamma}\\in\\mathbb{R}^n$: Scale param.\n",
    "    - $\\boldsymbol{\\beta}\\in\\mathbb{R}^n$: Shift param.\n",
    "\n",
    "Forward:\n",
    "1. Calculate the mean and variance for each feature.\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    \\mu_i&=\\frac{1}{n}\\sum_{j=1}^{n}x_{ij}\\\\\n",
    "    \\sigma_i^2&=\\frac{1}{n}\\sum_{j=1}^{n}(x_{ij}-\\mu_i)^2\n",
    "    \\end{align*}$$\n",
    "\n",
    "2. Normalize each feature.\n",
    "\n",
    "    $$\n",
    "    z_{ij}=\\frac{x_{ij}-\\mu_i}{\\sqrt{\\sigma_i^2+\\epsilon}}\n",
    "    $$\n",
    "\n",
    "    where $\\epsilon$ is a small constant to avoid dividing by 0.\n",
    "\n",
    "3. Scale and shift the normalized output.\n",
    "\n",
    "    $$\n",
    "    y_{ij}=\\gamma_jz_{ij}+\\beta_j\n",
    "    $$\n",
    "\n",
    "Backward:\n",
    "1. Gradient w.r.t. params:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\gamma_j}=\\sum_{i=1}^{m}g_{ij}z_{ij}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\beta_j}=\\sum_{i=1}^{m}g_{ij}\n",
    "    \\end{align*}$$\n",
    "2. Gradient w.r.t. input:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}=\\gamma_jg_{ij}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\sigma_i^2}=\\sum_{j=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}(x_{ij}-\\mu_i)\\left(-\\frac{1}{2}(\\sigma_i^2+\\epsilon)^{-\\frac{3}{2}}\\right)\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\mu_i}=\\sum_{j=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}\\cdot\\left(-\\frac{1}{\\sqrt{\\sigma_i^2+\\epsilon}}\\right)+\\frac{\\partial\\mathcal{L}}{\\partial\\sigma_i^2}\\cdot\\left(-\\frac{2}{n}\\sum_{j=1}^{n}(x_{ij}-\\mu_i)\\right)\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{x_{ij}}=\\frac{1}{\\sqrt{\\sigma_i^2+\\epsilon}}\\left(\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}+\\frac{2}{n}\\frac{\\partial\\mathcal{L}}{\\partial\\sigma_i^2}(x_{ij}-\\mu_i)+\\frac{1}{n}\\frac{\\partial\\mathcal{L}}{\\partial\\mu_i}\\right)\n",
    "    \\end{align*}$$\n",
    "```\n",
    "\n",
    "```{admonition} Q&A\n",
    ":class: tip, dropdown\n",
    "*Pros*:\n",
    "- Reduces hyperparam tuning effort.\n",
    "- High consistency during training and inference.\n",
    "- Mitigates [vanishing/exploding gradients](../dl/issues.md/#vanishing/exploding-gradient).\n",
    "\n",
    "*Cons*:\n",
    "- Adds computation overhead and complexity.\n",
    "- Inapplicable in CNNs due to varied statistics of spatial features.\n",
    "```\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Convolution\n",
    "- **What**: Apply a set of filters to input data to extract local features. ([paper](https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf))\n",
    "- **Why**: To learn spatial hierarchies of features.\n",
    "- **How**: Slide multiple filters/kernels (i.e., small matrices) over the input data.\n",
    "    - At each step, perform element-wise multiplication and summation between each filter and the scanned area, producing a feature map.\n",
    "\n",
    "```{admonition} Math\n",
    ":class: note, dropdown\n",
    "Notations:\n",
    "    - IO:\n",
    "        - $\\mathbf{X}\\in\\mathbb{R}^{H_{in}\\times W_{in}\\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{out}}$: Output volume.\n",
    "    - Params:\n",
    "        - $\\mathbf{W}\\in\\mathbb{R}^{F_{H}\\times F_{W}\\times C_{out}\\times C_{in}}$: Filters.\n",
    "        - $\\mathbf{b}\\in\\mathbb{R}^{C_{out}}$: Biases.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $C_{out}$: #Filters (i.e., #Output channels).\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "        - $p$: Padding size.\n",
    "Forward:\n",
    "\n",
    "    $$\n",
    "    Y_{h,w,c_{out}}=\\sum_{c_{in}=1}^{C_{in}}\\sum_{i=1}^{f_h}\\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\\cdot X_{sh+i-p,sw+j-p,c_{in}}+b_{c_{out}}\n",
    "    $$\n",
    "\n",
    "    where\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    H_{out}&=\\left\\lfloor\\frac{H_{in}+2p-f_h}{s}\\right\\rfloor+1\\\\\n",
    "    W_{out}&=\\left\\lfloor\\frac{W_{in}+2p-f_w}{s}\\right\\rfloor+1\n",
    "    \\end{align*}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial W_{i,j,c_{out},c_{in}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\cdot X_{sh+i-p, sw+j-p, c_{in}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial b_{c_{out}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial X_{i,j,c_{in}}}=\\sum_{c_{out}=1}^{C_{out}}\\sum_{h=1}^{f_h}\\sum_{w=1}^{f_w}g_{h,w,c_{out}}\\cdot W_{i-sh+p,j-sw+p,c_{out},c_{in}}\n",
    "    \\end{align*}$$\n",
    "\n",
    "    Notice it is similar to backprop of linear layer except it sums over the scanned area and removes padding.\n",
    "```\n",
    "\n",
    "```{admonition} Q&A\n",
    ":class: tip, dropdown\n",
    "*Pros*:\n",
    "- Translation invariance.\n",
    "- Efficiently captures spatial hierarchies.\n",
    "\n",
    "*Cons*:\n",
    "- High computational cost for big data.\n",
    "- Requires big data to be performant.\n",
    "- Requires extensive hyperparam tuning.\n",
    "```\n",
    "<!-- \n",
    "## Depthwise Separable Convolution\n",
    "- **What**: Depthwise convolution + Pointwise convolution. ([paper](https://arxiv.org/pdf/1610.02357))\n",
    "- **Why**: To significantly reduce computational cost and #params.\n",
    "- **How**:\n",
    "    - **Depthwise**: Use a single filter independently per channel.\n",
    "    - **Pointwise**: Use Conv1d to combine the outputs of depthwise convolution.\n",
    "- **When**: When computational efficiency and model size are crucial.\n",
    "- **Where**: [MobileNets](https://arxiv.org/pdf/1704.04861), [Xception](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf), etc.\n",
    "- **Pros**: Significantly higher computational efficiency (time & space).\n",
    "- **Cons**: Lower accuracy. -->\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "Notations:\n",
    "    - IO:\n",
    "        - $\\mathbf{X} \\in \\mathbb{R}^{H_{in} \\times W_{in} \\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y} \\in \\mathbb{R}^{H_{out} \\times W_{out} \\times C_{out}}$: Output volume.\n",
    "    - Params:\n",
    "        - $\\mathbf{W^d} \\in \\mathbb{R}^{f_h \\times f_w \\times C_{in}}$: Depthwise filters.\n",
    "        - $\\mathbf{b^d} \\in \\mathbb{R}^{C_{in}}$: Depthwise biases.\n",
    "        - $\\mathbf{W^p} \\in \\mathbb{R}^{1 \\times 1 \\times C_{in} \\times C_{out}}$: Pointwise filters.\n",
    "        - $\\mathbf{b^p} \\in \\mathbb{R}^{C_{out}}$: Pointwise biases.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $C_{out}$: #Output channels.\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "        - $p$: Padding size.\n",
    "Forward:\n",
    "    1. Depthwise convolution: Calculate $\\mathbf{Z} \\in \\mathbb{R}^{H_{out} \\times W_{out} \\times C_{in}}$:\n",
    "\n",
    "        $$\n",
    "        Z_{h,w,c_{in}} = \\sum_{i=1}^{f_h} \\sum_{j=1}^{f_w} W^d_{i,j,c_{in}} \\cdot X_{sh+i-p, sw+j-p, c_{in}} + b^d_{c_{in}}\n",
    "        $$\n",
    "\n",
    "    2. Pointwise convolution:\n",
    "\n",
    "        $$\n",
    "        Y_{h,w,c_{out}} = \\sum_{c_{in}=1}^{C_{in}} W^p_{1,1,c_{in},c_{out}} \\cdot Z_{h,w,c_{in}} + b^p_{c_{out}}\n",
    "        $$\n",
    "        where\n",
    "        $$\\begin{align*}\n",
    "        H_{out} &= \\left\\lfloor \\frac{H_{in} + 2p - f_h}{s} \\right\\rfloor + 1 \\\\\n",
    "        W_{out} &= \\left\\lfloor \\frac{W_{in} + 2p - f_w}{s} \\right\\rfloor + 1\n",
    "        \\end{align*}$$\n",
    "\n",
    "Backward:\n",
    "    1. Pointwise convolution: Let $g^{p}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{out}}$ be $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Y}}$.\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial W^p_{1,1,c_{in},c_{out}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}} \\cdot Z_{h,w,c_{in}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial b^p_{c_{out}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial Z_{h,w,c_{in}}} = \\sum_{c_{out}=1}^{C_{out}} g^{p}_{h,w,c_{out}} \\cdot W^p_{1,1,c_{in},c_{out}}\n",
    "        \\end{align*}$$\n",
    "    2. Depthwise convolution: Let $g^{d}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{in}}$ be $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Z}}$.\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial W^d_{i,j,c_{in}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}} \\cdot X_{sh+i-p, sw+j-p, c_{in}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial b_{d,c_{in}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial X_{i,j,c_{in}}} = \\sum_{h=1}^{f_h} \\sum_{w=1}^{f_w} g^d_{h,w,c_{in}} \\cdot W^d_{i-sh+p,j-sw+p,c_{in}}\n",
    "        \\end{align*}$$\n",
    "``` -->\n",
    "\n",
    "<!-- ## Atrous/Dilated Convolution\n",
    "- **What**: Add holes between filter elements (i.e., dilation). ([paper](https://arxiv.org/pdf/1511.07122))\n",
    "- **Why**: The filters can capture larger contextual info without increasing #params.\n",
    "- **How**: Introduce a dilation rate $r$ to determine the space between the filter elements. Then compute convolution accordingly.\n",
    "- **When**: When understanding the broader context is important.\n",
    "- **Where**: Semantic image segmentation, object detection, depth estimation, optical flow estimation, etc.\n",
    "- **Pros**:\n",
    "    - Larger receptive fields without increasing #params.\n",
    "    - Captures multi-scale info without upsampling layers.\n",
    "- **Cons**:\n",
    "    - Requires very careful hyperparam tuning, or info loss. -->\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "Notations:\n",
    "    - IO:\n",
    "        - $\\mathbf{X}\\in\\mathbb{R}^{H_{in}\\times W_{in}\\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{out}}$: Output volume.\n",
    "    - Params:\n",
    "        - $\\mathbf{W}\\in\\mathbb{R}^{F_{H}\\times F_{W}\\times C_{out}\\times C_{in}}$: Filters.\n",
    "        - $\\mathbf{b}\\in\\mathbb{R}^{C_{out}}$: Biases.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $C_{out}$: #Filters (i.e., #Output channels).\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "        - $p$: Padding size.\n",
    "        - $r$: Dilation rate.\n",
    "Forward:\n",
    "    1. (optional) Pad input tensor: $\\mathbf{X}^\\text{pad}\\in\\mathbb{R}^{(H_{in}+2p)\\times (W_{in}+2p)\\times C_{in}}$\n",
    "    2. Perform element-wise multiplication (i.e., convolution):\n",
    "\n",
    "        $$\n",
    "        Y_{h,w,c_{out}}=\\sum_{c_{in}=1}^{C_{in}}\\sum_{i=1}^{f_h}\\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\\cdot X_{sh+r(i-1)-p,sw+r(j-1)-p,c_{in}}+b_{c_{out}}\n",
    "        $$\n",
    "\n",
    "        where\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        H_{out}&=\\left\\lfloor\\frac{H_{in}+2p-r(f_h-1)-1}{s}\\right\\rfloor+1\\\\\n",
    "        W_{out}&=\\left\\lfloor\\frac{W_{in}+2p-r(f_w-1)-1}{s}\\right\\rfloor+1\n",
    "        \\end{align*}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial W_{i,j,c_{out},c_{in}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\cdot X_{sh+r(i-1)-p, sw+r(j-1)-p, c_{in}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial b_{c_{out}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial X_{i,j,c_{in}}}=\\sum_{c_{out}=1}^{C_{out}}\\sum_{h=1}^{f_h}\\sum_{w=1}^{f_w}g_{h,w,c_{out}}\\cdot W_{r(i-1)-sh+p,r(j-1)-sw+p,c_{out},c_{in}}\n",
    "    \\end{align*}$$\n",
    "``` -->\n",
    "\n",
    "## Pooling\n",
    "- **What**: Convolution but ([paper](https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf))\n",
    "    - computes a heuristic per scanned patch.\n",
    "    - uses the same #channels.\n",
    "- **Why**: Dimensionality reduction while preserving dominant features.\n",
    "- **How**: Slide the pooling window over the input & apply the heuristic within the scanned patch.\n",
    "    - **Max**: Output the maximum value from each patch.\n",
    "    - **Average**: Output the average value of each patch.\n",
    "- **When**: When downsampling is necessary.\n",
    "- **Where**: After convolutional layer.\n",
    "- **Pros**:\n",
    "    - Significantly higher computational efficiency (time & space).\n",
    "    - No params to train.\n",
    "    - Reduces overfitting.\n",
    "    - Preserves translation invariance without losing too much info.\n",
    "    - High robustness.\n",
    "- **Cons**:\n",
    "    - Slight spatial info loss.\n",
    "    - Requires hyperparam tuning.\n",
    "        - Large filter or stride results in coarse features.\n",
    "- **Max vs Average**:\n",
    "    - **Max**: Captures most dominant features; higher robustness.\n",
    "    - **Avg**: Preserves more info; provides smoother features; dilutes the importance of dominant features.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "Notations:\n",
    "    - IO:\n",
    "        - $\\mathbf{X}\\in\\mathbb{R}^{H_{in}\\times W_{in}\\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{in}}$: Output volume.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "Forward:\n",
    "\n",
    "    $$\\begin{array}{ll}\n",
    "    \\text{Max:} & Y_{h,w,c}=\\max_{i=1,\\cdots,f_h\\ |\\ j=1,\\cdots,f_w}X_{sh+i,sw+j,c}\\\\\n",
    "    \\text{Avg:} & Y_{h,w,c}=\\frac{1}{f_hf_w}\\sum_{i=1}^{f_h}\\sum_{j=1}^{f_w}X_{sh+i,sw+j,c}\n",
    "    \\end{array}$$\n",
    "\n",
    "    where\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    H_{out}&=\\left\\lfloor\\frac{H_{in}-f_h}{s}\\right\\rfloor+1\\\\\n",
    "    W_{out}&=\\left\\lfloor\\frac{W_{in}-f_h}{s}\\right\\rfloor+1\n",
    "    \\end{align*}$$\n",
    "\n",
    "Backward:\n",
    "\n",
    "    $$\\begin{array}{ll}\n",
    "    \\text{Max:} & \\frac{\\partial\\mathcal{L}}{\\partial X_{sh+i,sw+j,c}}=g_{h,w,c}\\text{ if }X_{sh+i,sw+j,c}=Y_{h,w,c}\\\\\n",
    "    \\text{Avg:} & \\frac{\\partial\\mathcal{L}}{\\partial X_{sh+i,sw+j,c}}=\\frac{g_{h,w,c}}{f_hf_w}\n",
    "    \\end{array}$$\n",
    "\n",
    "    - Max: Gradients only propagate to the max element of each window.\n",
    "    - Avg: Gradients are equally distributed among all elements in each window.\n",
    "``` -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Recurrent\n",
    "```{image} ../images/RNN.png\n",
    ":width: 400\n",
    ":align: center\n",
    "```\n",
    "\n",
    "$$\n",
    "h_t=\\tanh(x_tW_{xh}^T+h_{t-1}W_{hh}^T)\n",
    "$$\n",
    "\n",
    "Idea: **recurrence** - maintain a hidden state that captures information about previous inputs in the sequence\n",
    "\n",
    "Notations:\n",
    "- $ x_t$: input at time $t$ of shape $(m,H_{in}) $\n",
    "- $ h_t$: hidden state at time $t$ of shape $(D,m,H_{out}) $\n",
    "- $ W_{xh}$: weight matrix of shape $(H_{out},H_{in})$ if initial layer, else $(H_{out},DH_{out}) $\n",
    "- $ W_{hh}$: weight matrix of shape $(H_{out},H_{out}) $\n",
    "- $ H_{in}$: input size, #features in $x_t $\n",
    "- $ H_{out}$: hidden size, #features in $h_t $\n",
    "- $ m $: batch size\n",
    "- $ D$: $=2$ if bi-directional else $1 $\n",
    "\n",
    "Cons:\n",
    "- Short-term memory: hard to carry info from earlier steps to later ones if long seq\n",
    "- Vanishing gradient: gradients in earlier parts become extremely small if long seq\n",
    "\n",
    "## GRU\n",
    "\n",
    "```{image} ../images/GRU.png\n",
    ":width: 400\n",
    ":align: center\n",
    "```\n",
    "\n",
    "$$\\begin{align*}\n",
    "&r_t=\\sigma(x_tW_{xr}^T+h_{t-1}W_{hr}^T) \\\\\\\\\n",
    "&z_t=\\sigma(x_tW_{xz}^T+h_{t-1}W_{hz}^T) \\\\\\\\\n",
    "&\\tilde{h}\\_t=\\tanh(x_tW_{xn}^T+r_t\\odot(h_{t-1}W_{hn}^T)) \\\\\\\\\n",
    "&h_t=(1-z_t)\\odot\\tilde{h}\\_t+z_t\\odot h_{t-1}\n",
    "\\end{align*}$$\n",
    "\n",
    "Idea: Gated Recurrent Unit - use 2 gates to address long-term info propagation issue in RNN:\n",
    "1. **Reset gate**: determine how much of $ h_{t-1}$ should be ignored when computing $\\tilde{h}\\_t $.\n",
    "2. **Update gate**: determine how much of $ h_{t-1}$ should be retained for $h_t $.\n",
    "3. **Candidate**: calculate candidate $ \\tilde{h}\\_t$ with reset $h_{t-1} $.\n",
    "4. **Final**: calculate weighted average between candidate $ \\tilde{h}\\_t$ and prev state $h_{t-1} $ with the retain ratio.\n",
    "\n",
    "Notations:\n",
    "- $ r_t$: reset gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ z_t$: update gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ \\tilde{h}\\_t$: candidate hidden state at time $t$ of shape $(m,H_{out}) $\n",
    "- $ \\odot $: element-wise product\n",
    "\n",
    "## LSTM\n",
    "\n",
    "```{image} ../images/LSTM.png\n",
    ":width: 400\n",
    ":align: center\n",
    "```\n",
    "\n",
    "$$\\begin{align*}\n",
    "&i_t=\\sigma(x_tW_{xi}^T+h_{t-1}W_{hi}^T) \\\\\\\\\n",
    "&f_t=\\sigma(x_tW_{xf}^T+h_{t-1}W_{hf}^T) \\\\\\\\\n",
    "&\\tilde{c}\\_t=\\tanh(x_tW_{xc}^T+h_{t-1}W_{hc}^T) \\\\\\\\\n",
    "&c_t=f_t\\odot c_{t-1}+i_t\\odot \\tilde{c}\\_t \\\\\\\\\n",
    "&o_t=\\sigma(x_tW_{xo}^T+h_{t-1}W_{ho}^T) \\\\\\\\\n",
    "&h_t=o_t\\odot\\tanh(c_t)\n",
    "\\end{align*}$$\n",
    "\n",
    "Idea: Long Short-Term Memory - use 3 gates:\n",
    "1. **Input gate**: determine what new info from $ x_t$ should be added to cell state $c_t $.\n",
    "2. **Forget gate**: determine what info from prev cell $ c_{t-1} $ should be forgotten.\n",
    "3. **Candidate cell**: create a new candidate cell from $ x_t$ and $h_{t-1} $.\n",
    "4. **Update cell**: use $ i_t$ and $f_t $ to combine prev and new candidate cells.\n",
    "5. **Output gate**: determine what info from curr cell $ c_t$ should be added to output $h_t $.\n",
    "6. **Final**: simply apply $ o_t$ to activated cell $c_t $.\n",
    "\n",
    "Notations:\n",
    "- $ i_t$: input gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ f_t$: forget gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ c_t$: cell state at time $t$ of shape $(m,H_{cell}) $\n",
    "- $ o_t$: output gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ H_{cell}$: cell hidden size (in most cases same as $H_{out} $)\n",
    "\n",
    "## Bidirectional\n",
    "## Stacked\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Activation\n",
    "- **What**: An element-wise non-linear function over a layer's output.\n",
    "- **Why**: Non-linearity.\n",
    "    - Without it, a full NN is just simple linear regression.\n",
    "\n",
    "## Binary-like\n",
    "- **What**: Functions with near-binary outputs.\n",
    "- **Why**: Biology - Biological neurons generally:\n",
    "    - react little to small inputs\n",
    "    - react rapidly after input stimulus passes a threshold\n",
    "    - converge to a max as stimulus increases\n",
    "\n",
    "### Sigmoid\n",
    "- **What**: Sigmoid function.\n",
    "- **Why**: Mathematically convenient $\\leftarrow$ Smooth gradient.\n",
    "\n",
    "```{admonition} Math\n",
    ":class: note, dropdown\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "y=\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "- $\\sigma(z)\\in(0,1)$\n",
    "- $\\sigma(0)=0.5$\n",
    "\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial z}=\\frac{\\partial\\mathcal{L}}{\\partial y}y(1-y)\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} Q&A\n",
    ":class: tip, dropdown\n",
    "*Cons?*\n",
    "-  Vanishing gradient.\n",
    "-  Non-zero centric bias $\\rightarrow$ Non-zero mean activations\n",
    "```\n",
    "\n",
    "### Tanh\n",
    "- **What**: Tanh function.\n",
    "- **Why**: Mathematically convenient $\\leftarrow$ Smooth gradient.\n",
    "\n",
    "```{admonition} Math\n",
    ":class: note, dropdown\n",
    "Forward:\n",
    "\n",
    "$$\n",
    "y=\\tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n",
    "$$\n",
    "- $\\tanh(z)\\in(-1,1)$\n",
    "- $\\tanh(0)=0$\n",
    "\n",
    "Backward:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial z}=\\frac{\\partial\\mathcal{L}}{\\partial y}(1-y^2)\n",
    "$$\n",
    "```\n",
    "\n",
    "```{admonition} Q&A\n",
    ":class: tip, dropdown\n",
    "*Pros?*\n",
    "- Zero-centered.\n",
    "\n",
    "*Cons?*\n",
    "-  Vanishing gradient.\n",
    "```\n",
    "\n",
    "## Linear Units (Rectified)\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(z)=\\max{(0,z)}\n",
    "$$\n",
    "\n",
    "Name: Rectified Linear Unit\n",
    "\n",
    "Idea:\n",
    "- convert negative linear outputs to 0.\n",
    "\n",
    "Pros:\n",
    "- no vanishing gradient\n",
    "- activate fewer neurons\n",
    "- much less computationally expensive compared to sigmoid and tanh.\n",
    "\n",
    "Cons:\n",
    "- dying ReLU: if most inputs are negative, then most neurons output 0 $ \\rightarrow$ no gradient for such neurons $\\rightarrow$ no param update $\\rightarrow $ they die. (NOTE: A SOLVABLE DISADVANTAGE)\n",
    "\n",
    "    - Cause 1: high learning rate $ \\rightarrow$ too much subtraction in param update $\\rightarrow$ weight too negative $\\rightarrow $ input for neuron too negative.\n",
    "    - Cause 2: bias too negative $ \\rightarrow $ input for neuron too negative.\n",
    "\n",
    "-  activation explosion as $ z\\rightarrow\\infty $. (NOTE: NOT A SEVERE DISADVANTAGE SO FAR)\n",
    "\n",
    "\n",
    "### LReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{LReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Leaky Rectified Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha\\in(0,1) $: hyperparam (negative slope), default 0.01.\n",
    "\n",
    "Idea:\n",
    "- scale negative linear outputs by $ \\alpha $.\n",
    "\n",
    "Pros:\n",
    "- no dying ReLU.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "### PReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{PReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "Name: Parametric Rectified Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha\\in(0,1) $: learnable param (negative slope), default 0.25.\n",
    "\n",
    "Idea:\n",
    "- scale negative linear outputs by a learnable $ \\alpha $.\n",
    "\n",
    "Pros:\n",
    "- a variable, adaptive param learned from data.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than LReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### RReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{RReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Randomized Rectified Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha\\sim\\mathrm{Uniform}(l,u) $: a random number sampled from a uniform distribution.\n",
    "- $ l,u $: hyperparams (lower bound, upper bound)\n",
    "\n",
    "Idea:\n",
    "- scale negative linear outputs by a random $ \\alpha $.\n",
    "\n",
    "Pros:\n",
    "- reduce overfitting by randomization.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than LReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "## Linear Units (Exponential)\n",
    "\n",
    "### ELU\n",
    "\n",
    "$$\n",
    "\\mathrm{ELU}(z)=\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0 \\\\\\\\\n",
    "\\alpha(e^z-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.\n",
    "\n",
    "Idea:\n",
    "- convert negative linear outputs to the non-linear exponential function above.\n",
    "\n",
    "Pros:\n",
    "- mean unit activation is closer to 0 $ \\rightarrow $ reduce bias shift (i.e., non-zero mean activation is intrinsically a bias for the next layer.)\n",
    "- lower computational complexity compared to batch normalization.\n",
    "- smooth to $ -\\alpha $ slowly with smaller derivatives that decrease forwardprop variation.\n",
    "- faster learning and higher accuracy for image classification in practice.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### SELU\n",
    "\n",
    "$$\n",
    "\\mathrm{SELU}(z)=\\lambda\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0 \\\\\n",
    "\\alpha(e^z-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Scaled Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.67326.\n",
    "- $ \\lambda $: hyperparam (scale), default 1.05070.\n",
    "\n",
    "Idea:\n",
    "- scale ELU.\n",
    "\n",
    "Pros:\n",
    "- self-normalization $ \\rightarrow $ activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance.\n",
    "\n",
    "Cons:\n",
    "- more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### CELU\n",
    "\n",
    "$$\n",
    "\\mathrm{CELU}(z)=\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0\\\\\n",
    "\\alpha(e^{\\frac{z}{\\alpha}}-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Continuously Differentiable Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.\n",
    "\n",
    "Idea:\n",
    "- scale the exponential part of ELU with $ \\frac{1}{\\alpha} $ to make it continuously differentiable.\n",
    "\n",
    "Pros:\n",
    "- smooth gradient due to continuous differentiability (i.e., $ \\mathrm{CELU}'(0)=1 $).\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ELU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "## Linear Units (Others)\n",
    "\n",
    "### GELU\n",
    "\n",
    "$$\n",
    "\\mathrm{GELU}(z)=z*\\Phi(z)=0.5z(1+\\tanh{[\\sqrt{\\frac{2}{\\pi}}(z+0.044715z^3)]})\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Gaussian Error Linear Unit\n",
    "\n",
    "Idea:\n",
    "- weigh each output value by its Gaussian cdf.\n",
    "\n",
    "Pros:\n",
    "- throw away gate structure and add probabilistic-ish feature to neuron outputs.\n",
    "- seemingly better performance than the ReLU and ELU families, SOTA in transformers.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ReLU.\n",
    "- lack of practical testing at the moment.\n",
    "\n",
    "\n",
    "\n",
    "### SiLU\n",
    "\n",
    "$$\n",
    "\\mathrm{SiLU}(z)=z*\\sigma(z)\n",
    "$$\n",
    "\n",
    "Name: Sigmoid Linear Unit\n",
    "\n",
    "Idea:\n",
    "- weigh each output value by its sigmoid value.\n",
    "\n",
    "Pros:\n",
    "- throw away gate structure.\n",
    "- seemingly better performance than the ReLU and ELU families.\n",
    "\n",
    "Cons:\n",
    "- worse than GELU.\n",
    "\n",
    "\n",
    "\n",
    "### Softplus\n",
    "\n",
    "$$\n",
    "\\mathrm{softplus}(z)=\\frac{1}{\\beta}\\log{(1+e^{\\beta z})}\n",
    "$$\n",
    "\n",
    "\n",
    "Idea:\n",
    "- smooth approximation of ReLU.\n",
    "\n",
    "Pros:\n",
    "- differentiable and thus theoretically better than ReLU.\n",
    "\n",
    "Cons:\n",
    "- empirically far worse than ReLU in terms of computation and performance.\n",
    "\n",
    "\n",
    "\n",
    "## Multiclass\n",
    "\n",
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(z_i)=\\frac{\\exp{(z_i)}}{\\sum_j{\\exp{(z_j)}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Idea:\n",
    "- convert each value $ z_i$ in the output tensor $\\mathbf{z}$ into its corresponding exponential probability s.t. $\\sum_i{\\mathrm{softmax}(z_i)}=1 $.\n",
    "\n",
    "Pros:\n",
    "- your single best choice for multiclass classification.\n",
    "\n",
    "Cons:\n",
    "- mutually exclusive classes (i.e., one input can only be classified into one class.)\n",
    "\n",
    "### Softmin\n",
    "\n",
    "$$\n",
    "\\mathrm{softmin}(z_i)=\\mathrm{softmax}(-z_i)=\\frac{\\exp{(-z_i)}}{\\sum_j{\\exp{(-z_j)}}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "- reverse softmax.\n",
    "\n",
    "Pros:\n",
    "- suitable for multiclass classification.\n",
    "\n",
    "Cons:\n",
    "- why not softmax."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}