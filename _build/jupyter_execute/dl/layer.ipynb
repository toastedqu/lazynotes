{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "552bc1d9",
   "metadata": {},
   "source": [
    "# Layer\n",
    "A layer is a mapping from input tensor $X$ to output tensor $Y$.\n",
    "\n",
    "Let $g$ denote the gradient $\\frac{\\partial\\mathcal{L}}{\\partial y}$ for readability.\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Basic\n",
    "## Linear\n",
    "- **What**: Linear transformation.\n",
    "- **Why**: Linear algebra. The simplest way to transform data, learn patterns, and make predictions.\n",
    "- **How**: Multiply the input features with weights and add a bias on top.\n",
    "- **When**: There is a linear relationship between input & output.\n",
    "- **Where**: Anywhere. Typically used for feature dimension conversion.\n",
    "- **Pros**:\n",
    "    - Simple.\n",
    "    - High interpretability.\n",
    "    - Low computational cost.\n",
    "    - Widely used.\n",
    "- **Cons**:  Cannot capture non-linear/complex patterns.\n",
    "\n",
    "<!-- ````{admonition} Math\n",
    ":class: note, dropdown\n",
    "```{tab} One sample\n",
    "**Notations**:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{H_{out}}$: Output vector.\n",
    "- Params:\n",
    "    - $W\\in\\mathbb{R}^{H_{out}\\times H_{in}}$: Weight matrix.\n",
    "    - $\\textbf{b}\\in\\mathbb{R}^{H_{out}}$: Bias vector.\n",
    "- Hyperparams:\n",
    "    - $H_{in}$: Input feature dimension.\n",
    "    - $H_{out}$: Output feature dimension.\n",
    "\n",
    "**Forward**:\n",
    "\n",
    "$$\n",
    "\\textbf{y}=\\textbf{x}W^T+\\textbf{b}\n",
    "$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial W}=\\textbf{g}\\times\\textbf{x} \\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{b}}=\\textbf{g}\\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}}=\\textbf{g}W\n",
    "\\end{align*}$$\n",
    "```\n",
    "```{tab} Multi samples\n",
    "**Notations**:\n",
    "- IO:\n",
    "    - $\\mathbf{X}\\in\\mathbb{R}^{*\\times H_{in}}$: Input tensor.\n",
    "    - $\\mathbf{Y}\\in\\mathbb{R}^{*\\times H_{out}}$: Output tensor.\n",
    "- Params:\n",
    "    - $W\\in\\mathbb{R}^{H_{out}\\times H_{in}}$: Weight matrix.\n",
    "    - $\\textbf{b}\\in\\mathbb{R}^{H_{out}}$: Bias vector.\n",
    "- Hyperparams:\n",
    "    - $H_{in}$: Input feature dimension.\n",
    "    - $H_{out}$: Output feature dimension.\n",
    "\n",
    "**Forward**:\n",
    "\n",
    "$$\n",
    "\\textbf{Y}=\\textbf{X}W^T+\\textbf{b}\n",
    "$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial W}=\\textbf{g}^T\\textbf{X} \\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{b}}=\\sum_*\\textbf{g}_*\\\\\n",
    "&\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}}=\\textbf{g}W\n",
    "\\end{align*}$$\n",
    "```\n",
    "```` -->\n",
    "\n",
    "## Dropout\n",
    "- **What**: Randomly ignore some neurons during training. ([paper](https://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf))\n",
    "- **Why**: To reduce overfitting.\n",
    "- **How**:\n",
    "    1. Randomly set a fraction ($p$) of neurons to 0.\n",
    "    2. Scale the outputs/gradients on active neurons by $\\frac{1}{1-p}$.\n",
    "- **When**: The current model is overfitting on the current training data.\n",
    "- **Where**: Typically after linear layers & convolutional layers.\n",
    "- **Pros**: Simple, efficient regularization.\n",
    "- **Cons**: Requires hyperparam tuning; Can slow down convergence.\n",
    "\n",
    "<!-- ````{admonition} Math\n",
    ":class: note, dropdown\n",
    "```{tab} One sample\n",
    "**Notations**:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{H_{in}}$: Output vector.\n",
    "- Hyperparams:\n",
    "    - $p$: Dropout probability.\n",
    "\n",
    "**Forward**:\n",
    "\n",
    "$$\n",
    "\\textbf{y}=\\frac{1}{1-p}\\textbf{x}_\\text{active}\n",
    "$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}_\\text{active}}= \\frac{1}{1-p}\\mathbf{g}\n",
    "$$\n",
    "```\n",
    "```{tab} Multi samples\n",
    "**Notations**:\n",
    "- IO:\n",
    "    - $\\mathbf{X}\\in\\mathbb{R}^{*\\times H_{in}}$: Input tensor.\n",
    "    - $\\mathbf{Y}\\in\\mathbb{R}^{*\\times H_{in}}$: Output tensor.\n",
    "- Hyperparams:\n",
    "    - $p$: Dropout probability.\n",
    "\n",
    "**Forward**:\n",
    "\n",
    "$$\n",
    "\\textbf{Y}=\\frac{1}{1-p}\\textbf{X}_\\text{active}\n",
    "$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{X}_\\text{active}}= \\frac{1}{1-p}\\mathbf{g}\n",
    "$$\n",
    "```\n",
    "```` -->\n",
    "\n",
    "## Residual Connection\n",
    "- **What**: Model the residual ($Y-X$) instead of the output ($Y$). ([paper](https://arxiv.org/pdf/1512.03385))\n",
    "- **Why**: To mitigate [vanishing/exploding gradients](../dl/issues.md/#vanishing/exploding-gradient).\n",
    "- **How**:\n",
    "    1. Add the input $X$ to the block output $F(X)$.\n",
    "    2. If the feature dimension of $X$ and $F(X)$ doesn't match, use a shortcut linear layer on $X$ to change its feature dimension.\n",
    "- **When**: There is clear evidence of convergence failures or extreme gradient values.\n",
    "- **Where**: Inside deep NNs.\n",
    "- **Pros**: Higher performance on complex tasks.\n",
    "- **Cons**: Slightly high computational cost.\n",
    "\n",
    "<!-- ````{admonition} Math\n",
    ":class: note, dropdown\n",
    "```{tab} One sample\n",
    "**Notation**:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{H_{out}}$: Output vector.\n",
    "- Hyperparams:\n",
    "    - $F(\\cdot)\\in\\mathbb{R}^{H_{out}}$: The aggregate function of all layers within the residual block.\n",
    "\n",
    "**Forward**:\n",
    "\n",
    "$$\n",
    "\\textbf{y}=F(\\textbf{x})+\\textbf{x}\n",
    "$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}}=\\mathbf{g}(1+\\frac{\\partial F(\\textbf{x})}{\\partial\\textbf{x}})\n",
    "$$\n",
    "```\n",
    "```{tab} Multi samples\n",
    "**Notation**:\n",
    "- IO:\n",
    "    - $\\mathbf{x}\\in\\mathbb{R}^{*\\times H_{in}}$: Input vector.\n",
    "    - $\\mathbf{y}\\in\\mathbb{R}^{*\\times H_{out}}$: Output vector.\n",
    "- Hyperparams:\n",
    "    - $F(\\cdot)\\in\\mathbb{R}^{H_{out}}$: The aggregate function of all layers within the residual block.\n",
    "\n",
    "**Forward**:\n",
    "\n",
    "$$\n",
    "\\textbf{Y}=F(\\textbf{X})+\\textbf{X}\n",
    "$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{X}}=\\mathbf{g}(1+\\frac{\\partial F(\\textbf{X})}{\\partial\\textbf{X}})\n",
    "$$\n",
    "```\n",
    "```` -->\n",
    "\n",
    "## Normalization\n",
    "### Batch Normalization\n",
    "- **What**: Normalize each feature across the input samples to zero mean and unit variance. ([paper](https://arxiv.org/pdf/1502.03167))\n",
    "- **Why**: To mitigate internal covariate shift.\n",
    "- **How**:\n",
    "    1. Calculate the mean and variance for each batch.\n",
    "    2. Normalize the batch.\n",
    "    3. Scale and shift the normalized output using learnable params.\n",
    "- **When**: Each mini-batch is representative of the overall input distribution to accurately estimate the mean and variance.\n",
    "- **Where**: Typically applied before activation functions, after linear layers & convolutional layers.\n",
    "- **Pros**:\n",
    "    - Accelerates training with higher learning rates.\n",
    "    - Reduces sensitivity to weight initialization.\n",
    "    - Mitigates [vanishing/exploding gradients](../dl/issues.md/#vanishing/exploding-gradient).\n",
    "- **Cons**:\n",
    "    - Adds computation overhead and complexity.\n",
    "    - Causes potential issues in certain cases like small mini-batches or when batch statistics differ from overall dataset statistics.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown -->\n",
    "<!-- **Notation**:\n",
    "- IO:\n",
    "    - $\\mathbf{X}\\in\\mathbb{R}^{m\\times n}$: Input matrix.\n",
    "    - $\\mathbf{Y}\\in\\mathbb{R}^{m\\times n}$: Output matrix.\n",
    "- Params:\n",
    "    - $\\gamma\\in\\mathbb{R}$: Scale param.\n",
    "    - $\\beta\\in\\mathbb{R}$: Shift param.\n",
    "\n",
    "**Forward**:\n",
    "1. Calculate the mean and variance for each batch.\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    \\boldsymbol{\\mu}_B&=\\frac{1}{m}\\sum_{i=1}^{m}\\textbf{x}_i\\\\\n",
    "    \\boldsymbol{\\sigma}_B^2&=\\frac{1}{m}\\sum_{i=1}^{m}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)^2\n",
    "    \\end{align*}$$\n",
    "\n",
    "2. Normalize each batch.\n",
    "\n",
    "    $$\n",
    "    \\textbf{z}_i=\\frac{\\textbf{x}_i-\\boldsymbol{\\mu}_B}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}}\n",
    "    $$\n",
    "\n",
    "    where $\\epsilon$ is a small constant to avoid dividing by 0.\n",
    "\n",
    "3. Scale and shift the normalized output.\n",
    "\n",
    "    $$\n",
    "    \\textbf{y}_i=\\gamma\\textbf{z}_i+\\beta\n",
    "    $$\n",
    "\n",
    "**Backward**:\n",
    "1. Gradient w.r.t. params:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\gamma}=\\sum_{i=1}^{m}\\textbf{g}_i\\textbf{z}_i\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\beta}=\\sum_{i=1}^{m}\\textbf{g}_i\n",
    "    \\end{align*}$$\n",
    "2. Gradient w.r.t. input:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}=\\gamma\\textbf{g}_i\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\sigma}_B^2}=\\sum_{i=1}^{m}\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)\\left(-\\frac{1}{2}(\\boldsymbol{\\sigma}_B^2+\\epsilon)^{-\\frac{3}{2}}\\right)\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mu}_B}=\\sum_{i=1}^{m}\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}\\cdot\\left(-\\frac{1}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}}\\right)+\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\sigma}_B^2}\\cdot\\left(-\\frac{2}{m}\\sum_{i=1}^{m}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)\\right)\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{x}_i}=\\frac{1}{\\sqrt{\\boldsymbol{\\sigma}_B^2+\\epsilon}}\\left(\\frac{\\partial\\mathcal{L}}{\\partial\\textbf{z}_i}+\\frac{2}{m}\\frac{\\partial\\ma                                                                         thcal{L}}{\\partial\\boldsymbol{\\sigma}_B^2}(\\textbf{x}_i-\\boldsymbol{\\mu}_B)+\\frac{1}{m}\\frac{\\partial\\mathcal{L}}{\\partial\\boldsymbol{\\mu}_B}\\right)\n",
    "    \\end{align*}$$\n",
    "``` -->\n",
    "\n",
    "### Layer Normalization\n",
    "- **What**: Normalize each sample across the input features to zero mean and unit variance. ([paper](https://arxiv.org/pdf/1607.06450))\n",
    "- **Why**: Batch normalization depends on the batch size.\n",
    "    - When it's too big, high computational cost.\n",
    "    - When it's too small, the batch may not be representative of the underlying data distribution.\n",
    "    - Hyperparam tuning is required to find the optimal batch size, leading to high computational cost.\n",
    "- **How**:\n",
    "    1. Calculate the mean and variance for each feature.\n",
    "    2. Normalize the feature.\n",
    "    3. Scale and shift the normalized output using learnable params.\n",
    "- **When**: Layer-wise statistics are more representative than batch-wise statistics.\n",
    "- **Where**: Typically applied to NLP tasks that are sensitive to batch size.\n",
    "- **Pros**:\n",
    "    - Reduces hyperparam tuning effort.\n",
    "    - High consistency during training and inference.\n",
    "    - Mitigates [vanishing/exploding gradients](#residual-connection).\n",
    "- **Cons**:\n",
    "    - Adds computation overhead and complexity.\n",
    "    - Inapplicable in CNNs due to varied statistics of spatial features.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "It's easy to explain with the vector form for batch normalization, but it's more intuitive to explain with the scalar form for layer normalization.\n",
    "\n",
    "- Notation\n",
    "    - IO:\n",
    "        - $x_{ij}\\in\\mathbb{R}$: $j$th feature value for $i$th input sample.\n",
    "        - $y_{ij}\\in\\mathbb{R}$: $j$th feature value for $i$th output sample.\n",
    "    - Params:\n",
    "        - $\\boldsymbol{\\gamma}\\in\\mathbb{R}^n$: Scale param.\n",
    "        - $\\boldsymbol{\\beta}\\in\\mathbb{R}^n$: Shift param.\n",
    "**Forward**:\n",
    "    1. Calculate the mean and variance for each feature.\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        \\mu_i&=\\frac{1}{n}\\sum_{j=1}^{n}x_{ij}\\\\\n",
    "        \\sigma_i^2&=\\frac{1}{n}\\sum_{j=1}^{n}(x_{ij}-\\mu_i)^2\n",
    "        \\end{align*}$$\n",
    "\n",
    "    2. Normalize each feature.\n",
    "\n",
    "        $$\n",
    "        z_{ij}=\\frac{x_{ij}-\\mu_i}{\\sqrt{\\sigma_i^2+\\epsilon}}\n",
    "        $$\n",
    "\n",
    "        where $\\epsilon$ is a small constant to avoid dividing by 0.\n",
    "\n",
    "    3. Scale and shift the normalized output.\n",
    "\n",
    "        $$\n",
    "        y_{ij}=\\gamma_jz_{ij}+\\beta_j\n",
    "        $$\n",
    "**Backward**:\n",
    "    1. Gradient w.r.t. params:\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        &\\frac{\\partial\\mathcal{L}}{\\partial\\gamma_j}=\\sum_{i=1}^{m}g_{ij}z_{ij}\\\\\n",
    "        &\\frac{\\partial\\mathcal{L}}{\\partial\\beta_j}=\\sum_{i=1}^{m}g_{ij}\n",
    "        \\end{align*}$$\n",
    "    2. Gradient w.r.t. input:\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        &\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}=\\gamma_jg_{ij}\\\\\n",
    "        &\\frac{\\partial\\mathcal{L}}{\\partial\\sigma_i^2}=\\sum_{j=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}(x_{ij}-\\mu_i)\\left(-\\frac{1}{2}(\\sigma_i^2+\\epsilon)^{-\\frac{3}{2}}\\right)\\\\\n",
    "        &\\frac{\\partial\\mathcal{L}}{\\partial\\mu_i}=\\sum_{j=1}^{n}\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}\\cdot\\left(-\\frac{1}{\\sqrt{\\sigma_i^2+\\epsilon}}\\right)+\\frac{\\partial\\mathcal{L}}{\\partial\\sigma_i^2}\\cdot\\left(-\\frac{2}{n}\\sum_{j=1}^{n}(x_{ij}-\\mu_i)\\right)\\\\\n",
    "        &\\frac{\\partial\\mathcal{L}}{x_{ij}}=\\frac{1}{\\sqrt{\\sigma_i^2+\\epsilon}}\\left(\\frac{\\partial\\mathcal{L}}{\\partial z_{ij}}+\\frac{2}{n}\\frac{\\partial\\mathcal{L}}{\\partial\\sigma_i^2}(x_{ij}-\\mu_i)+\\frac{1}{n}\\frac{\\partial\\mathcal{L}}{\\partial\\mu_i}\\right)\n",
    "        \\end{align*}$$\n",
    "``` -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Transformer\n",
    "```{image} ../images/transformer.png\n",
    ":width: 500\n",
    ":align: center\n",
    "```\n",
    "- **What**: **Self-attention** for sequential data.\n",
    "- **Why**: RNNs had severe limitations:\n",
    "    - **Lack of long-range dependencies**: RNNs struggled to capture long-range dependencies in sequences due to vanishing gradients.\n",
    "    - **Sequential computation**: The training for RNN was extremely slow, especially for long sequences, due to its sequential nature.\n",
    "- **How**:\n",
    "    - [Positional Encoding](#positional-encoding)\n",
    "    - [Residual Connection](#residual-connection)\n",
    "    - [Multi-Head Attention](#multi-head-attention)\n",
    "    - [Layer Normalization](#layer-normalization)\n",
    "    - [Position-wise Feed-Forward Networks](#postion-wise-feed-forward-networks)\n",
    "- **When**:\n",
    "    - There are sufficient data.\n",
    "    - There are sufficient computational resources.\n",
    "    - The data can be converted into a sequential format and processed in parallel without affecting the underlying data distribution.\n",
    "- **Where**: Widely applicable.\n",
    "- **Pros**:\n",
    "    - Long-range dependencies.\n",
    "    - Parallel processing $\\rightarrow$ Lower training time compared to RNN.\n",
    "    - High scalability.\n",
    "    - Allows transfer learning.\n",
    "    - Wide range of applications.\n",
    "- **Cons**:\n",
    "    - High computational cost $\\rightarrow$ $O(n^2)$, where $n$ is sequence length.\n",
    "    -\n",
    "\n",
    "**Training**:\n",
    "- **Parameters**:\n",
    "    - Encoder: $ \\text{\\\\#params}=h\\cdot d\\_{\\text{model}}\\cdot (2d\\_k+d\\_v)+2\\cdot d\\_{\\text{model}}\\cdot d\\_{\\text{ff}} $\n",
    "    - Decoder: $ \\text{\\\\#params}=2\\cdot h\\cdot d\\_{\\text{model}}\\cdot (2d\\_k+d\\_v)+2\\cdot d\\_{\\text{model}}\\cdot d\\_{\\text{ff}} $\n",
    "- **Hyperparams**:\n",
    "    - #layers\n",
    "    - hidden size\n",
    "    - #heads\n",
    "    - learning rate (& warm-up steps)\n",
    "\n",
    "**Inference**:\n",
    "1. Process input tokens in parallel via encoder.\n",
    "2. Generate output tokens sequentially via decoder.\n",
    "\n",
    "**Pros**:\n",
    "- high computation efficiency (training & inference)\n",
    "- high performance\n",
    "- wide applicability\n",
    "\n",
    "**Cons**:\n",
    "- require sufficient computation resources\n",
    "- require sufficient large-scale data\n",
    "\n",
    "## Positional Encoding\n",
    "**What**: Positional Encoding encodes sequence order info of tokens into embeddings.\n",
    "\n",
    "**Why**: So that the model can still make use of the sequence order info since no recurrence/convolution is available for it.\n",
    "\n",
    "**Where**: After tokenization & Before feeding into model.\n",
    "\n",
    "**When**: The hypothesis that **relative positions** allow the model to learn to attend easier holds.\n",
    "\n",
    "**How**: sinusoid with wavelengths from a geometric progression from $ 2\\pi$ to $10000\\cdot2\\pi $\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{PE}\\_{(pos,2i)}&=\\sin(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}})\\\\\\\\\n",
    "\\text{PE}\\_{(pos,2i+1)}&=\\cos(\\frac{pos}{10000^{\\frac{2i}{d_\\text{model}}}})\n",
    "\\end{align*}$$\n",
    "\n",
    "- $ pos $: absolute position of the token\n",
    "- $ i $: dimension\n",
    "- For any fixed offset $ k$, $PE_{pos+k}$ is a linear function of $PE_{pos} $.\n",
    "\n",
    "**Pros**:\n",
    "- allow model to extrapolate to sequence lengths longer than the training sequences\n",
    "\n",
    "**Cons**: ???\n",
    "\n",
    "\n",
    "\n",
    "## Scaled Dot-Product Attention\n",
    "\n",
    "```{image} ../images/scaled_dot_product_attention.png\n",
    ":width: 200\n",
    ":align: center\n",
    "```\n",
    "\n",
    "**What**: An effective & efficient variation of self-attention.\n",
    "\n",
    "**Why**:\n",
    "- The end goal is **Attention** - \"Which parts of the sentence should we focus on?\"\n",
    "- We want to **capture the most relevant info** in the sentence.\n",
    "- And we also want to **keep track of all info** in the sentence as well, just with different weights.\n",
    "- We want to create **contextualized representations** of the sentence.\n",
    "- Therefore, attention mechanism - we want to assign different attention scores to each token.\n",
    "\n",
    "**When**:\n",
    "- **linearity**: Relationship between tokens can be captured via linear transformation.\n",
    "- **Position independence**: Relationship between tokens are independent of positions (fixed by Positional Encoding).\n",
    "\n",
    "**How**:\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q,K,V)=\\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V\n",
    "$$\n",
    "\n",
    "- Preliminaries:\n",
    "    - **Query (Q)**: a **question** about a token - \"How important is this token in the context of the whole sentence?\"\n",
    "    - **Key (K)**: a piece of **unique identifier** about a token - \"Here's something unique about this token.\"\n",
    "    - **Value (V)**: the **actual meaning** of a token - \"Here's the content about this token.\"\n",
    "- Procedure:\n",
    "    1. **Compare the similarity** between the **Q** of one word and the **K** of every other word.\n",
    "        - The more similar, the more attention we should give to that word for the queried word.\n",
    "    2. **Scale down** by $ \\sqrt{d_k} $ to avoid the similarity scores being too large.\n",
    "        - Dot products grow large in magnitude, pushing the softmax function into regions where it has extremely small gradients.\n",
    "        - They grow large because, if $ q,k\\sim N(0,1)$, then $qk=\\sum_{i=1}^{d_k}q_ik_i\\sim N(0,d_k) $.\n",
    "    3. Convert the attention scores into a **probability distribution**.\n",
    "        - Softmax sums up to 1 and emphasizes important attention weights (and reduces the impact of negligible ones).\n",
    "    4. Calculate the **weighted combination** of all words, for each queried word, as the final attention score.\n",
    "\n",
    "**Pros**:\n",
    "- significantly higher computational efficiency (time & space) than additive attention\n",
    "\n",
    "**Cons**:\n",
    "- outperformed by additive attention if without scaling for large values of $ d_k $\n",
    "\n",
    "\n",
    "\n",
    "## Multi-Head Attention\n",
    "\n",
    "```{image} ../images/mha.png\n",
    ":width: 300\n",
    ":align: center\n",
    "```\n",
    "\n",
    "**What**: A combination of multiple scaled dot-product attention heads in parallel.\n",
    "- Masked MHA: mask the succeeding tokens off because they can't be seen during decoding.\n",
    "\n",
    "**Why**: To allow the model to jointly attend to info from different representation subspaces at different positions.\n",
    "\n",
    "**When**: The assumption of independence of attention heads holds.\n",
    "\n",
    "**How**:\n",
    "\n",
    "$$\\begin{align*}\n",
    "\\text{MultiHead}(Q,K,V)&=\\text{Concat}(\\text{head}_1,\\cdots,\\text{head}_h)W^O \\\\\\\\\n",
    "\\text{head}_i&=\\text{Attention}(QW_i^Q,KW_i^K,VW_i^V)\n",
    "\\end{align*}$$\n",
    "\n",
    "- $ W_i^Q\\in\\mathbb{R}^{d_\\text{model}\\times d_k},W_i^K\\in\\mathbb{R}^{d_\\text{model}\\times d_k},W_i^V\\in\\mathbb{R}^{d_\\text{model}\\times d_v} $: learnable linear projection params.\n",
    "- $ W^O\\in\\mathbb{R}^{d_\\text{model}\\times hd_v} $: learnable linear combination weights.\n",
    "- $ h=8, d_k=d_v=\\frac{d_\\text{model}}{h}=64 $ in the original paper.\n",
    "\n",
    "**Pros**:\n",
    "- better performance than single head\n",
    "\n",
    "**Cons**: ???\n",
    "\n",
    "## Postion-wise Feed-Forward Networks\n",
    "**What**: 2 linear transformations with ReLU in between.\n",
    "\n",
    "**Why**: Just like 2 convolutions with kernel size 1.\n",
    "\n",
    "**How**:\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x)=\\max(0,xW_1+b_1)W_2+b_2\n",
    "$$\n",
    "\n",
    "- $ d_\\text{model}=512 $\n",
    "- $ d_\\text{FF}=2048 $\n",
    "\n",
    "<!-- # Transformer\n",
    "## Positional Encoding\n",
    "## Attention\n",
    "### Self-Attention\n",
    "### Multi-Head Attention -->\n",
    "\n",
    "# Convolutional\n",
    "- **What**: Apply a set of filters to input data to extract local features. ([paper](https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf))\n",
    "- **Why**: To learn spatial hierarchies of features.\n",
    "- **How**: Slide multiple filters/kernel (i.e., small matrices) over the input data.\n",
    "    - At each step, perform element-wise multiplication and summation between each filter and the scanned area, producing a feature map.\n",
    "- **When**: Used with grid-like data such as images and video frames.\n",
    "- **Where**: Computer Vision.\n",
    "- **Pros**:\n",
    "    - Translation invariance.\n",
    "    - Efficiently captures spatial hierarchies.\n",
    "- **Cons**:\n",
    "    - High computational cost for big data\n",
    "    - Requires big data to be performant.\n",
    "    - Requires extensive hyperparam tuning.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "**Notations**:\n",
    "    - IO:\n",
    "        - $\\mathbf{X}\\in\\mathbb{R}^{H_{in}\\times W_{in}\\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{out}}$: Output volume.\n",
    "    - Params:\n",
    "        - $\\mathbf{W}\\in\\mathbb{R}^{F_{H}\\times F_{W}\\times C_{out}\\times C_{in}}$: Filters.\n",
    "        - $\\mathbf{b}\\in\\mathbb{R}^{C_{out}}$: Biases.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $C_{out}$: #Filters (i.e., #Output channels).\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "        - $p$: Padding size.\n",
    "**Forward**:\n",
    "\n",
    "    $$\n",
    "    Y_{h,w,c_{out}}=\\sum_{c_{in}=1}^{C_{in}}\\sum_{i=1}^{f_h}\\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\\cdot X_{sh+i-p,sw+j-p,c_{in}}+b_{c_{out}}\n",
    "    $$\n",
    "\n",
    "    where\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    H_{out}&=\\left\\lfloor\\frac{H_{in}+2p-f_h}{s}\\right\\rfloor+1\\\\\n",
    "    W_{out}&=\\left\\lfloor\\frac{W_{in}+2p-f_w}{s}\\right\\rfloor+1\n",
    "    \\end{align*}$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial W_{i,j,c_{out},c_{in}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\cdot X_{sh+i-p, sw+j-p, c_{in}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial b_{c_{out}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial X_{i,j,c_{in}}}=\\sum_{c_{out}=1}^{C_{out}}\\sum_{h=1}^{f_h}\\sum_{w=1}^{f_w}g_{h,w,c_{out}}\\cdot W_{i-sh+p,j-sw+p,c_{out},c_{in}}\n",
    "    \\end{align*}$$\n",
    "\n",
    "    Notice it is similar to backprop of linear layer except it sums over the scanned area and removes padding.\n",
    "``` -->\n",
    "\n",
    "## Depthwise Separable Convolution\n",
    "- **What**: Depthwise convolution + Pointwise convolution. ([paper](https://arxiv.org/pdf/1610.02357))\n",
    "- **Why**: To significantly reduce computational cost and #params.\n",
    "- **How**:\n",
    "    - **Depthwise**: Use a single filter independently per channel.\n",
    "    - **Pointwise**: Use Conv1d to combine the outputs of depthwise convolution.\n",
    "- **When**: When computational efficiency and model size are crucial.\n",
    "- **Where**: [MobileNets](https://arxiv.org/pdf/1704.04861), [Xception](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf), etc.\n",
    "- **Pros**: Significantly higher computational efficiency (time & space).\n",
    "- **Cons**: Lower accuracy.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "**Notations**:\n",
    "    - IO:\n",
    "        - $\\mathbf{X} \\in \\mathbb{R}^{H_{in} \\times W_{in} \\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y} \\in \\mathbb{R}^{H_{out} \\times W_{out} \\times C_{out}}$: Output volume.\n",
    "    - Params:\n",
    "        - $\\mathbf{W^d} \\in \\mathbb{R}^{f_h \\times f_w \\times C_{in}}$: Depthwise filters.\n",
    "        - $\\mathbf{b^d} \\in \\mathbb{R}^{C_{in}}$: Depthwise biases.\n",
    "        - $\\mathbf{W^p} \\in \\mathbb{R}^{1 \\times 1 \\times C_{in} \\times C_{out}}$: Pointwise filters.\n",
    "        - $\\mathbf{b^p} \\in \\mathbb{R}^{C_{out}}$: Pointwise biases.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $C_{out}$: #Output channels.\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "        - $p$: Padding size.\n",
    "**Forward**:\n",
    "    1. Depthwise convolution: Calculate $\\mathbf{Z} \\in \\mathbb{R}^{H_{out} \\times W_{out} \\times C_{in}}$:\n",
    "\n",
    "        $$\n",
    "        Z_{h,w,c_{in}} = \\sum_{i=1}^{f_h} \\sum_{j=1}^{f_w} W^d_{i,j,c_{in}} \\cdot X_{sh+i-p, sw+j-p, c_{in}} + b^d_{c_{in}}\n",
    "        $$\n",
    "\n",
    "    2. Pointwise convolution:\n",
    "\n",
    "        $$\n",
    "        Y_{h,w,c_{out}} = \\sum_{c_{in}=1}^{C_{in}} W^p_{1,1,c_{in},c_{out}} \\cdot Z_{h,w,c_{in}} + b^p_{c_{out}}\n",
    "        $$\n",
    "        where\n",
    "        $$\\begin{align*}\n",
    "        H_{out} &= \\left\\lfloor \\frac{H_{in} + 2p - f_h}{s} \\right\\rfloor + 1 \\\\\n",
    "        W_{out} &= \\left\\lfloor \\frac{W_{in} + 2p - f_w}{s} \\right\\rfloor + 1\n",
    "        \\end{align*}$$\n",
    "\n",
    "**Backward**:\n",
    "    1. Pointwise convolution: Let $g^{p}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{out}}$ be $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Y}}$.\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial W^p_{1,1,c_{in},c_{out}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}} \\cdot Z_{h,w,c_{in}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial b^p_{c_{out}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial Z_{h,w,c_{in}}} = \\sum_{c_{out}=1}^{C_{out}} g^{p}_{h,w,c_{out}} \\cdot W^p_{1,1,c_{in},c_{out}}\n",
    "        \\end{align*}$$\n",
    "    2. Depthwise convolution: Let $g^{d}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{in}}$ be $\\frac{\\partial\\mathcal{L}}{\\partial\\mathbf{Z}}$.\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial W^d_{i,j,c_{in}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}} \\cdot X_{sh+i-p, sw+j-p, c_{in}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial b_{d,c_{in}}} = \\sum_{h=1}^{H_{out}} \\sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}}\\\\\n",
    "        &\\frac{\\partial \\mathcal{L}}{\\partial X_{i,j,c_{in}}} = \\sum_{h=1}^{f_h} \\sum_{w=1}^{f_w} g^d_{h,w,c_{in}} \\cdot W^d_{i-sh+p,j-sw+p,c_{in}}\n",
    "        \\end{align*}$$\n",
    "``` -->\n",
    "\n",
    "## Atrous/Dilated Convolution\n",
    "- **What**: Add holes between filter elements (i.e., dilation). ([paper](https://arxiv.org/pdf/1511.07122))\n",
    "- **Why**: The filters can capture larger contextual info without increasing #params.\n",
    "- **How**: Introduce a dilation rate $r$ to determine the space between the filter elements. Then compute convolution accordingly.\n",
    "- **When**: When understanding the broader context is important.\n",
    "- **Where**: Semantic image segmentation, object detection, depth estimation, optical flow estimation, etc.\n",
    "- **Pros**:\n",
    "    - Larger receptive fields without increasing #params.\n",
    "    - Captures multi-scale info without upsampling layers.\n",
    "- **Cons**:\n",
    "    - Requires very careful hyperparam tuning, or info loss.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "**Notations**:\n",
    "    - IO:\n",
    "        - $\\mathbf{X}\\in\\mathbb{R}^{H_{in}\\times W_{in}\\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{out}}$: Output volume.\n",
    "    - Params:\n",
    "        - $\\mathbf{W}\\in\\mathbb{R}^{F_{H}\\times F_{W}\\times C_{out}\\times C_{in}}$: Filters.\n",
    "        - $\\mathbf{b}\\in\\mathbb{R}^{C_{out}}$: Biases.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $C_{out}$: #Filters (i.e., #Output channels).\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "        - $p$: Padding size.\n",
    "        - $r$: Dilation rate.\n",
    "**Forward**:\n",
    "    1. (optional) Pad input tensor: $\\mathbf{X}^\\text{pad}\\in\\mathbb{R}^{(H_{in}+2p)\\times (W_{in}+2p)\\times C_{in}}$\n",
    "    2. Perform element-wise multiplication (i.e., convolution):\n",
    "\n",
    "        $$\n",
    "        Y_{h,w,c_{out}}=\\sum_{c_{in}=1}^{C_{in}}\\sum_{i=1}^{f_h}\\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\\cdot X_{sh+r(i-1)-p,sw+r(j-1)-p,c_{in}}+b_{c_{out}}\n",
    "        $$\n",
    "\n",
    "        where\n",
    "\n",
    "        $$\\begin{align*}\n",
    "        H_{out}&=\\left\\lfloor\\frac{H_{in}+2p-r(f_h-1)-1}{s}\\right\\rfloor+1\\\\\n",
    "        W_{out}&=\\left\\lfloor\\frac{W_{in}+2p-r(f_w-1)-1}{s}\\right\\rfloor+1\n",
    "        \\end{align*}$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial W_{i,j,c_{out},c_{in}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\cdot X_{sh+r(i-1)-p, sw+r(j-1)-p, c_{in}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial b_{c_{out}}}=\\sum_{h=1}^{H_{out}}\\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\\\\n",
    "    &\\frac{\\partial\\mathcal{L}}{\\partial X_{i,j,c_{in}}}=\\sum_{c_{out}=1}^{C_{out}}\\sum_{h=1}^{f_h}\\sum_{w=1}^{f_w}g_{h,w,c_{out}}\\cdot W_{r(i-1)-sh+p,r(j-1)-sw+p,c_{out},c_{in}}\n",
    "    \\end{align*}$$\n",
    "``` -->\n",
    "\n",
    "## Pooling\n",
    "- **What**: Convolution but ([paper](https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf))\n",
    "    - computes a heuristic per scanned patch.\n",
    "    - uses the same #channels.\n",
    "- **Why**: Dimensionality reduction while preserving dominant features.\n",
    "- **How**: Slide the pooling window over the input & apply the heuristic within the scanned patch.\n",
    "    - **Max**: Output the maximum value from each patch.\n",
    "    - **Average**: Output the average value of each patch.\n",
    "- **When**: When downsampling is necessary.\n",
    "- **Where**: After convolutional layer.\n",
    "- **Pros**:\n",
    "    - Significantly higher computational efficiency (time & space).\n",
    "    - No params to train.\n",
    "    - Reduces overfitting.\n",
    "    - Preserves translation invariance without losing too much info.\n",
    "    - High robustness.\n",
    "- **Cons**:\n",
    "    - Slight spatial info loss.\n",
    "    - Requires hyperparam tuning.\n",
    "        - Large filter or stride results in coarse features.\n",
    "- **Max vs Average**:\n",
    "    - **Max**: Captures most dominant features; higher robustness.\n",
    "    - **Avg**: Preserves more info; provides smoother features; dilutes the importance of dominant features.\n",
    "\n",
    "<!-- ```{admonition} Math\n",
    ":class: note, dropdown\n",
    "**Notations**:\n",
    "    - IO:\n",
    "        - $\\mathbf{X}\\in\\mathbb{R}^{H_{in}\\times W_{in}\\times C_{in}}$: Input volume.\n",
    "        - $\\mathbf{Y}\\in\\mathbb{R}^{H_{out}\\times W_{out}\\times C_{in}}$: Output volume.\n",
    "    - Hyperparams:\n",
    "        - $H_{in}, W_{in}$: Input height & width.\n",
    "        - $C_{in}$: #Input channels.\n",
    "        - $f_h, f_w$: Filter height & width.\n",
    "        - $s$: Stride size.\n",
    "**Forward**:\n",
    "\n",
    "    $$\\begin{array}{ll}\n",
    "    \\text{Max:} & Y_{h,w,c}=\\max_{i=1,\\cdots,f_h\\ |\\ j=1,\\cdots,f_w}X_{sh+i,sw+j,c}\\\\\n",
    "    \\text{Avg:} & Y_{h,w,c}=\\frac{1}{f_hf_w}\\sum_{i=1}^{f_h}\\sum_{j=1}^{f_w}X_{sh+i,sw+j,c}\n",
    "    \\end{array}$$\n",
    "\n",
    "    where\n",
    "\n",
    "    $$\\begin{align*}\n",
    "    H_{out}&=\\left\\lfloor\\frac{H_{in}-f_h}{s}\\right\\rfloor+1\\\\\n",
    "    W_{out}&=\\left\\lfloor\\frac{W_{in}-f_h}{s}\\right\\rfloor+1\n",
    "    \\end{align*}$$\n",
    "\n",
    "**Backward**:\n",
    "\n",
    "    $$\\begin{array}{ll}\n",
    "    \\text{Max:} & \\frac{\\partial\\mathcal{L}}{\\partial X_{sh+i,sw+j,c}}=g_{h,w,c}\\text{ if }X_{sh+i,sw+j,c}=Y_{h,w,c}\\\\\n",
    "    \\text{Avg:} & \\frac{\\partial\\mathcal{L}}{\\partial X_{sh+i,sw+j,c}}=\\frac{g_{h,w,c}}{f_hf_w}\n",
    "    \\end{array}$$\n",
    "\n",
    "    - Max: Gradients only propagate to the max element of each window.\n",
    "    - Avg: Gradients are equally distributed among all elements in each window.\n",
    "``` -->\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Recurrent\n",
    "```{image} ../images/RNN.png\n",
    ":width: 400\n",
    ":align: center\n",
    "```\n",
    "\n",
    "$$\n",
    "h_t=\\tanh(x_tW_{xh}^T+h_{t-1}W_{hh}^T)\n",
    "$$\n",
    "\n",
    "Idea: **recurrence** - maintain a hidden state that captures information about previous inputs in the sequence\n",
    "\n",
    "Notations:\n",
    "- $ x_t$: input at time $t$ of shape $(m,H_{in}) $\n",
    "- $ h_t$: hidden state at time $t$ of shape $(D,m,H_{out}) $\n",
    "- $ W_{xh}$: weight matrix of shape $(H_{out},H_{in})$ if initial layer, else $(H_{out},DH_{out}) $\n",
    "- $ W_{hh}$: weight matrix of shape $(H_{out},H_{out}) $\n",
    "- $ H_{in}$: input size, #features in $x_t $\n",
    "- $ H_{out}$: hidden size, #features in $h_t $\n",
    "- $ m $: batch size\n",
    "- $ D$: $=2$ if bi-directional else $1 $\n",
    "\n",
    "Cons:\n",
    "- Short-term memory: hard to carry info from earlier steps to later ones if long seq\n",
    "- Vanishing gradient: gradients in earlier parts become extremely small if long seq\n",
    "\n",
    "## GRU\n",
    "\n",
    "```{image} ../images/GRU.png\n",
    ":width: 400\n",
    ":align: center\n",
    "```\n",
    "\n",
    "$$\\begin{align*}\n",
    "&r_t=\\sigma(x_tW_{xr}^T+h_{t-1}W_{hr}^T) \\\\\\\\\n",
    "&z_t=\\sigma(x_tW_{xz}^T+h_{t-1}W_{hz}^T) \\\\\\\\\n",
    "&\\tilde{h}\\_t=\\tanh(x_tW_{xn}^T+r_t\\odot(h_{t-1}W_{hn}^T)) \\\\\\\\\n",
    "&h_t=(1-z_t)\\odot\\tilde{h}\\_t+z_t\\odot h_{t-1}\n",
    "\\end{align*}$$\n",
    "\n",
    "Idea: Gated Recurrent Unit - use 2 gates to address long-term info propagation issue in RNN:\n",
    "1. **Reset gate**: determine how much of $ h_{t-1}$ should be ignored when computing $\\tilde{h}\\_t $.\n",
    "2. **Update gate**: determine how much of $ h_{t-1}$ should be retained for $h_t $.\n",
    "3. **Candidate**: calculate candidate $ \\tilde{h}\\_t$ with reset $h_{t-1} $.\n",
    "4. **Final**: calculate weighted average between candidate $ \\tilde{h}\\_t$ and prev state $h_{t-1} $ with the retain ratio.\n",
    "\n",
    "Notations:\n",
    "- $ r_t$: reset gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ z_t$: update gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ \\tilde{h}\\_t$: candidate hidden state at time $t$ of shape $(m,H_{out}) $\n",
    "- $ \\odot $: element-wise product\n",
    "\n",
    "## LSTM\n",
    "\n",
    "```{image} ../images/LSTM.png\n",
    ":width: 400\n",
    ":align: center\n",
    "```\n",
    "\n",
    "$$\\begin{align*}\n",
    "&i_t=\\sigma(x_tW_{xi}^T+h_{t-1}W_{hi}^T) \\\\\\\\\n",
    "&f_t=\\sigma(x_tW_{xf}^T+h_{t-1}W_{hf}^T) \\\\\\\\\n",
    "&\\tilde{c}\\_t=\\tanh(x_tW_{xc}^T+h_{t-1}W_{hc}^T) \\\\\\\\\n",
    "&c_t=f_t\\odot c_{t-1}+i_t\\odot \\tilde{c}\\_t \\\\\\\\\n",
    "&o_t=\\sigma(x_tW_{xo}^T+h_{t-1}W_{ho}^T) \\\\\\\\\n",
    "&h_t=o_t\\odot\\tanh(c_t)\n",
    "\\end{align*}$$\n",
    "\n",
    "Idea: Long Short-Term Memory - use 3 gates:\n",
    "1. **Input gate**: determine what new info from $ x_t$ should be added to cell state $c_t $.\n",
    "2. **Forget gate**: determine what info from prev cell $ c_{t-1} $ should be forgotten.\n",
    "3. **Candidate cell**: create a new candidate cell from $ x_t$ and $h_{t-1} $.\n",
    "4. **Update cell**: use $ i_t$ and $f_t $ to combine prev and new candidate cells.\n",
    "5. **Output gate**: determine what info from curr cell $ c_t$ should be added to output $h_t $.\n",
    "6. **Final**: simply apply $ o_t$ to activated cell $c_t $.\n",
    "\n",
    "Notations:\n",
    "- $ i_t$: input gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ f_t$: forget gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ c_t$: cell state at time $t$ of shape $(m,H_{cell}) $\n",
    "- $ o_t$: output gate at time $t$ of shape $(m,H_{out}) $\n",
    "- $ H_{cell}$: cell hidden size (in most cases same as $H_{out} $)\n",
    "\n",
    "## Bidirectional\n",
    "## Stacked\n",
    "\n",
    "<br/>\n",
    "\n",
    "# Activation\n",
    "An activation function adds nonlinearity to the output of a layer to enhance complexity. [ReLU](#relu) and [Softmax](#softmax) are SOTA.\n",
    "\n",
    "Notations:\n",
    "- $ z $: input (element-wise)\n",
    "\n",
    "## Binary-like\n",
    "\n",
    "### Sigmoid\n",
    "\n",
    "$$\n",
    "\\sigma(z)=\\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "-  $ \\sigma(z)\\in(0,1)$ and $\\sigma(0)=0.5 $.\n",
    "\n",
    "Pros:\n",
    "-  imitation of the firing rate of a neuron, 0 if too negative and 1 if too positive.\n",
    "-  smooth gradient.\n",
    "\n",
    "Cons:\n",
    "-  vanishing gradient: gradients rapidly shrink to 0 along backprop as long as any input is too positive or too negative.\n",
    "-  non-zero centric bias $ \\rightarrow $ non-zero mean activations.\n",
    "-  computationally expensive.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "$$\n",
    "\\tanh(z)=\\frac{e^z-e^{-z}}{e^z+e^{-z}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "- $ \\tanh(z)\\in(-1,1)$ and $\\tanh(0)=0 $.\n",
    "\n",
    "Pros:\n",
    "- zero-centered\n",
    "- imitation of the firing rate of a neuron, -1 if too negative and 1 if too positive.\n",
    "- smooth gradient.\n",
    "\n",
    "Cons:\n",
    "- vanishing gradient.\n",
    "- computationally expensive.\n",
    "\n",
    "## Linear Units (Rectified)\n",
    "\n",
    "### ReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{ReLU}(z)=\\max{(0,z)}\n",
    "$$\n",
    "\n",
    "Name: Rectified Linear Unit\n",
    "\n",
    "Idea:\n",
    "- convert negative linear outputs to 0.\n",
    "\n",
    "Pros:\n",
    "- no vanishing gradient\n",
    "- activate fewer neurons\n",
    "- much less computationally expensive compared to sigmoid and tanh.\n",
    "\n",
    "Cons:\n",
    "- dying ReLU: if most inputs are negative, then most neurons output 0 $ \\rightarrow$ no gradient for such neurons $\\rightarrow$ no param update $\\rightarrow $ they die. (NOTE: A SOLVABLE DISADVANTAGE)\n",
    "\n",
    "    - Cause 1: high learning rate $ \\rightarrow$ too much subtraction in param update $\\rightarrow$ weight too negative $\\rightarrow $ input for neuron too negative.\n",
    "    - Cause 2: bias too negative $ \\rightarrow $ input for neuron too negative.\n",
    "\n",
    "-  activation explosion as $ z\\rightarrow\\infty $. (NOTE: NOT A SEVERE DISADVANTAGE SO FAR)\n",
    "\n",
    "\n",
    "### LReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{LReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Leaky Rectified Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha\\in(0,1) $: hyperparam (negative slope), default 0.01.\n",
    "\n",
    "Idea:\n",
    "- scale negative linear outputs by $ \\alpha $.\n",
    "\n",
    "Pros:\n",
    "- no dying ReLU.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "### PReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{PReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "Name: Parametric Rectified Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha\\in(0,1) $: learnable param (negative slope), default 0.25.\n",
    "\n",
    "Idea:\n",
    "- scale negative linear outputs by a learnable $ \\alpha $.\n",
    "\n",
    "Pros:\n",
    "- a variable, adaptive param learned from data.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than LReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### RReLU\n",
    "\n",
    "$$\n",
    "\\mathrm{RReLU}(z)=\\max{(\\alpha z,z)}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Randomized Rectified Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha\\sim\\mathrm{Uniform}(l,u) $: a random number sampled from a uniform distribution.\n",
    "- $ l,u $: hyperparams (lower bound, upper bound)\n",
    "\n",
    "Idea:\n",
    "- scale negative linear outputs by a random $ \\alpha $.\n",
    "\n",
    "Pros:\n",
    "- reduce overfitting by randomization.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than LReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "## Linear Units (Exponential)\n",
    "\n",
    "### ELU\n",
    "\n",
    "$$\n",
    "\\mathrm{ELU}(z)=\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0 \\\\\\\\\n",
    "\\alpha(e^z-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.\n",
    "\n",
    "Idea:\n",
    "- convert negative linear outputs to the non-linear exponential function above.\n",
    "\n",
    "Pros:\n",
    "- mean unit activation is closer to 0 $ \\rightarrow $ reduce bias shift (i.e., non-zero mean activation is intrinsically a bias for the next layer.)\n",
    "- lower computational complexity compared to batch normalization.\n",
    "- smooth to $ -\\alpha $ slowly with smaller derivatives that decrease forwardprop variation.\n",
    "- faster learning and higher accuracy for image classification in practice.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### SELU\n",
    "\n",
    "$$\n",
    "\\mathrm{SELU}(z)=\\lambda\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0 \\\\\n",
    "\\alpha(e^z-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Scaled Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.67326.\n",
    "- $ \\lambda $: hyperparam (scale), default 1.05070.\n",
    "\n",
    "Idea:\n",
    "- scale ELU.\n",
    "\n",
    "Pros:\n",
    "- self-normalization $ \\rightarrow $ activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance.\n",
    "\n",
    "Cons:\n",
    "- more computationally expensive than ReLU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "\n",
    "\n",
    "### CELU\n",
    "\n",
    "$$\n",
    "\\mathrm{CELU}(z)=\\begin{cases}\n",
    "z & \\mathrm{if}\\ z\\geq0\\\\\n",
    "\\alpha(e^{\\frac{z}{\\alpha}}-1) & \\mathrm{if}\\ z<0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Continuously Differentiable Exponential Linear Unit\n",
    "\n",
    "Params:\n",
    "- $ \\alpha $: hyperparam, default 1.\n",
    "\n",
    "Idea:\n",
    "- scale the exponential part of ELU with $ \\frac{1}{\\alpha} $ to make it continuously differentiable.\n",
    "\n",
    "Pros:\n",
    "- smooth gradient due to continuous differentiability (i.e., $ \\mathrm{CELU}'(0)=1 $).\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ELU.\n",
    "- activation explosion as $ z\\rightarrow\\infty $.\n",
    "\n",
    "## Linear Units (Others)\n",
    "\n",
    "### GELU\n",
    "\n",
    "$$\n",
    "\\mathrm{GELU}(z)=z*\\Phi(z)=0.5z(1+\\tanh{[\\sqrt{\\frac{2}{\\pi}}(z+0.044715z^3)]})\n",
    "$$\n",
    "\n",
    "\n",
    "Name: Gaussian Error Linear Unit\n",
    "\n",
    "Idea:\n",
    "- weigh each output value by its Gaussian cdf.\n",
    "\n",
    "Pros:\n",
    "- throw away gate structure and add probabilistic-ish feature to neuron outputs.\n",
    "- seemingly better performance than the ReLU and ELU families, SOTA in transformers.\n",
    "\n",
    "Cons:\n",
    "- slightly more computationally expensive than ReLU.\n",
    "- lack of practical testing at the moment.\n",
    "\n",
    "\n",
    "\n",
    "### SiLU\n",
    "\n",
    "$$\n",
    "\\mathrm{SiLU}(z)=z*\\sigma(z)\n",
    "$$\n",
    "\n",
    "Name: Sigmoid Linear Unit\n",
    "\n",
    "Idea:\n",
    "- weigh each output value by its sigmoid value.\n",
    "\n",
    "Pros:\n",
    "- throw away gate structure.\n",
    "- seemingly better performance than the ReLU and ELU families.\n",
    "\n",
    "Cons:\n",
    "- worse than GELU.\n",
    "\n",
    "\n",
    "\n",
    "### Softplus\n",
    "\n",
    "$$\n",
    "\\mathrm{softplus}(z)=\\frac{1}{\\beta}\\log{(1+e^{\\beta z})}\n",
    "$$\n",
    "\n",
    "\n",
    "Idea:\n",
    "- smooth approximation of ReLU.\n",
    "\n",
    "Pros:\n",
    "- differentiable and thus theoretically better than ReLU.\n",
    "\n",
    "Cons:\n",
    "- empirically far worse than ReLU in terms of computation and performance.\n",
    "\n",
    "\n",
    "\n",
    "## Multiclass\n",
    "\n",
    "### Softmax\n",
    "\n",
    "$$\n",
    "\\mathrm{softmax}(z_i)=\\frac{\\exp{(z_i)}}{\\sum_j{\\exp{(z_j)}}}\n",
    "$$\n",
    "\n",
    "\n",
    "Idea:\n",
    "- convert each value $ z_i$ in the output tensor $\\mathbf{z}$ into its corresponding exponential probability s.t. $\\sum_i{\\mathrm{softmax}(z_i)}=1 $.\n",
    "\n",
    "Pros:\n",
    "- your single best choice for multiclass classification.\n",
    "\n",
    "Cons:\n",
    "- mutually exclusive classes (i.e., one input can only be classified into one class.)\n",
    "\n",
    "### Softmin\n",
    "\n",
    "$$\n",
    "\\mathrm{softmin}(z_i)=\\mathrm{softmax}(-z_i)=\\frac{\\exp{(-z_i)}}{\\sum_j{\\exp{(-z_j)}}}\n",
    "$$\n",
    "\n",
    "Idea:\n",
    "- reverse softmax.\n",
    "\n",
    "Pros:\n",
    "- suitable for multiclass classification.\n",
    "\n",
    "Cons:\n",
    "- why not softmax."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  },
  "source_map": [
   11
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}