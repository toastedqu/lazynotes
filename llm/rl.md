---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# RL for LLMs
## Overview
### RL
- **What**: Agent $\overset{\text{action}}{\underset{\text{reward}}{\rightleftarrows}}$ Environment
    
- **Why**: For decision-making where actions have delayed consequence in dynamic, sequential tasks.
    - In contrast, Supervised Learning teaches "correct answers" for static tasks.
- **How**:
    - **Objective**: Cumulative Reward $\xleftarrow{\text{maximize}}$ Optimal Policy
    - **Process**: Repeat: $s_t$ $\xrightarrow{a_t}$ $s_{t+1}$ $\xrightarrow{\text{get}}$ $r_t$ $\xrightarrow{\text{update}}$ $\pi$

### LLM Alignment
- **What**: LLM $\xleftarrow{\text{match}}$ human values
- **Why**: To reduce the odds of generating undesired, sometimes harmful responses despite pretraining & SFT.
- **How**: Humans $\xrightarrow{\text{collect}}$ Feedback $\xrightarrow{\text{train}}$ Pretrained LLM

### RL for LLM Alignment
- **What**: Frame LLM Alignment as an RL problem:
    - **Agent**: LLM.
    - **State**: Input token sequence.
    - **Action**: Next-token prediction.
    - **Next State**: Input token sequence + Predicted next token.
    - **Reward**: Reward.
        - Determined by an external reward model OR preference labels.
        - Typically computed after a full token sequence is generated.
    - **Policy**: LLM weights.
        - Dictates how LLM predicts next token given input token sequence.
        - Initial policy $\leftarrow$ Pretraining (& SFT).
- **Why**: Human values are dynamic, subjective, and constantly evolving. There isn't always one "correct answer" for IRL scenarios, so SFT falls short.
- **How**:
    - **Process**: Feedback $\xrightarrow{\text{train}}$ RM $\xrightarrow{\text{train}}$ Policy
    - **Key factors**:
        - Feedback Data
        - Reward Model
        - Policy Optimization

```{dropdown} Table 1: Feedback Data
| Subcategory | Type | Description | Pros | Cons |
|------------|------|-------------|------|------|
| **Label** | **Preference** | Rating on a scale ($y_w>y_l$) | Captures nuance | Hard to collect |
|  | **Binary** | Thumbs up & down ($y^+\ \&\ y^-$) | Easy to collect | Less informative (no middle ground) |
| **Style** | **Pairwise** | Compare 2 responses | Easy to interpret | Slow for large datasets (have to create pairs for all responses) |
|  | **Listwise** | Rank multiple responses at once | More informative, Fast | Hard to interpret |
| **Source** | **Human** | Feedback from human evaluators | Represents actual human values | Expensive, slow, inconsistent due to subjectivity |
|  | **AI** | Feedback generated by AI models | Cheap, fast, scalable | Does not necessarily represent human values (risk of unsafe responses) |
```

```{dropdown} Table 2: Reward Model
| Subcategory | Type | Description | Pros | Cons |
|------------|------|-------------|------|------|
| **Form** | **Explicit** | An external model, typically from SFT of a pretrained LLM | Interpretable & Scalable | High computational cost |
|  | **Implicit** | No external model (e.g., DPO) | Low computational cost, No reward overfitting | Less control |
| **Style** | **Pointwise** | Outputs a reward score $r(x,y)$ given an input-output pair | Simple & Interpretable | Ignores relative preferences |
|  | **Preferencewise** | Outputs probability of desired response being preferred over undesired response:<br>$P(y_w>y_l\|x)=\sigma(r(x,y_w)-r(x,y_l))$ | Provides comparisons | No pairwise preferences, Sensitive to human label inconsistencies |
| **Level** | **Token-level** | Reward is given per token/action | Fine-grained feedback | High computational cost, Noisy rewards |
|  | **Response-level** | Reward is given per response (most commonly used) | Simple | Coarse feedback |
| **Source** | **Positive** | Humans label both desired and undesired responses | More control | Expensive & Time-consuming |
|  | **Negative** | Humans label undesired responses, LLMs generate desired responses | Cheap & Scalable | Less control |
```

```{admonition} Math
:class: note, dropdown
Example: PPO [2]: Reward Max + Deviation Min.

$$\begin{align*}
\pi_\theta^*(y|x)&=\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D}}[\mathbb{E}_{y\sim\pi_\theta(y|x)}r(x,y)-\beta D_\text{KL}(\pi_\theta(y|x)||\pi_\text{ref}(y|x))] \\
&=\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_\theta(y|x)}r(x,y)-\beta\mathbb{E}_{y\sim\pi_\theta(y|x)}\left[\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\right]\right] \\
&=\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_\theta(y|x)}\left[r(x,y)-\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\right]
\end{align*}$$

Notations:
- IO:
    - $x\sim\mathcal{D}$: Input token sequence, drawn from dataset $\mathcal{D}$.
    - $y\sim\pi_\theta(y|x)$: Output token sequence, drawn from current policy.
- Params:
    - $\pi_\theta(y|x)$: Current policy, which gives the probability of generating $y$ given $x$.
    - $\pi_\theta^*(y|x)$: Optimal policy, which balances reward maximization and deviation minimization.
- Hyperparams:
    - $\pi_\text{ref}(y|x)$: Reference policy, the initial policy of the pretrained model.
    - $r(x,y)$: Reward function for input-output pair $(x,y)$.
    - $\beta$: Regularization coefficient for the KL divergence penalty.
```

## Variations
### RLHF/PPO
- **What**: RLHF + PPO/PPO-ptx.
- **Why**: Traditional NLG evaluation metrics do NOT align with human preferences, so OpenAI researchers came up with a way to directly teach LLMs human preferences and evaluate on human metrics instead: Helpful, Honest, Harm.
- **How**:
    - **Data**: Pairwise + Human.
    - **RM**: Explicit + Pointwise.
    - **PO**:
        1. **PPO** (Proximal Policy Optimization): Max Reward + **Min Deviation**
            - Deviation Minimization: Minimize deviation of aligned policy from initial policy $\rightarrow$ Trust Region Constraint
                - *Why?* We want to keep what works while steering toward our goal via minimal adjustments. Drastic changes could make it forget the basics.
        2. **PPO-ptx**: Max Reward + Min Deviation + **Min Alignment Tax**
            - Alignment Tax Minimization: Minimize degradation of downstream task performance due to alignment.

<!-- ```{admonition} Math
:class: note, dropdown -->
RM:

$$
L_\text{RM}(r_\phi)=-\frac{1}{\binom{K}{2}}\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\sigma(r_\phi(x,y_w)-r_\phi(x,y_l))\right]
$$

PPO-ptx:

$$
\pi_\theta^*(y|x)=\max_{\pi_\theta}\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}\left[r_\phi(x,y)-\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\right]+\gamma\mathbb{E}_{x\sim\mathcal{D}_\text{pretrain}}[\log\pi_\theta(x)]
$$


Notations:
- IO:
    - $x\sim\mathcal{D}$: Input token sequence, drawn from dataset $\mathcal{D}$.
    - $y\sim\pi_\theta(y|x)$: Output token sequence, drawn from current policy.
- Params:
    - $\pi_\theta(y|x)$: Current policy, which gives the probability of generating $y$ given $x$.
    - $\pi_\theta^*(y|x)$: Optimal policy, which balances reward maximization and deviation minimization.
- Hyperparams:
    - $\pi_\text{ref}(y|x)$: Reference policy, the initial policy of the pretrained model.
    - $r(x,y)$: Reward function for input-output pair $(x,y)$.
    - $\beta$: Regularization coefficient for the KL divergence penalty.
<!-- ``` -->



References:
1. Wang, S., Zhang, S., Zhang, J., Hu, R., Li, X., Zhang, T., ... & Hovy, E. (2024). Reinforcement Learning Enhanced LLMs: A Survey. arXiv preprint arXiv:2412.10400.
2. Wang, Z., Bi, B., Pentyala, S. K., Ramnath, K., Chaudhuri, S., Mehrotra, S., ... & Asur, S. (2024). A comprehensive survey of LLM alignment techniques: RLHF, RLAIF, PPO, DPO and more. arXiv preprint arXiv:2407.16216.
3. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744.
