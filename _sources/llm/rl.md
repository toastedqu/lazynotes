---
jupytext:
  formats: md:myst
  text_representation:
    extension: .md
    format_name: myst
kernelspec:
  display_name: Python 3
  language: python
  name: python3
---
# RL for LLMs
## Overview
### RL
- **What**: Agent $\overset{\text{action}}{\underset{\text{reward}}{\rightleftarrows}}$ Environment
- **Why**: For decision-making where actions have delayed consequence in dynamic, sequential tasks.
    - In contrast, Supervised Learning teaches "correct answers" for static tasks.
- **How**:
    - **Objective**: Cumulative Reward $\xleftarrow{\text{maximize}}$ Optimal Policy
    - **Process**: Repeat: $s_t$ $\xrightarrow{a_t}$ $s_{t+1}$ $\xrightarrow{\text{get}}$ $r_t$ $\xrightarrow{\text{update}}$ $\pi$

### LLM Alignment
- **What**: LLM $\xleftarrow{\text{match}}$ human values
- **Why**: To reduce the odds of generating undesired, sometimes harmful responses despite pretraining & SFT.
- **How**: Humans $\xrightarrow{\text{collect}}$ Feedback $\xrightarrow{\text{train}}$ Pretrained LLM

### RL for LLM Alignment
- **What**: Frame LLM Alignment as an RL problem:
    - **Agent**: LLM.
    - **State**: Input token sequence.
    - **Action**: Next-token prediction.
    - **Next State**: Input token sequence + Predicted next token.
    - **Reward**: Reward.
        - Determined by an external reward model OR preference labels.
        - Typically computed after a full token sequence is generated.
    - **Policy**: LLM weights.
        - Dictates how LLM predicts next token given input token sequence.
        - Initial policy $\leftarrow$ Pretraining (& SFT).
- **Why**: Human values are dynamic, subjective, and constantly evolving. There isn't always one "correct answer" for IRL scenarios, so SFT falls short.
- **How**:
    - **Process**: Feedback $\xrightarrow{\text{train}}$ RM $\xrightarrow{\text{train}}$ Policy
    - **Key factors**:
        - Feedback Data
        - Reward Model
        - Policy Optimization

```{dropdown} Table 1: Feedback Data
| Subcategory | Type | Description | Pros | Cons |
|------------|------|-------------|------|------|
| **Label** | **Preference** | Rating on a scale ($y_w>y_l$) | Captures nuance | Hard to collect |
|  | **Binary** | Thumbs up & down ($y^+\ \&\ y^-$) | Easy to collect | Less informative (no middle ground) |
| **Style** | **Pairwise** | Compare 2 responses | Easy to interpret | Slow for large datasets (have to create pairs for all responses) |
|  | **Listwise** | Rank multiple responses at once | More informative, Fast | Hard to interpret |
| **Source** | **Human** | Feedback from human evaluators | Represents actual human values | Expensive, slow, inconsistent due to subjectivity |
|  | **AI** | Feedback generated by AI models | Cheap, fast, scalable | Does not necessarily represent human values (risk of unsafe responses) |
```

```{dropdown} Table 2: Reward Model
| Subcategory | Type | Description | Pros | Cons |
|------------|------|-------------|------|------|
| **Form** | **Explicit** | An external model, typically from SFT of a pretrained LLM | Interpretable & Scalable | High computational cost |
|  | **Implicit** | No external model (e.g., DPO) | Low computational cost, No reward overfitting | Less control |
| **Style** | **Pointwise** | Outputs a reward score $r(x,y)$ given an input-output pair | Simple & Interpretable | Ignores relative preferences |
|  | **Preferencewise** | Outputs probability of desired response being preferred over undesired response:<br>$P(y_w>y_l\|x)=\sigma(r(x,y_w)-r(x,y_l))$ | Provides comparisons | No pairwise preferences, Sensitive to human label inconsistencies |
| **Level** | **Token-level** | Reward is given per token/action | Fine-grained feedback | High computational cost, Noisy rewards |
|  | **Response-level** | Reward is given per response (most commonly used) | Simple | Coarse feedback |
| **Source** | **Positive** | Humans label both desired and undesired responses | More control | Expensive & Time-consuming |
|  | **Negative** | Humans label undesired responses, LLMs generate desired responses | Cheap & Scalable | Less control |
```

## Variations
### InstructGPT (RLHF/PPO)
- **What**: RLHF + PPO/PPO-ptx.
- **Why**: Traditional NLG evaluation metrics do NOT align with human preferences, so OpenAI researchers came up with a way to directly teach LLMs human preferences and evaluate on human metrics instead: Helpful, Honest, Harm.
- **How**:
    - **Data**: Pairwise + Human.
    - **RM**: Explicit + Pointwise.
    - **PO**: (tbf, ❌PPO ✅TRPO)
        1. **PPO**: Max Reward + **Min Deviation**
            - Deviation Minimization: Aligned policy $\Leftrightarrow$ Initial policy $\leftarrow$ Trust Region Constraint
                - *Why?* To keep what works while aiming at what we want, via minimal changes. Drastic changes could make it forget what worked.
        2. **PPO-ptx**: Max Reward + Min Deviation + **Min Alignment Tax**
            - Alignment Tax Minimization: ❌Degradation of Pre/SFT task performance.

```{admonition} Math
:class: note, dropdown
RM:

$$
L^\text{RM}(r_\phi)=-\frac{1}{\binom{K}{2}}\mathbb{E}_{(x,y_w,y_l)\sim\mathcal{D}}\left[\log\sigma\left(r_\phi(x,y_w)-r_\phi(x,y_l)\right)\right]
$$
- $r_\phi(x,y)$: Reward function for input $x$ and output $y$, parameterized by $\phi$.
- $\binom{K}{2}=\frac{K(K-1)}{2}$: #Comparisons for each prompt shown to each labeler.
- $x$: Input token sequence.
- $y_w$: Desired (W) output token sequence.
- $y_l$: Undesired (L) output token sequence.
- $\mathcal{D}$: RL Dataset.
- $\sigma(\cdot)$: Sigmoid function for BCE.

---

PO:
- PPO:

    $$\begin{align*}
    L^\text{PPO}(\pi_\theta)&=\mathbb{E}_{x\sim\mathcal{D}}[\mathbb{E}_{y\sim\pi_\theta(y|x)}r_\phi(x,y)-\beta\text{KL}\left[\pi_\theta(y|x)||\pi_\text{ref}(y|x)\right]] \\
    &=\mathbb{E}_{x\sim\mathcal{D}}\left[\mathbb{E}_{y\sim\pi_\theta(y|x)}r_\phi(x,y)-\beta\mathbb{E}_{y\sim\pi_\theta(y|x)}\left[\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\right]\right] \\
    &=\mathbb{E}_{x\sim\mathcal{D}, y\sim\pi_\theta(y|x)}\left[r_\phi(x,y)-\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\right]
    \end{align*}$$
    - $\pi_\theta(y|x)$: Curr policy, which gives the probability of generating $y$ given $x$.
    - $\pi_\text{ref}(y|x)$: Reference policy (initial policy of pretrained LLM).
    - $\beta$: KL divergence penalty coefficient.
    - $\text{KL}[\pi_\theta||\pi_\text{ref}]$: Per-token KL divergence.
- PPO-ptx:

    $$
    L^\text{PPO-ptx}(\pi_\theta)=\mathbb{E}_{x\sim\mathcal{D},y\sim\pi_\theta(y|x)}\left[r_\phi(x,y)-\beta\log\frac{\pi_\theta(y|x)}{\pi_\text{ref}(y|x)}\right]+\gamma\mathbb{E}_{x\sim\mathcal{D}_\text{pretrain}}[\log\pi_\theta(x)]
    $$
    - $\gamma$: Pretrain loss coefficient.
    - $\mathcal{D}_\text{pretrain}$: Pretraining dataset.
```

#### PPO
- **What**: Policy gradient, but **proximal** (close) to current policy.
- **Why**:
    1. Stable gradients.
    2. No optimal KL penalty coefficient for **TRPO** to work well.
- **How**: TRPO + **Better Penalty**
    1. **Clipped Surrogate Objective**: Trap the probability ratio in a range.
        - $\rightarrow$ Penalize moving the ratio away from 1.
        - $\rightarrow$ Penalize the incentive to deviate from current policy.
    2. **Adaptive KL coefficient**: Adapt the coefficient to match a target KL divergence value per update.
        - $\rightarrow$ Minimize impact of hyperparam tuning.
    - (Empirically, 1>2)

```{admonition} Math
:class: note, dropdown
TRPO (quick recap):
- **Probability Ratio**: How much more/less likely to take the given action under new policy vs old policy.

    $$
    \rho_t(\theta)=\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}
    $$
    - $\theta$: Policy parameter(s).
    - $t$: Curr time step.
    - $a_t$: Curr action.
    - $s_t$: Curr state.
    - $\pi_\theta$: New policy.
    - $\pi_{\theta_\text{old}}$: Old policy.
- **Advantage Estimate**: (roughly) How much better/worse of the given action compared to baseline.

    $$
    \hat{A}_t=\sum_{l=0}^{T-t-1}(\gamma\lambda)^l\left[r_{t+l}+\gamma V(s_{t+l+1})-V(s_{t+l})\right]
    $$
    - $T$: Total time steps.
    - $l$: Time step increment.
    - $\gamma$: Discount factor for future rewards.
    - $\lambda\in[0,1]$: Bias-variance tradeoff coefficient.
        - $\lambda=0$: One-step TD $\rightarrow$ Bias⬆️ Variance⬇️
        - $\lambda=1$: Full Monte Carlo return $\rightarrow$ Bias⬇️ Variance⬆️
    - $r_t$: Curr reward.
    - $V(s)$: Value function at state $s$.
- **KL Divergence**:

    $$
    \text{KL}[\pi_\theta(\cdot|s_t)||\pi_{\theta_\text{old}}(\cdot|s_t)]=\mathbb{E}_{a_t\in\mathcal{A}_t}\left[\log\frac{\pi_\theta(a_t|s_t)}{\pi_{\theta_\text{old}}(a_t|s_t)}\right]
    $$
    - $\mathcal{A}_t$: Curr action space.
- Objective:

    $$
    L^\text{TRPO}=\hat{\mathbb{E}}_t\left[\rho_t(\theta)\hat{A}_t-\beta\text{KL}[\pi_\theta(\cdot|s_t)||\pi_{\theta_\text{old}}(\cdot|s_t)]\right]
    $$

---
PPO:
- **Clipped Surrogate Objective**:
    - Clip Function:

        $$
        \text{clip}(x, a, b)=\begin{cases}
        a & \text{if } x\leq a \\
        x & \text{if } x\in(a,b) \\
        b & \text{if } x\geq b
        \end{cases}
        $$
    - Objective:

        $$
        L^\text{CLIP}(\theta)=\hat{\mathbb{E}}_t\left[\min\left(\rho_t(\theta)\hat{A}_t, \text{clip}(\rho_t(\theta), 1-\epsilon, 1+\epsilon)\right)\right]
        $$
        - $\epsilon$: Tiny value to control ratio change.
- **Adaptive KL Penalty Coefficient**:
    - For each policy update:
        1. Optimize TRPO objective.
        2. Compute **divergence**:

            $$
            d=\hat{\mathbb{E}_t}\left[\text{KL}[\pi_\theta(\cdot|s_t)||\pi_{\theta_\text{old}}(\cdot|s_t)]\right]
            $$
        3. Update $\beta$ via case switch:

            $$\begin{align*}
            &d<d_\text{tar} /1.5 &\Longrightarrow\ &\beta\leftarrow\beta/2 \\
            &d>d_\text{tar}\cdot 1.5 &\Longrightarrow\ &\beta\leftarrow\beta\cdot 2
            \end{align*}$$
            - $d_\text{tar}$: Target divergence.
```

References:
1. Wang, S., Zhang, S., Zhang, J., Hu, R., Li, X., Zhang, T., ... & Hovy, E. (2024). Reinforcement Learning Enhanced LLMs: A Survey. arXiv preprint arXiv:2412.10400.
2. Wang, Z., Bi, B., Pentyala, S. K., Ramnath, K., Chaudhuri, S., Mehrotra, S., ... & Asur, S. (2024). A comprehensive survey of LLM alignment techniques: RLHF, RLAIF, PPO, DPO and more. arXiv preprint arXiv:2407.16216.
3. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., ... & Lowe, R. (2022). Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35, 27730-27744.
