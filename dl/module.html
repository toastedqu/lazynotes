
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Module &#8212; LazyNotes</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=03e43079" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=eba8b062" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/tabs.css?v=4c969af8" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script src="../_static/tabs.js?v=3ee01567"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dl/module';</script>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Loss" href="loss.html" />
    <link rel="prev" title="LazyNotes" href="../intro.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo.png" class="logo__image only-light" alt="LazyNotes - Home"/>
    <script>document.write(`<img src="../_static/logo.png" class="logo__image only-dark" alt="LazyNotes - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Deep Learning</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Module</a></li>



<li class="toctree-l1"><a class="reference internal" href="loss.html">Loss</a></li>







</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">LLM</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../llm/transformer.html">Transformer</a></li>


<li class="toctree-l1"><a class="reference internal" href="../llm/decoding.html">Decoding</a></li>





<li class="toctree-l1"><a class="reference internal" href="../llm/peft.html">PEFT</a></li>
<li class="toctree-l1"><a class="reference internal" href="../llm/rlhf.html">RL for LLMs</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Math</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../math/info.html">Information Theory</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Misc</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
<li class="toctree-l1"><a class="reference internal" href="../about.html">About</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fdl/module.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/dl/module.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Module</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Module</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#basic">Basic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">Convolution</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#depthwise-convolution">Depthwise Convolution</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#activation">Activation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-like">Binary-like</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lrelu">LReLU</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="module">
<h1>Module<a class="headerlink" href="#module" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>What</strong>: A function mapping input tensor <span class="math notranslate nohighlight">\(X\)</span> to output tensor <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
<li><p><strong>Why</strong>:</p>
<ol class="arabic simple">
<li><p>ML = Function Approximation.</p></li>
<li><p>DL = Function Approximation with a deep NN.</p></li>
<li><p>Deep NN = A bunch of modules stacked together.</p></li>
</ol>
</li>
</ul>
<p>Let <span class="math notranslate nohighlight">\(g\)</span> denote the gradient <span class="math notranslate nohighlight">\(\frac{\partial\mathcal{L}}{\partial y}\)</span> for readability.</p>
<br/>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="basic">
<h1>Basic<a class="headerlink" href="#basic" title="Link to this heading">#</a></h1>
<section id="linear">
<h2>Linear<a class="headerlink" href="#linear" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Linear transform.</p></li>
<li><p><strong>Why</strong>: Simplest way to transform data &amp; learn patterns.</p></li>
<li><p><strong>How</strong>: Sum of weighted input features (+ bias).</p></li>
</ul>
<!-- #, toggle-hidden -->
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--0-input--1" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--1">Vector</label><div class="tab-content docutils">
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{H_{in}}\)</span>: Input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{H_{out}}\)</span>: Output vector.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W\in\mathbb{R}^{H_{out}\times H_{in}}\)</span>: Weight matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{b}\in\mathbb{R}^{H_{out}}\)</span>: Bias vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_{in}\)</span>: Input feature dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_{out}\)</span>: Output feature dimension.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
\textbf{y}=W\textbf{x}+\textbf{b}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W}=\textbf{g}\textbf{x}^T \\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{b}}=\textbf{g}\\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{x}}=W^T\textbf{g}
\end{align*}\end{split}\]</div>
</div>
<input class="tab-input" id="tab-set--0-input--2" name="tab-set--0" type="radio"><label class="tab-label" for="tab-set--0-input--2">Tensor</label><div class="tab-content docutils">
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{*\times H_{in}}\)</span>: Input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{*\times H_{out}}\)</span>: Output tensor.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(W\in\mathbb{R}^{H_{out}\times H_{in}}\)</span>: Weight matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\textbf{b}\in\mathbb{R}^{H_{out}}\)</span>: Bias vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_{in}\)</span>: Input feature dimension.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_{out}\)</span>: Output feature dimension.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
\textbf{Y}=\textbf{X}W^T+\textbf{b}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W}=\textbf{g}^T\textbf{X} \\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{b}}=\sum_*\textbf{g}_*\\
&amp;\frac{\partial\mathcal{L}}{\partial\textbf{x}}=\textbf{g}W
\end{align*}\end{split}\]</div>
</div>
</div>
</div>
</section>
<section id="dropout">
<h2>Dropout<a class="headerlink" href="#dropout" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Randomly ignore some neurons during training.</p></li>
<li><p><strong>Why</strong>: To reduce overfitting.</p></li>
<li><p><strong>How</strong>: During training:</p>
<ol class="arabic simple">
<li><p>Randomly set a fraction of neurons to 0.</p></li>
<li><p>Scale the outputs/gradients on active neurons by the keep probability.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--1-input--1" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--1">Vector</label><div class="tab-content docutils">
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{H_{in}}\)</span>: Input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{H_{in}}\)</span>: Output vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: Keep probability.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textbf{m}\in\mathbb{R}^{H_{in}}\)</span>: binary mask, where each element <span class="math notranslate nohighlight">\(m\sim\text{Bernoulli}(p)\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
\textbf{y}=\frac{\textbf{m}\odot\textbf{x}}{p}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{x}} = \frac{\textbf{m}\odot\textbf{g}}{p}
\]</div>
</div>
<input class="tab-input" id="tab-set--1-input--2" name="tab-set--1" type="radio"><label class="tab-label" for="tab-set--1-input--2">Tensor</label><div class="tab-content docutils">
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{*\times H_{in}}\)</span>: Input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{*\times H_{in}}\)</span>: Output tensor.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: Keep probability.</p></li>
</ul>
</li>
<li><p>Intermediate values:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\textbf{M}\in\mathbb{R}^{*\times H_{in}}\)</span>: binary mask, where each element <span class="math notranslate nohighlight">\(m\sim\text{Bernoulli}(p)\)</span>.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
\textbf{Y}=\frac{\textbf{M}\odot\textbf{X}}{p}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{X}} = \frac{\textbf{M}\odot\textbf{g}}{p}
\]</div>
</div>
</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>⬆️Training time <span class="math notranslate nohighlight">\(\leftarrow\)</span> ⬇️Convergence speed <span class="math notranslate nohighlight">\(\leftarrow\)</span> Sparsity</p></li>
<li><p>✅Hyperparameter Tuning.</p></li>
</ul>
</div>
</section>
<section id="residual-connection">
<h2>Residual Connection<a class="headerlink" href="#residual-connection" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Model the residual (<span class="math notranslate nohighlight">\(Y-X\)</span>) instead of the output (<span class="math notranslate nohighlight">\(Y\)</span>).</p></li>
<li><p><strong>Why</strong>: To mitigate <a class="reference internal" href="issues.html#vanishing/exploding-gradient"><span class="std std-ref">vanishing/exploding gradients</span></a>.</p></li>
<li><p><strong>How</strong>: Add input <span class="math notranslate nohighlight">\(X\)</span> to block output <span class="math notranslate nohighlight">\(F(X)\)</span>.</p>
<ul>
<li><p>If the feature dimension of <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(F(X)\)</span> doesn’t match, use a shortcut linear layer on <span class="math notranslate nohighlight">\(X\)</span> to change its feature dimension.</p></li>
</ul>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<div class="tab-set docutils">
<input checked="True" class="tab-input" id="tab-set--2-input--1" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--1">Vector</label><div class="tab-content docutils">
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{H_{in}}\)</span>: Input vector.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{H_{out}}\)</span>: Output vector.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(\cdot)\in\mathbb{R}^{H_{out}}\)</span>: The aggregate function of all layers within the residual block.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
\textbf{y}=F(\textbf{x})+\textbf{x}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{x}}=\mathbf{g}(1+\frac{\partial F(\textbf{x})}{\partial\textbf{x}})
\]</div>
</div>
<input class="tab-input" id="tab-set--2-input--2" name="tab-set--2" type="radio"><label class="tab-label" for="tab-set--2-input--2">Tensor</label><div class="tab-content docutils">
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{x}\in\mathbb{R}^{*\times H_{in}}\)</span>: Input tensor.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{y}\in\mathbb{R}^{*\times H_{out}}\)</span>: Output tensor.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(F(\cdot)\in\mathbb{R}^{H_{out}}\)</span>: The aggregate function of all layers within the residual block.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
\textbf{Y}=F(\textbf{X})+\textbf{X}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial\textbf{X}}=\mathbf{g}(1+\frac{\partial F(\textbf{X})}{\partial\textbf{X}})
\]</div>
</div>
</div>
</div>
</section>
<section id="normalization">
<h2>Normalization<a class="headerlink" href="#normalization" title="Link to this heading">#</a></h2>
<section id="batch-normalization">
<h3>Batch Normalization<a class="headerlink" href="#batch-normalization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Normalize each feature across input samples to zero mean &amp; unit variance.</p></li>
<li><p><strong>Why</strong>: To mitigate <a class="reference internal" href="issues.html#vanishing/internal-covariate-shift"><span class="std std-ref">internal covariate shift</span></a>.</p></li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Calculate the mean and variance for each batch.</p></li>
<li><p>Normalize the batch.</p></li>
<li><p>Scale and shift the normalized output using learnable params.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p><strong>Notation</strong>:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{m\times n}\)</span>: Input matrix.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{m\times n}\)</span>: Output matrix.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\gamma\in\mathbb{R}\)</span>: Scale param.</p></li>
<li><p><span class="math notranslate nohighlight">\(\beta\in\mathbb{R}\)</span>: Shift param.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<ol class="arabic">
<li><p>Calculate the mean and variance for each batch.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \boldsymbol{\mu}_B&amp;=\frac{1}{m}\sum_{i=1}^{m}\textbf{x}_i\\
    \boldsymbol{\sigma}_B^2&amp;=\frac{1}{m}\sum_{i=1}^{m}(\textbf{x}_i-\boldsymbol{\mu}_B)^2
    \end{align*}\end{split}\]</div>
</li>
<li><p>Normalize each batch.</p>
<div class="math notranslate nohighlight">
\[
    \textbf{z}_i=\frac{\textbf{x}_i-\boldsymbol{\mu}_B}{\sqrt{\boldsymbol{\sigma}_B^2+\epsilon}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant to avoid dividing by 0.</p>
</li>
<li><p>Scale and shift the normalized output.</p>
<div class="math notranslate nohighlight">
\[
    \textbf{y}_i=\gamma\textbf{z}_i+\beta
    \]</div>
</li>
</ol>
<p>Backward:</p>
<ol class="arabic">
<li><p>Gradient w.r.t. params:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial\gamma}=\sum_{i=1}^{m}\textbf{g}_i\textbf{z}_i\\
    &amp;\frac{\partial\mathcal{L}}{\partial\beta}=\sum_{i=1}^{m}\textbf{g}_i
    \end{align*}\end{split}\]</div>
</li>
<li><p>Gradient w.r.t. input:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}=\gamma\textbf{g}_i\\
    &amp;\frac{\partial\mathcal{L}}{\partial\boldsymbol{\sigma}_B^2}=\sum_{i=1}^{m}\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}(\textbf{x}_i-\boldsymbol{\mu}_B)\left(-\frac{1}{2}(\boldsymbol{\sigma}_B^2+\epsilon)^{-\frac{3}{2}}\right)\\
    &amp;\frac{\partial\mathcal{L}}{\partial\boldsymbol{\mu}_B}=\sum_{i=1}^{m}\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}\cdot\left(-\frac{1}{\sqrt{\boldsymbol{\sigma}_B^2+\epsilon}}\right)+\frac{\partial\mathcal{L}}{\partial\boldsymbol{\sigma}_B^2}\cdot\left(-\frac{2}{m}\sum_{i=1}^{m}(\textbf{x}_i-\boldsymbol{\mu}_B)\right)\\
    &amp;\frac{\partial\mathcal{L}}{\partial\textbf{x}_i}=\frac{1}{\sqrt{\boldsymbol{\sigma}_B^2+\epsilon}}\left(\frac{\partial\mathcal{L}}{\partial\textbf{z}_i}+\frac{2}{m}\frac{\partial\ma                                                                         thcal{L}}{\partial\boldsymbol{\sigma}_B^2}(\textbf{x}_i-\boldsymbol{\mu}_B)+\frac{1}{m}\frac{\partial\mathcal{L}}{\partial\boldsymbol{\mu}_B}\right)
    \end{align*}\end{split}\]</div>
</li>
</ol>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>Accelerates training with higher learning rates.</p></li>
<li><p>Reduces sensitivity to weight initialization.</p></li>
<li><p>Mitigates <a class="reference internal" href="issues.html#vanishing/exploding-gradient"><span class="std std-ref">vanishing/exploding gradients</span></a>.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Adds computation overhead and complexity.</p></li>
<li><p>Works best when each mini-batch is representative of the overall input distribution to accurately estimate the mean and variance.</p></li>
<li><p>Causes potential issues in certain cases like small mini-batches or when batch statistics differ from overall dataset statistics.</p></li>
</ul>
</div>
</section>
<section id="layer-normalization">
<h3>Layer Normalization<a class="headerlink" href="#layer-normalization" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Normalize each sample across input features to zero mean and unit variance.</p></li>
<li><p><strong>Why</strong>: Batch normalization depends on the batch size.</p>
<ul>
<li><p>When it’s too big, high computational cost.</p></li>
<li><p>When it’s too small, the batch may not be representative of the underlying data distribution.</p></li>
<li><p>Hyperparam tuning is required to find the optimal batch size, leading to high computational cost.</p></li>
</ul>
</li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Calculate the mean and variance for each feature.</p></li>
<li><p>Normalize the feature.</p></li>
<li><p>Scale and shift the normalized output using learnable params.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>It’s easy to explain with the vector form for batch normalization, but it’s more intuitive to explain with the scalar form for layer normalization.</p>
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(x_{ij}\in\mathbb{R}\)</span>: <span class="math notranslate nohighlight">\(j\)</span>th feature value for <span class="math notranslate nohighlight">\(i\)</span>th input sample.</p></li>
<li><p><span class="math notranslate nohighlight">\(y_{ij}\in\mathbb{R}\)</span>: <span class="math notranslate nohighlight">\(j\)</span>th feature value for <span class="math notranslate nohighlight">\(i\)</span>th output sample.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\gamma}\in\mathbb{R}^n\)</span>: Scale param.</p></li>
<li><p><span class="math notranslate nohighlight">\(\boldsymbol{\beta}\in\mathbb{R}^n\)</span>: Shift param.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<ol class="arabic">
<li><p>Calculate the mean and variance for each feature.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    \mu_i&amp;=\frac{1}{n}\sum_{j=1}^{n}x_{ij}\\
    \sigma_i^2&amp;=\frac{1}{n}\sum_{j=1}^{n}(x_{ij}-\mu_i)^2
    \end{align*}\end{split}\]</div>
</li>
<li><p>Normalize each feature.</p>
<div class="math notranslate nohighlight">
\[
    z_{ij}=\frac{x_{ij}-\mu_i}{\sqrt{\sigma_i^2+\epsilon}}
    \]</div>
<p>where <span class="math notranslate nohighlight">\(\epsilon\)</span> is a small constant to avoid dividing by 0.</p>
</li>
<li><p>Scale and shift the normalized output.</p>
<div class="math notranslate nohighlight">
\[
    y_{ij}=\gamma_jz_{ij}+\beta_j
    \]</div>
</li>
</ol>
<p>Backward:</p>
<ol class="arabic">
<li><p>Gradient w.r.t. params:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial\gamma_j}=\sum_{i=1}^{m}g_{ij}z_{ij}\\
    &amp;\frac{\partial\mathcal{L}}{\partial\beta_j}=\sum_{i=1}^{m}g_{ij}
    \end{align*}\end{split}\]</div>
</li>
<li><p>Gradient w.r.t. input:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
    &amp;\frac{\partial\mathcal{L}}{\partial z_{ij}}=\gamma_jg_{ij}\\
    &amp;\frac{\partial\mathcal{L}}{\partial\sigma_i^2}=\sum_{j=1}^{n}\frac{\partial\mathcal{L}}{\partial z_{ij}}(x_{ij}-\mu_i)\left(-\frac{1}{2}(\sigma_i^2+\epsilon)^{-\frac{3}{2}}\right)\\
    &amp;\frac{\partial\mathcal{L}}{\partial\mu_i}=\sum_{j=1}^{n}\frac{\partial\mathcal{L}}{\partial z_{ij}}\cdot\left(-\frac{1}{\sqrt{\sigma_i^2+\epsilon}}\right)+\frac{\partial\mathcal{L}}{\partial\sigma_i^2}\cdot\left(-\frac{2}{n}\sum_{j=1}^{n}(x_{ij}-\mu_i)\right)\\
    &amp;\frac{\partial\mathcal{L}}{x_{ij}}=\frac{1}{\sqrt{\sigma_i^2+\epsilon}}\left(\frac{\partial\mathcal{L}}{\partial z_{ij}}+\frac{2}{n}\frac{\partial\mathcal{L}}{\partial\sigma_i^2}(x_{ij}-\mu_i)+\frac{1}{n}\frac{\partial\mathcal{L}}{\partial\mu_i}\right)
    \end{align*}\end{split}\]</div>
</li>
</ol>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>Reduces hyperparam tuning effort.</p></li>
<li><p>High consistency during training and inference.</p></li>
<li><p>Mitigates <a class="reference internal" href="issues.html#vanishing/exploding-gradient"><span class="std std-ref">vanishing/exploding gradients</span></a>.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Adds computation overhead and complexity.</p></li>
<li><p>Inapplicable in CNNs due to varied statistics of spatial features.</p></li>
</ul>
</div>
<br/>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="convolution">
<h1>Convolution<a class="headerlink" href="#convolution" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>What</strong>: Slide a set of filters over input data to extract local features.</p></li>
<li><p><strong>Why</strong>: To learn spatial hierarchies of local features.</p></li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Initialize multiple small matrices (i.e., <strong>filters/kernels</strong>).</p></li>
<li><p>From top-left to Bottom-right:</p>
<ol class="arabic simple">
<li><p>Perform element-wise multiplication and summation between each filter &amp; scanned area of input data</p></li>
<li><p>Store the output in the corresponding position as a feature map.</p></li>
</ol>
</li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}\)</span>: Input volume.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{out}}\)</span>: Output volume.</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}\in\mathbb{R}^{F_{H}\times F_{W}\times C_{out}\times C_{in}}\)</span>: Filters.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}\in\mathbb{R}^{C_{out}}\)</span>: Biases.</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_{in}, W_{in}\)</span>: Input height &amp; width.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_{in}\)</span>: #Input channels.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_{out}\)</span>: #Filters (i.e., #Output channels).</p></li>
<li><p><span class="math notranslate nohighlight">\(f_h, f_w\)</span>: Filter height &amp; width.</p></li>
<li><p><span class="math notranslate nohighlight">\(s\)</span>: Stride size.</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: Padding size.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
Y_{h,w,c_{out}}=\sum_{c_{in}=1}^{C_{in}}\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\cdot X_{s(h-1)+i-p,s(w-1)+j-p,c_{in}}+b_{c_{out}}
\]</div>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
H_{out}&amp;=\left\lfloor\frac{H_{in}+2p-f_h}{s}\right\rfloor+1\\
W_{out}&amp;=\left\lfloor\frac{W_{in}+2p-f_w}{s}\right\rfloor+1
\end{align*}\end{split}\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W_{i,j,c_{out},c_{in}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\cdot X_{s(h-1)+i-p, s(w-1)+j-p, c_{in}}\\
&amp;\frac{\partial\mathcal{L}}{\partial b_{c_{out}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\
&amp;\frac{\partial\mathcal{L}}{\partial X_{i,j,c_{in}}}=\sum_{c_{out}=1}^{C_{out}}\sum_{h=1}^{f_h}\sum_{w=1}^{f_w}g_{h,w,c_{out}}\cdot W_{i-s(h-1)+p,j-s(w-1)+p,c_{out},c_{in}}
\end{align*}\end{split}\]</div>
<p>Notice it is similar to backprop of linear layer except it sums over the scanned area and removes padding.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>Translation invariance.</p></li>
<li><p>Efficiently captures spatial hierarchies.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>High computational cost.</p></li>
<li><p>Requires big data to be performant.</p></li>
<li><p>Requires extensive hyperparam tuning.</p></li>
</ul>
</div>
<section id="depthwise-convolution">
<h2>Depthwise Convolution<a class="headerlink" href="#depthwise-convolution" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Apply a single convolutional filter to each input channel independently.</p></li>
<li><p><strong>Why</strong>: To learn spatial features within each channel separately, significantly reducing computational cost and model parameters compared to standard convolution.</p></li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Initialize a set of filters, one for each input channel.</p></li>
<li><p>For each input channel, from top-left to bottom-right:</p>
<ol class="arabic simple">
<li><p>Perform element-wise multiplication &amp; summation between its dedicated filter &amp; the scanned area.</p></li>
<li><p>Store the output in the corresponding position in the respective output feature map.</p></li>
</ol>
</li>
<li><p>The resulting feature maps (one for each input channel) are typically stacked together.</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Notations:</p>
<ul class="simple">
<li><p>IO:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}\)</span>: Input volume.</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{in}}\)</span>: Output volume. (Note: <span class="math notranslate nohighlight">\(C_{out} = C_{in}\)</span> for a pure depthwise convolution layer)</p></li>
</ul>
</li>
<li><p>Params:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\mathbf{W}\in\mathbb{R}^{F_{H}\times F_{W}\times C_{in}}\)</span>: Filters (one <span class="math notranslate nohighlight">\(F_H \times F_W\)</span> filter per input channel).</p></li>
<li><p><span class="math notranslate nohighlight">\(\mathbf{b}\in\mathbb{R}^{C_{in}}\)</span>: Biases (one bias per input channel).</p></li>
</ul>
</li>
<li><p>Hyperparams:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(H_{in}, W_{in}\)</span>: Input height &amp; width.</p></li>
<li><p><span class="math notranslate nohighlight">\(C_{in}\)</span>: #Input channels (and also #Output channels).</p></li>
<li><p><span class="math notranslate nohighlight">\(f_h, f_w\)</span>: Filter height &amp; width.</p></li>
<li><p><span class="math notranslate nohighlight">\(s\)</span>: Stride size.</p></li>
<li><p><span class="math notranslate nohighlight">\(p\)</span>: Padding size.</p></li>
</ul>
</li>
</ul>
<p>Forward:</p>
<p>The output for each channel <span class="math notranslate nohighlight">\(c\)</span> is computed independently:
$<span class="math notranslate nohighlight">\(
Y_{h,w,c}=\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}W_{i,j,c}\cdot X_{s(h-1)+i-p,s(w-1)+j-p,c}+b_{c}
\)</span>$</p>
<p>where</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
H_{out}&amp;=\left\lfloor\frac{H_{in}+2p-f_h}{s}\right\rfloor+1\\
W_{out}&amp;=\left\lfloor\frac{W_{in}+2p-f_w}{s}\right\rfloor+1
\end{align*}\end{split}\]</div>
<p>Backward:</p>
<p>Let <span class="math notranslate nohighlight">\(g_{h,w,c} = \frac{\partial\mathcal{L}}{\partial Y_{h,w,c}}\)</span> be the gradient of the loss <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to the output <span class="math notranslate nohighlight">\(Y_{h,w,c}\)</span>.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{align*}
&amp;\frac{\partial\mathcal{L}}{\partial W_{i,j,c}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c}\cdot X_{s(h-1)+i-p, s(w-1)+j-p, c}\\
&amp;\frac{\partial\mathcal{L}}{\partial b_{c}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c}\\
&amp;\frac{\partial\mathcal{L}}{\partial X_{i',j',c}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}\sum_{k_h=1}^{f_h}\sum_{k_w=1}^{f_w} \left( g_{h,w,c} \cdot W_{k_h,k_w,c} \cdot \mathbb{I}(i' = s(h-1)+k_h-p \land j' = s(w-1)+k_w-p) \right)
\end{align*}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbb{I}(\cdot)\)</span> is the indicator function. More practically, the gradient with respect to the input <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> involves a “full” convolution of the gradients <span class="math notranslate nohighlight">\(g_c\)</span> (padded appropriately) with the corresponding flipped filter <span class="math notranslate nohighlight">\(W_c\)</span>.</p>
<p>Notice the similarity to the standard convolution’s backpropagation but applied independently for each channel.</p>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>Computational cost⬇️⬇️ <span class="math notranslate nohighlight">\(\leftarrow\)</span> #Params⬇️⬇️ &amp; #Multiplications⬇️⬇️</p></li>
<li><p>Learns per-channel spatial features.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>❌Cross-channel info.</p></li>
</ul>
</div>
<!-- ## Depthwise Separable Convolution
- **What**: Depthwise convolution + Pointwise convolution. ([paper](https://arxiv.org/pdf/1610.02357))
- **Why**: To significantly reduce computational cost and #params.
- **How**:
    - **Depthwise**: Use a single filter independently per channel.
    - **Pointwise**: Use Conv1d to combine the outputs of depthwise convolution.
- **When**: When computational efficiency and model size are crucial.
- **Where**: [MobileNets](https://arxiv.org/pdf/1704.04861), [Xception](https://openaccess.thecvf.com/content_cvpr_2017/papers/Chollet_Xception_Deep_Learning_CVPR_2017_paper.pdf), etc.
- **Pros**: Significantly higher computational efficiency (time & space).
- **Cons**: Lower accuracy.

```{admonition} Math
:class: note, dropdown
Notations:
    - IO:
        - $\mathbf{X} \in \mathbb{R}^{H_{in} \times W_{in} \times C_{in}}$: Input volume.
        - $\mathbf{Y} \in \mathbb{R}^{H_{out} \times W_{out} \times C_{out}}$: Output volume.
    - Params:
        - $\mathbf{W^d} \in \mathbb{R}^{f_h \times f_w \times C_{in}}$: Depthwise filters.
        - $\mathbf{b^d} \in \mathbb{R}^{C_{in}}$: Depthwise biases.
        - $\mathbf{W^p} \in \mathbb{R}^{1 \times 1 \times C_{in} \times C_{out}}$: Pointwise filters.
        - $\mathbf{b^p} \in \mathbb{R}^{C_{out}}$: Pointwise biases.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $C_{out}$: #Output channels.
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
        - $p$: Padding size.
Forward:
    1. Depthwise convolution: Calculate $\mathbf{Z} \in \mathbb{R}^{H_{out} \times W_{out} \times C_{in}}$:

        $$
        Z_{h,w,c_{in}} = \sum_{i=1}^{f_h} \sum_{j=1}^{f_w} W^d_{i,j,c_{in}} \cdot X_{sh+i-p, sw+j-p, c_{in}} + b^d_{c_{in}}
        $$

    2. Pointwise convolution:

        $$
        Y_{h,w,c_{out}} = \sum_{c_{in}=1}^{C_{in}} W^p_{1,1,c_{in},c_{out}} \cdot Z_{h,w,c_{in}} + b^p_{c_{out}}
        $$
        where
        $$\begin{align*}
        H_{out} &= \left\lfloor \frac{H_{in} + 2p - f_h}{s} \right\rfloor + 1 \\
        W_{out} &= \left\lfloor \frac{W_{in} + 2p - f_w}{s} \right\rfloor + 1
        \end{align*}$$

Backward:
    1. Pointwise convolution: Let $g^{p}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{out}}$ be $\frac{\partial\mathcal{L}}{\partial\mathbf{Y}}$.

        $$\begin{align*}
        &\frac{\partial \mathcal{L}}{\partial W^p_{1,1,c_{in},c_{out}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}} \cdot Z_{h,w,c_{in}}\\
        &\frac{\partial \mathcal{L}}{\partial b^p_{c_{out}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^{p}_{h,w,c_{out}}\\
        &\frac{\partial \mathcal{L}}{\partial Z_{h,w,c_{in}}} = \sum_{c_{out}=1}^{C_{out}} g^{p}_{h,w,c_{out}} \cdot W^p_{1,1,c_{in},c_{out}}
        \end{align*}$$
    2. Depthwise convolution: Let $g^{d}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{in}}$ be $\frac{\partial\mathcal{L}}{\partial\mathbf{Z}}$.

        $$\begin{align*}
        &\frac{\partial \mathcal{L}}{\partial W^d_{i,j,c_{in}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}} \cdot X_{sh+i-p, sw+j-p, c_{in}}\\
        &\frac{\partial \mathcal{L}}{\partial b_{d,c_{in}}} = \sum_{h=1}^{H_{out}} \sum_{w=1}^{W_{out}} g^d_{h,w,c_{in}}\\
        &\frac{\partial \mathcal{L}}{\partial X_{i,j,c_{in}}} = \sum_{h=1}^{f_h} \sum_{w=1}^{f_w} g^d_{h,w,c_{in}} \cdot W^d_{i-sh+p,j-sw+p,c_{in}}
        \end{align*}$$
``` -->
<!-- ## Atrous/Dilated Convolution
- **What**: Add holes between filter elements (i.e., dilation). ([paper](https://arxiv.org/pdf/1511.07122))
- **Why**: The filters can capture larger contextual info without increasing #params.
- **How**: Introduce a dilation rate $r$ to determine the space between the filter elements. Then compute convolution accordingly.
- **When**: When understanding the broader context is important.
- **Where**: Semantic image segmentation, object detection, depth estimation, optical flow estimation, etc.
- **Pros**:
    - Larger receptive fields without increasing #params.
    - Captures multi-scale info without upsampling layers.
- **Cons**:
    - Requires very careful hyperparam tuning, or info loss. -->
<!-- ```{admonition} Math
:class: note, dropdown
Notations:
    - IO:
        - $\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}$: Input volume.
        - $\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{out}}$: Output volume.
    - Params:
        - $\mathbf{W}\in\mathbb{R}^{F_{H}\times F_{W}\times C_{out}\times C_{in}}$: Filters.
        - $\mathbf{b}\in\mathbb{R}^{C_{out}}$: Biases.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $C_{out}$: #Filters (i.e., #Output channels).
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
        - $p$: Padding size.
        - $r$: Dilation rate.
Forward:
    1. (optional) Pad input tensor: $\mathbf{X}^\text{pad}\in\mathbb{R}^{(H_{in}+2p)\times (W_{in}+2p)\times C_{in}}$
    2. Perform element-wise multiplication (i.e., convolution):

        $$
        Y_{h,w,c_{out}}=\sum_{c_{in}=1}^{C_{in}}\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}W_{i,j,c_{out},c_{in}}\cdot X_{sh+r(i-1)-p,sw+r(j-1)-p,c_{in}}+b_{c_{out}}
        $$

        where

        $$\begin{align*}
        H_{out}&=\left\lfloor\frac{H_{in}+2p-r(f_h-1)-1}{s}\right\rfloor+1\\
        W_{out}&=\left\lfloor\frac{W_{in}+2p-r(f_w-1)-1}{s}\right\rfloor+1
        \end{align*}$$

Backward:

    $$\begin{align*}
    &\frac{\partial\mathcal{L}}{\partial W_{i,j,c_{out},c_{in}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\cdot X_{sh+r(i-1)-p, sw+r(j-1)-p, c_{in}}\\
    &\frac{\partial\mathcal{L}}{\partial b_{c_{out}}}=\sum_{h=1}^{H_{out}}\sum_{w=1}^{W_{out}}g_{h,w,c_{out}}\\
    &\frac{\partial\mathcal{L}}{\partial X_{i,j,c_{in}}}=\sum_{c_{out}=1}^{C_{out}}\sum_{h=1}^{f_h}\sum_{w=1}^{f_w}g_{h,w,c_{out}}\cdot W_{r(i-1)-sh+p,r(j-1)-sw+p,c_{out},c_{in}}
    \end{align*}$$
``` -->
<!-- ## Pooling
- **What**: Convolution but ([paper](https://proceedings.neurips.cc/paper_files/paper/1989/file/53c3bce66e43be4f209556518c2fcb54-Paper.pdf))
    - computes a heuristic per scanned patch.
    - uses the same #channels.
- **Why**: Dimensionality reduction while preserving dominant features.
- **How**: Slide the pooling window over the input & apply the heuristic within the scanned patch.
    - **Max**: Output the maximum value from each patch.
    - **Average**: Output the average value of each patch.
- **When**: When downsampling is necessary.
- **Where**: After convolutional layer.
- **Pros**:
    - Significantly higher computational efficiency (time & space).
    - No params to train.
    - Reduces overfitting.
    - Preserves translation invariance without losing too much info.
    - High robustness.
- **Cons**:
    - Slight spatial info loss.
    - Requires hyperparam tuning.
        - Large filter or stride results in coarse features.
- **Max vs Average**:
    - **Max**: Captures most dominant features; higher robustness.
    - **Avg**: Preserves more info; provides smoother features; dilutes the importance of dominant features. -->
<!-- ```{admonition} Math
:class: note, dropdown
Notations:
    - IO:
        - $\mathbf{X}\in\mathbb{R}^{H_{in}\times W_{in}\times C_{in}}$: Input volume.
        - $\mathbf{Y}\in\mathbb{R}^{H_{out}\times W_{out}\times C_{in}}$: Output volume.
    - Hyperparams:
        - $H_{in}, W_{in}$: Input height & width.
        - $C_{in}$: #Input channels.
        - $f_h, f_w$: Filter height & width.
        - $s$: Stride size.
Forward:

    $$\begin{array}{ll}
    \text{Max:} & Y_{h,w,c}=\max_{i=1,\cdots,f_h\ |\ j=1,\cdots,f_w}X_{sh+i,sw+j,c}\\
    \text{Avg:} & Y_{h,w,c}=\frac{1}{f_hf_w}\sum_{i=1}^{f_h}\sum_{j=1}^{f_w}X_{sh+i,sw+j,c}
    \end{array}$$

    where

    $$\begin{align*}
    H_{out}&=\left\lfloor\frac{H_{in}-f_h}{s}\right\rfloor+1\\
    W_{out}&=\left\lfloor\frac{W_{in}-f_h}{s}\right\rfloor+1
    \end{align*}$$

Backward:

    $$\begin{array}{ll}
    \text{Max:} & \frac{\partial\mathcal{L}}{\partial X_{sh+i,sw+j,c}}=g_{h,w,c}\text{ if }X_{sh+i,sw+j,c}=Y_{h,w,c}\\
    \text{Avg:} & \frac{\partial\mathcal{L}}{\partial X_{sh+i,sw+j,c}}=\frac{g_{h,w,c}}{f_hf_w}
    \end{array}$$

    - Max: Gradients only propagate to the max element of each window.
    - Avg: Gradients are equally distributed among all elements in each window.
``` -->
<br/>
<!-- # Recurrent
```{image} ../images/RNN.png
:width: 400
:align: center
```

$$
h_t=\tanh(x_tW_{xh}^T+h_{t-1}W_{hh}^T)
$$

Idea: **recurrence** - maintain a hidden state that captures information about previous inputs in the sequence

Notations:
- $ x_t$: input at time $t$ of shape $(m,H_{in}) $
- $ h_t$: hidden state at time $t$ of shape $(D,m,H_{out}) $
- $ W_{xh}$: weight matrix of shape $(H_{out},H_{in})$ if initial layer, else $(H_{out},DH_{out}) $
- $ W_{hh}$: weight matrix of shape $(H_{out},H_{out}) $
- $ H_{in}$: input size, #features in $x_t $
- $ H_{out}$: hidden size, #features in $h_t $
- $ m $: batch size
- $ D$: $=2$ if bi-directional else $1 $

Cons:
- Short-term memory: hard to carry info from earlier steps to later ones if long seq
- Vanishing gradient: gradients in earlier parts become extremely small if long seq

## GRU

```{image} ../images/GRU.png
:width: 400
:align: center
```

$$\begin{align*}
&r_t=\sigma(x_tW_{xr}^T+h_{t-1}W_{hr}^T) \\\\
&z_t=\sigma(x_tW_{xz}^T+h_{t-1}W_{hz}^T) \\\\
&\tilde{h}\_t=\tanh(x_tW_{xn}^T+r_t\odot(h_{t-1}W_{hn}^T)) \\\\
&h_t=(1-z_t)\odot\tilde{h}\_t+z_t\odot h_{t-1}
\end{align*}$$

Idea: Gated Recurrent Unit - use 2 gates to address long-term info propagation issue in RNN:
1. **Reset gate**: determine how much of $ h_{t-1}$ should be ignored when computing $\tilde{h}\_t $.
2. **Update gate**: determine how much of $ h_{t-1}$ should be retained for $h_t $.
3. **Candidate**: calculate candidate $ \tilde{h}\_t$ with reset $h_{t-1} $.
4. **Final**: calculate weighted average between candidate $ \tilde{h}\_t$ and prev state $h_{t-1} $ with the retain ratio.

Notations:
- $ r_t$: reset gate at time $t$ of shape $(m,H_{out}) $
- $ z_t$: update gate at time $t$ of shape $(m,H_{out}) $
- $ \tilde{h}\_t$: candidate hidden state at time $t$ of shape $(m,H_{out}) $
- $ \odot $: element-wise product

## LSTM

```{image} ../images/LSTM.png
:width: 400
:align: center
```

$$\begin{align*}
&i_t=\sigma(x_tW_{xi}^T+h_{t-1}W_{hi}^T) \\\\
&f_t=\sigma(x_tW_{xf}^T+h_{t-1}W_{hf}^T) \\\\
&\tilde{c}\_t=\tanh(x_tW_{xc}^T+h_{t-1}W_{hc}^T) \\\\
&c_t=f_t\odot c_{t-1}+i_t\odot \tilde{c}\_t \\\\
&o_t=\sigma(x_tW_{xo}^T+h_{t-1}W_{ho}^T) \\\\
&h_t=o_t\odot\tanh(c_t)
\end{align*}$$

Idea: Long Short-Term Memory - use 3 gates:
1. **Input gate**: determine what new info from $ x_t$ should be added to cell state $c_t $.
2. **Forget gate**: determine what info from prev cell $ c_{t-1} $ should be forgotten.
3. **Candidate cell**: create a new candidate cell from $ x_t$ and $h_{t-1} $.
4. **Update cell**: use $ i_t$ and $f_t $ to combine prev and new candidate cells.
5. **Output gate**: determine what info from curr cell $ c_t$ should be added to output $h_t $.
6. **Final**: simply apply $ o_t$ to activated cell $c_t $.

Notations:
- $ i_t$: input gate at time $t$ of shape $(m,H_{out}) $
- $ f_t$: forget gate at time $t$ of shape $(m,H_{out}) $
- $ c_t$: cell state at time $t$ of shape $(m,H_{cell}) $
- $ o_t$: output gate at time $t$ of shape $(m,H_{out}) $
- $ H_{cell}$: cell hidden size (in most cases same as $H_{out} $)

## Bidirectional
## Stacked -->
<br/>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="activation">
<h1>Activation<a class="headerlink" href="#activation" title="Link to this heading">#</a></h1>
<ul class="simple">
<li><p><strong>What</strong>: An element-wise non-linear function over a layer’s output.</p></li>
<li><p><strong>Why</strong>: Non-linearity.</p>
<ul>
<li><p>Without it, a full NN is just simple linear regression.</p></li>
</ul>
</li>
</ul>
<section id="binary-like">
<h2>Binary-like<a class="headerlink" href="#binary-like" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Functions with near-binary outputs.</p></li>
<li><p><strong>Why</strong>: Biology - Biological neurons generally:</p>
<ul>
<li><p>react little to small inputs</p></li>
<li><p>react rapidly after input stimulus passes a threshold</p></li>
<li><p>converge to a max as stimulus increases</p></li>
</ul>
</li>
</ul>
<section id="sigmoid">
<h3>Sigmoid<a class="headerlink" href="#sigmoid" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Sigmoid function.</p></li>
<li><p><strong>Why</strong>: Mathematically convenient <span class="math notranslate nohighlight">\(\leftarrow\)</span> Smooth gradient.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
y=\sigma(z)=\frac{1}{1+e^{-z}}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\sigma(z)\in(0,1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\sigma(0)=0.5\)</span></p></li>
</ul>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial z}=\frac{\partial\mathcal{L}}{\partial y}y(1-y)
\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Vanishing gradient.</p></li>
<li><p>Non-zero centric bias <span class="math notranslate nohighlight">\(\rightarrow\)</span> Non-zero mean activations</p></li>
</ul>
</div>
</section>
<section id="tanh">
<h3>Tanh<a class="headerlink" href="#tanh" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Tanh function.</p></li>
<li><p><strong>Why</strong>: Mathematically convenient <span class="math notranslate nohighlight">\(\leftarrow\)</span> Smooth gradient.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
y=\tanh(z)=\frac{e^z-e^{-z}}{e^z+e^{-z}}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\tanh(z)\in(-1,1)\)</span></p></li>
<li><p><span class="math notranslate nohighlight">\(\tanh(0)=0\)</span></p></li>
</ul>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[
\frac{\partial\mathcal{L}}{\partial z}=\frac{\partial\mathcal{L}}{\partial y}(1-y^2)
\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>Zero-centered.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p>Vanishing gradient.</p></li>
</ul>
</div>
</section>
</section>
<section id="relu">
<h2>ReLU<a class="headerlink" href="#relu" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Rectified Linear Unit</p></li>
<li><p><strong>Why</strong>:</p>
<ul>
<li><p>Binary-like activation functions suffered from vanishing gradients.</p></li>
<li><p>Biological neurons either fire or remain inactive.</p></li>
<li><p>ReLU-like functions existed long ago (<a class="reference external" href="https://link.springer.com/article/10.1007/BF02478220">Householder, 1941</a>).</p></li>
</ul>
</li>
<li><p><strong>How</strong>: Linear for positive, 0 for negative.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
y=\text{ReLU}(z)=\max{(0,z)}
\]</div>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial\mathcal{L}}{\partial z}=\begin{cases}
\frac{\partial\mathcal{L}}{\partial y} &amp; z\geq0 \\
0 &amp; z&lt;0
\end{cases}
\end{split}\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Pros?</em></p>
<ul class="simple">
<li><p>❌Vanishing gradient.</p></li>
<li><p>✅Sparsity.</p></li>
<li><p>✅Computational efficiency.</p></li>
</ul>
<p><em>Cons?</em></p>
<ul class="simple">
<li><p><strong>Dying ReLU</strong>: If most inputs are negative, then most neurons output 0 <span class="math notranslate nohighlight">\(\rightarrow\)</span> No gradient <span class="math notranslate nohighlight">\(\rightarrow\)</span> No param update <span class="math notranslate nohighlight">\(\rightarrow \)</span> Dead. (NOTE: A SOLVABLE DISADVANTAGE)</p>
<ul>
<li><p>Cause 1: High learning rate <span class="math notranslate nohighlight">\( \rightarrow\)</span> Too much subtraction in param update <span class="math notranslate nohighlight">\(\rightarrow\)</span> Weight⬇️⬇️ <span class="math notranslate nohighlight">\(\rightarrow\)</span> Input for neuron⬇️⬇️.</p></li>
<li><p>Cause 2: Bias too negative <span class="math notranslate nohighlight">\(\rightarrow\)</span> Input for neuron⬇️⬇️.</p></li>
</ul>
</li>
<li><p>Activation explosion <span class="math notranslate nohighlight">\(\longleftarrow\)</span> <span class="math notranslate nohighlight">\(z\rightarrow\infty\)</span>. (NOTE: NOT A SEVERE DISADVANTAGE SO FAR)</p></li>
</ul>
</div>
<section id="lrelu">
<h3>LReLU<a class="headerlink" href="#lrelu" title="Link to this heading">#</a></h3>
<ul class="simple">
<li><p><strong>What</strong>: Leaky ReLU.</p></li>
<li><p><strong>Why</strong>: Dying ReLU.</p></li>
<li><p><strong>How</strong>: Linear for positive, tiny linear for negative.</p></li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
y=\text{LReLU}(z)=\max{(\alpha z,z)}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\alpha\in(0,1)\)</span>: Negative slope hyperparameter, default 0.01.</p></li>
</ul>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial\mathcal{L}}{\partial z}=\begin{cases}
\frac{\partial\mathcal{L}}{\partial y} &amp; z\geq0 \\
\alpha\frac{\partial\mathcal{L}}{\partial y} &amp; z&lt;0
\end{cases}
\end{split}\]</div>
</div>
<div class="tip dropdown admonition">
<p class="admonition-title">Q&amp;A</p>
<p><em>Why aren’t we using LReLU in place of ReLU?</em></p>
<p>Because Dying ReLU became insignificant.</p>
<ul class="simple">
<li><p>Empirical performance: ReLU &gt;&gt; LReLU <span class="math notranslate nohighlight">\(\leftarrow\)</span> Sparsity</p></li>
<li><p>Dying ReLU is solvable with other structural changes:</p>
<ul>
<li><p>Weight Init <span class="math notranslate nohighlight">\(\rightarrow\)</span> Ensure sufficient initial positive weights.</p></li>
<li><p>Batch Norm <span class="math notranslate nohighlight">\(\rightarrow\)</span> Ensure <span class="math notranslate nohighlight">\(\sim\)</span>50% input values are positive.</p></li>
<li><p>Residual Connection <span class="math notranslate nohighlight">\(\rightarrow\)</span> Gradients can flow directly back to input even if ReLU is dead.</p></li>
</ul>
</li>
</ul>
</div>
<!-- 
### PReLU

$$
\mathrm{PReLU}(z)=\max{(\alpha z,z)}
$$

Name: Parametric Rectified Linear Unit

Params:
- $ \alpha\in(0,1) $: learnable param (negative slope), default 0.25.

Idea:
- scale negative linear outputs by a learnable $ \alpha $.

Pros:
- a variable, adaptive param learned from data.

Cons:
- slightly more computationally expensive than LReLU.
- activation explosion as $ z\rightarrow\infty $.



### RReLU

$$
\mathrm{RReLU}(z)=\max{(\alpha z,z)}
$$


Name: Randomized Rectified Linear Unit

Params:
- $ \alpha\sim\mathrm{Uniform}(l,u) $: a random number sampled from a uniform distribution.
- $ l,u $: hyperparams (lower bound, upper bound)

Idea:
- scale negative linear outputs by a random $ \alpha $.

Pros:
- reduce overfitting by randomization.

Cons:
- slightly more computationally expensive than LReLU.
- activation explosion as $ z\rightarrow\infty $.

## ELU
- **What**: Exponential Linear Units
- **Why**: 

$$
\mathrm{ELU}(z)=\begin{cases}
z & z\geq0 \\\\
\alpha(e^z-1) & z<0
\end{cases}
$$

Params:
- $ \alpha $: hyperparam, default 1.

Idea:
- convert negative linear outputs to the non-linear exponential function above.

Pros:
- mean unit activation is closer to 0 $ \rightarrow $ reduce bias shift (i.e., non-zero mean activation is intrinsically a bias for the next layer.)
- lower computational complexity compared to batch normalization.
- smooth to $ -\alpha $ slowly with smaller derivatives that decrease forwardprop variation.
- faster learning and higher accuracy for image classification in practice.

Cons:
- slightly more computationally expensive than ReLU.
- activation explosion as $ z\rightarrow\infty $. -->
<!-- ### SELU

$$
\mathrm{SELU}(z)=\lambda\begin{cases}
z & \mathrm{if}\ z\geq0 \\
\alpha(e^z-1) & \mathrm{if}\ z<0
\end{cases}
$$


Name: Scaled Exponential Linear Unit

Params:
- $ \alpha $: hyperparam, default 1.67326.
- $ \lambda $: hyperparam (scale), default 1.05070.

Idea:
- scale ELU.

Pros:
- self-normalization $ \rightarrow $ activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance.

Cons:
- more computationally expensive than ReLU.
- activation explosion as $ z\rightarrow\infty $.



### CELU

$$
\mathrm{CELU}(z)=\begin{cases}
z & \mathrm{if}\ z\geq0\\
\alpha(e^{\frac{z}{\alpha}}-1) & \mathrm{if}\ z<0
\end{cases}
$$


Name: Continuously Differentiable Exponential Linear Unit

Params:
- $ \alpha $: hyperparam, default 1.

Idea:
- scale the exponential part of ELU with $ \frac{1}{\alpha} $ to make it continuously differentiable.

Pros:
- smooth gradient due to continuous differentiability (i.e., $ \mathrm{CELU}'(0)=1 $).

Cons:
- slightly more computationally expensive than ELU.
- activation explosion as $ z\rightarrow\infty $.

## Linear Units (Others)

### GELU

$$
\mathrm{GELU}(z)=z*\Phi(z)=0.5z(1+\tanh{[\sqrt{\frac{2}{\pi}}(z+0.044715z^3)]})
$$


Name: Gaussian Error Linear Unit

Idea:
- weigh each output value by its Gaussian cdf.

Pros:
- throw away gate structure and add probabilistic-ish feature to neuron outputs.
- seemingly better performance than the ReLU and ELU families, SOTA in transformers.

Cons:
- slightly more computationally expensive than ReLU.
- lack of practical testing at the moment. -->
<!-- ### SiLU

$$
\mathrm{SiLU}(z)=z*\sigma(z)
$$

Name: Sigmoid Linear Unit

Idea:
- weigh each output value by its sigmoid value.

Pros:
- throw away gate structure.
- seemingly better performance than the ReLU and ELU families.

Cons:
- worse than GELU.



### Softplus

$$
\mathrm{softplus}(z)=\frac{1}{\beta}\log{(1+e^{\beta z})}
$$


Idea:
- smooth approximation of ReLU.

Pros:
- differentiable and thus theoretically better than ReLU.

Cons:
- empirically far worse than ReLU in terms of computation and performance.



## Multiclass -->
</section>
</section>
<section id="softmax">
<h2>Softmax<a class="headerlink" href="#softmax" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>What</strong>: Numbers <span class="math notranslate nohighlight">\(\rightarrow\)</span> Probabilities</p></li>
<li><p><strong>Why</strong>: Multiclass classification.</p></li>
<li><p><strong>How</strong>:</p>
<ol class="arabic simple">
<li><p>Exponentiation: Larger/Smaller numbers <span class="math notranslate nohighlight">\(\rightarrow\)</span> even larger/smaller numbers.</p></li>
<li><p>Normalization: Numbers <span class="math notranslate nohighlight">\(\rightarrow\)</span> Probabilities</p></li>
</ol>
</li>
</ul>
<div class="note dropdown admonition">
<p class="admonition-title">Math</p>
<p>Forward:</p>
<div class="math notranslate nohighlight">
\[
y_i=\text{softmax}(z_i)=\frac{\exp{(z_i)}}{\sum_j{\exp{(z_j)}}}
\]</div>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(i \&amp; j\)</span>: Class indices.</p></li>
</ul>
<p>Backward:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{\partial\mathcal{L}}{\partial z_i}=\begin{cases}
\frac{\partial\mathcal{L}}{\partial y_i}y_i(1-y_i) &amp; i=j \\
-\frac{\partial\mathcal{L}}{\partial y_i}y_iy_j &amp; i\neq j
\end{cases}
\end{split}\]</div>
</div>
<!-- ### Softmin

$$
\mathrm{softmin}(z_i)=\mathrm{softmax}(-z_i)=\frac{\exp{(-z_i)}}{\sum_j{\exp{(-z_j)}}}
$$

Idea:
- reverse softmax.

Pros:
- suitable for multiclass classification.

Cons:
- why not softmax. --></section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./dl"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="../intro.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">LazyNotes</p>
      </div>
    </a>
    <a class="right-next"
       href="loss.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Loss</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">Module</a></li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#basic">Basic</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#linear">Linear</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout">Dropout</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#residual-connection">Residual Connection</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#normalization">Normalization</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#batch-normalization">Batch Normalization</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#layer-normalization">Layer Normalization</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#convolution">Convolution</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#depthwise-convolution">Depthwise Convolution</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#activation">Activation</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#binary-like">Binary-like</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid">Sigmoid</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tanh">Tanh</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relu">ReLU</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lrelu">LReLU</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax">Softmax</a></li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Renyi Qu
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>